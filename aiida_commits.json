[
  {
    "pr_number": 3057,
    "commits_list": [
      "6f000cbf015fe60fbca4ed82cfeb632020b907b0"
    ],
    "message_list": [
      "Add aiida-core version to docs home page (1.0 branch)."
    ]
  },
  {
    "pr_number": 5330,
    "commits_list": [
      "97c96955f3bc89604aea46a2bb185d7c0f9510b4",
      "0e5099d7dd11fb68add06e2fbe4da8839640cd6f",
      "8603fb1154ed18f82a008c54582efd26b94ed55e",
      "5f983291e815361de9ae5f51690842649ce06537",
      "3c3fdf0ff34fcae62a9f2a2131ffc649e2c37b36",
      "7593d5bfc2b7b0211eb539d29282cefd0c79ceb1",
      "7d20946207e942b54bd7cc6b6ca697d620541a26",
      "c1e0e6511c957210ad1612a4f24a203fa5512520"
    ],
    "message_list": [
      "\ud83e\uddea TESTS: Add migration schema regression files\n\nThis commit pre-emptively adds auto-generated YAML files,\nwhich fully describe the database schema at each migration revision,\nfor both the django and sqlalchemy migrations.",
      "\u267b\ufe0f REFACTOR: Remove Django storage backend",
      "\ud83d\udcda DOCS: Update verdi CLI outputs",
      "\ud83d\udc4c IMPROVE: ArchiveReadOnlyBackend\n\nSo it can be used via a profile",
      "\ud83d\udc1b FIX: Account for variable legacy django database schema\n\nSee `aiida/backends/sqlalchemy/migrations/utils/reflect.py` for details.",
      "\ud83e\uddea TESTS: Fix sqlalchemy deprecation warnings",
      "\u267b\ufe0f REFACTOR: Implement naming conventions for PSQL schema\n\nAs described in https://alembic.sqlalchemy.org/en/latest/naming.html,\nnames are now auto-generated with specific conventions,\nspecified in `aiida/backends/sqlalchemy/models/base.py`.\nThe renaming, in the final migrations of the django and sqlalchemy branches,\nhave also been re-written to use the same code,\nensuring consistency.",
      "\ud83d\udd27 MAINTAIN: Add `NotImplementedError` for migration downgrades\n\nDowngrading storage version is not explicitly supported in aiida-core,\nor exposed for the user.\nPrevious to #5330, some downgrade functionality was required for testing,\nsince migration tests involved migrating down the global profile,\nthen migrating it back up to the target version.\nThese migrations though were often incomplete,\nmigrating database schema but not the actual data.\n\nNow, migration tests are performed by starting with a separate, empty profile,\nand migrating up.\nSince these downgrades are no longer required and are un-tested,\nwe replace their content with an explicit `NotImplementedError`."
    ]
  },
  {
    "pr_number": 2478,
    "commits_list": [
      "326bbccf0474a48cf838f7ba7082f70337ecb0b6",
      "1e7d93cf9535b7bf63f60fc884060ceb8bfcf3ef",
      "0599dabf0887bee172a04f308307e99e3c3f3ff2",
      "f379547ff79a6d877a5124e7415084733c0164b9",
      "e9f71a597b7aeb669ad72e7929fef068cab6b8b1",
      "cf5ee1da71ae5ce8bc148b551eb765c27dc0d8b8",
      "6142c1def7d9deba65892c2067ad5a25adb4523a"
    ],
    "message_list": [
      "Implement changes of database migrations for export archives\n\nThe following database migrations have been translated into a migration\nfor the contents of existing export archives with version 0.3:\n\n 9, 14, 16, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28\n\nThe ones that have been skipped were not applicable for export archives",
      "Setup the tests for export migrations of `v0.3` to `v0.4`",
      "Add latest migrations:\n\n- Rev. 0029\n- Rev. 0030\n- Rev. 0031\n- Rev. 0032\n- Rev. 0033\n\nIntroduce changes to `metadata.json`:\n\n- Add `Log` to `all_fields_info` and `unique_identifiers`\n- Add `Comment` to `all_fields_info` and `unique_identifiers`\n- Changes to key values of `all_fields_info`\n\nAdd extra dicts to `data.json`:\n\n- `node_extras`\n- `node_extras_conversion`\n\nBoth will be empty dicts.\nIf `Node`s are present in the exported file, key-value-pairs will be added,\nwhere the key is the `Node` pk/id and the value is an empty dict.\n\nRenamed new test file by prepending `test_`\n\nUpdated `test_v3_to_v4` in new test file to migrate `export_v0.3_no_UPF.aiida`\ninstead of `arithmetic.add.aiida`.\n\nRemoved UPF node attributes information from `data.json` files for:\n\n- `export_v0.1.aiida`\n- `export_v0.2.aiida`\n- `export_v0.3.aiida`\n\nRenamed them by appending `_no_UPF`.\n\nMigrated `export_v0.3_no_UPF.aiida` to export version 0.4.\n\nMigrated other existing export files under `fixtures` to export version 0.4.\nBut only for those that are expected to be the newest version.\n\nUpdated `test_calcjob.py` and found an issue for `inputcat` and `outputcat`.\nThis has been reported in issue #2611.\n\nUpdated all tests that use exported files from `fixtures`:\n\n- `test_export_and_import.py`\n- `test_calcjob.py`\n- `test_export.py`\n- `test_import.py`\n- `test_archive.py`\n\nAdded test `test_no_node_export` to check migration of export file with no Nodes succeed.\nAdded `export_v0.3_no_Nodes.aiida` export file for this test.\n\nUpdated test utils file `fixtures.py` to incorporate the  `get_archive_file` function\nfrom `test_export.py`.\n\nAll instances of reimplementations of this function have been updated\nwith the new `get_archive_file` util function.\n\nInserted key checks, if migration of an export file should have been partially performed,\nfor one reason or another.\n\nUpdate arithmetic.add.aiida",
      "Moving all migration functions out of `cmd_export.py` and into `aiida.cmdline.utils.migration`.\n\nFollowing files have been created:\n- `__init__.py`: Contains `migrate_recursive` and allows for import from `aiida.cmdline.utils.migration`\n- `utils.py`: Contains `verify_metadata_version` and `update_metadata`\n- `v01_to_v02.py`: Contains function to migrate from export version v0.1 to v0.2\n- `v02_to_v03.py`: Contains function to migrate from export version v0.2 to v0.3\n- `v03_to_v04.py`: Contains functions to migrate from export version v0.3 to v0.4\n\nMoving and renaming new test-file `test_export_file_migrations.py` from `aiida.backends.tests`\nto `aiida.backends.tests.cmdline.utils.migrations`, renaming it to `test_v03_to_v04.py`.\n\nSee previous commits for actual authors of the code.",
      "New tests for export file migrations - now in separate repo \"aiida-export-migration-tests\".\n\nTests for the following files in `cmdline.utils.migration`:\n- `__init__.py` in `test_migration.py`\n- `v01_to_v02.py` in `test_v01_to_v02.py`\n- `v02_to_v03.py` in `test_v02_to_v03.py`\n- `v03_to_v04.py` in `test_v03_to_v04.py`\n\nFound and fixed bug for migration of TrajectoryData in Rev. 1.0.26 and 1.0.27 for v0.3 to v0.4:\n\"symbols\" was only updated for \"node_attributes\" not \"node_attributes_conversion\".\n\nAdded more representative export files for the different export versions.\nThey have been created using the same workflow (by @yakutovicha) in specialized QuantumMobile VMs.\nAiiDA version used for export versions:\nExport: 0.2, AiiDA: 0.9.1\nExport: 0.3, AiiDA: 0.12.3\nExport: 0.4, AiiDA: 1.0.0b2\n\nExport version 0.1 was set to AiiDA version 0.6.0,\nbut AiiDA was never released on PyPi until version 0.8.0,\nand the changes from export version 0.1 to 0.2 are very minor.\nSo a more representative export file for export version 0.1 is left out.\nThe export files are in the separate repo.\n\nAdded dependency for separate repo if \"testing\" is chosen when installing via pip.\n\nTo be pythonic and have more transparent code,\nthe migration steps were divided into separate functions.\nThis also results in the fact that a lot of migrations can now be skipped,\nif no Nodes are present in the export file.",
      "Update to EXPORT_VERSION 0.5\n\nAdd migration from v0.4 to v0.5.\nThis includes migration 0034 and 0036 (0035 has no effect).\n\nCreate new simple export file for v0.5 and update tests to use it.\nMigrate other export files.\n\nUp version dependancy of `aiida-export-migration-tests` to 0.5.0.",
      "Fixes according to @sphuber review"
    ]
  },
  {
    "pr_number": 4345,
    "commits_list": [
      "ac0d20987b275a1b780f6c16ea735dc092bca704",
      "30fcf6ab58fc09d9345543895ef501ac8183531d",
      "45e7ca25fe8d74be09520f113b6566692cb82407",
      "583d09f779dc8cdf9e2db2419132d9edc2cedd85",
      "ee7a65ec8cc528a95798eeb0ff85a35b17c1e8db",
      "b89411c0a141d9d0ce067a28096fd2dcedeb3467",
      "f882df80fa6569d168c13701b2353bf8d4bcff6a",
      "2d4ae66f3c762a828fea2fc0dad2831b4519e609",
      "a3930ffaddb5e94caac02ecd3993ce8c5b5d00f8",
      "f30f000f985d0acf63c00b7f0d63ea6660feb9f6",
      "03bb76857457810704adc5f65a1ade66d3571079",
      "36939cdecede72834160c146425835f81d0b29aa",
      "0e250e6aa59dc81242eea4b0437bb2df31a687ec"
    ],
    "message_list": [
      "Add the `repository_metadata` column to the `Node` database models\n\nThis new JSONB column will be used to store the metadata of the contents\nof the file repository. It essentially is a mapping of the file object\nhierarchy, where each object contains the relative filepath of it within\nthe repository, which is purely symbolic, and a hashkey, which is the\nactual identifier that uniquely identifies the object in the repository\nbackend.\n\nThe column is made nullable and there is no default set. This is to make\nsure to not add unnecessary bytes to the database for all the nodes that\ncontain no files whatsoever, which will be a decent chunk. The front-end\nORM class `Node` will return an empty dictionary if the database model\nfield is null, such that the clients don't have to deal with `None`.",
      "Add the base implementation for the new repository\n\nThe `Repository` class is the new interface to interact with the file\nrepository. It contains a file hierarchy in memory to now what is\ncontained within it. A new instance can be constructed from an existing\nhierarchy using the `from_serialized` class method.\n\nThe actual storing of the objects in a repository is delegated to a\nbackend repository instance, represented by `AbstractRepositoryBackend`.\nThis backend class is only tasked with consuming byte streams and\nstoring them as objects in some repository under a unique key that\nallows the object to be retrieved at a later time. The backend\nrepository is not expected to keep a mapping of what it contains. It\nshould merely provide the interface to store byte streams and return\ntheir content, given their unique corresponding key.\n\nThis also means that the internal virtual hierarchy of a ``Repository``\ninstance does not necessarily represent all the files that are stored by\nrepository backend. The repository exposes a mere subset of all the file\nobjects stored in the backend. This is why object deletion is also\nimplemented as a soft delete, by default, where the files are just\nremoved from the internal virtual hierarchy, but not in the actual\nbackend. This is because those objects can be referenced by other\ninstances.\n\nThe interface is implemented for the `disk-objectstore` which is an\nefficient key-value store on the local file system. This commit also\nprovides a sandbox implementation which represents a temporary folder on\nthe local file system.",
      "Improve the storage effiency of the `repository_metadata` format\n\nSince now all nodes will have a full representation of the virtual file\nhierarchy of their repository content, which is stored in the database\nin the `repository_metadata` column, it is crucial that the format of\nthat metadata is optimized to use as little data as possible, to prevent\nthe database from bloating unnecessarily.\n\nCurrently, we only define two file types:\n\n * Directories: which can contain an arbitrary number of sub objects\n * Files: which cannot contain objects, but instead have a unique hashkey\n\nThis means that we can serialize the metadata, without encoding the\nexplicit type. If it contains a key, it is necessarily a file. If it is\nnot a file, it has to be a directory.\n\nThe serialization format therefore can make do with simple nested\ndictionaries, where each entry either contains the key `k`, indicating\nit is a file, or `o`, which can be a dictionary itself, containing the\nobjects contained within that directory. Note that we can even safely\nomit the `o` key for an empty dictionary. Since there is no `k` we know\nit has to be a directory anyway.\n\nThis logic is implemented in `FileObject.from_serialized` that can fully\nreconstruct all instance properties by inferring them from the sparse\nserialized data.",
      "Remove the code to perform incremental backup of the file repository\n\nThis code was put in place because a naive backup of the file repository\nwas impossible already for reasonably size projects. The problem was the\nunderlying design where each node had an associated folder on disk, even\nif it contained no files, and all files were stored separately, creating\na huge numbers of files. This meant that just rsync'ing the folder could\ntake days even to just calculate the difference with the backed up\nfolder.\n\nThe ad-hoc solution was this script that would only transfer the folders\nof the nodes added since the last backup date, the list of which was\ndetermined by a simple query. However, now that the new file repository\nis implemented, which has been explicitly designed to be easily backed\nup by tools like `rsync`, this custom solution is no longer needed.",
      "Integrate the new `Repository` implementation\n\nThe new `Repository` implementation is integrated by wrapping it in a\nnew class `NodeRepository` that is mixed into the `Node` class. This\napproach serves multiple purposes.\n\n 1. The class serves as a backwards compatibility layer between the old\n    repository interface, which would allow clients to set the mode for\n    file handles for reading, as well as passing in text-like streams,\n    and the new interface, which exclusively operates with bytes. The\n    interface will decode the bytes when the caller is expecting normal\n    strings in order to not break the interface.\n\n 2. It allows to implement the concept of immutability which is a\n    concept of the `Node` and is completely unknown to the `Repository`\n    class.\n\n 3. It nicely separates the file repository functionality into a\n    separate file preventing the already huge `Node` implementation from\n    getting even larger.\n\nThe downside of the latter is that the `NodeRepository` class contains\nsome potentially confusing logic. It accesses `Node` properties, but\nfrom the code it is not clear where they come from. Since this class is\nonly meant to be used as a mixin for the `Node` class, I think that is\na sacrifice that is worth the benefits.",
      "Add the repository mutating methods to the `Sealable` mixin.\n\nThis ensures that the repository is still mutable for sealable nodes as\nlong as they are not yet sealed. After sealing, the repository of even\nthese nodes becomes fully immutable.\n\nNote that the implementation completely overrides the methods of the\n`NodeRepositoryMixin`, because it needs to override the check in those\nthat will raise if the node is stored. An alternative would have been to\ncall the `super` from the `Sealable`, which due to the correct MRO,\nwould in fact the underlying method on the `NodeRepositoryMixin`, but it\nwould need to accept an argument, like for example `force`, to skip the\nmutability check. This is not desirable, however, since those repository\nmethods are public methods of the `Node` interface and so any user will\nbe able to disable the mutability check for any node.\n\nThis solution does not suffer from that vulnerability but the downside\nis that the implementation of the method from the `NodeRepositoryMixin`\nneeds to be copied and it needs to be kept in sync.",
      "Docs: add \"Internal architecture - File repository\" section\n\nThis section describes in detail the new file repository design,\nincluding the design guidelines that were kept in mind and what\nrequirements it was designed to respect. It also gives a detailed\noverview over how the implementation is integrated in the existing\ncode base.",
      "`Repository`: allow `File` class to be changed",
      "Add migrations for existing file repositories\n\nThe migration of an existing legacy file repository consists of:\n\n 1. Make inventory of all files in the current file repository\n 2. Write these files to the new backend (disk object store)\n 3. Store the metadata of a node's repository contents, containing the\n    virtual hierarchy to the corresponding `repository_metadata` column\n    in the database.\n\nThe repository metadata will contain a hashkey for each file that was\nwritten to the disk object store, which was generated by the latter and\nthat can be used to retrieve the content of the file.\n\nThe migration is performed in a single database migration, meaning that\neverything is executed in a single transaction. Either the entire\nmigration succeeds or in case of an error, it is a no-op. This why the\nmigration will not delete the legacy file repository in the same\nmigration.\n\nThe advantage of this approach is that now there is no risk of data loss\nand/or corruption. If the migration fails, all original data will be in\ntact. The downside, however, is that the content of the file repository\nis more or less duplicated. This means that the migration will require a\npotentially significant amount of disk space that is at worst equal to\nthe size of the existing file repository. This should be the upper limit\nsince the disk object store will both deduplicate as well as compress\nthe content that is written.",
      "First implementation of export/import using new disk object store\n\nUse the `Container.export` method in export/import functionality",
      "Database: add cross-reference of repository UUID to settings table\n\nThe new repository implementation, using the `disk-objectstore`\nunderneath, now provides a UUID for each repository. Currently, each\ndatabase can only be configured to work with a single repository. By\nwriting the UUID of the repository into the database, it will become\npossible to enable a consistency check of the repository and database\nthat are configured for a particular profile. This will prevent\naccidental misconfigurations where the wrong repository is coupled to a\nparticular database.\n\nThe repository UUID is generated automatically by the `disk-objectstore`\nlibrary when the container is created and it provides an interface to\nretrieve it. This value is written to the database in the `verdi setup`\ncommand when a new profile is created. If the database already has the\nrepository UUID setting defined, it will be cross-referenced with the\none of the repository to make sure it is compatible. This case is to\nfacilitate the creation of a new profile for an existing repository and\ndatabase. However, if the UUIDs do not match, the setup command fails.\n\nFor databases that were created before this commit, the database\nmigration that performed the migration of the legacy file repository\nincludes a statement that will insert the UUID of the new object store\ncontainer once is has been created.",
      "Profile: check database and repository compatibility on load\n\nSince the migration to the new repository implementation, each file\nrepository has its own UUID. This UUID is now written to the settings\ntable of the database that is associated to it. This allows to check\nthat the repository and database that are configured for a profile are\ncompatible.\n\nThe check is added to the `Manager._load_backend` method, as this is the\ncentral point where the database is loaded for the currently loaded\nprofile. We need to place the check here since in order to retrieve the\nrepository UUID from the database, the corresponding backend needs to be\nloaded first.\n\nIf the UUID of the repository and the one stored in the database are\nfound to be different, a warning is emitted instructing the user to make\nsure the repository and database are correctly configured. Since this is\nnew functionality and its stability is not known, a warning is chosen\ninstead of an exception in order to prevent AiiDA becoming unusable in\nthe case of an unintentional bug in the compatibility checking. In the\nfuture, when the check has been proven to be stable and reliable, the\nwarning can be changed into an exception.",
      "CLI: fix loading time of `verdi`\n\nThe `from disk_objectstore import Container` import has a significant\nloading time. The `aiida.manage.configuration.profile` module imported\nit at the top level, and since this module is loaded when `verdi` is\ncalled, the load time of `verdi` was significantly increased. This would\nhave a detrimental effect on the tab completion speed.\n\nThe work around is to only import the module within the methods that use\nit. Ideally, the loading of this library would not be so costly."
    ]
  },
  {
    "pr_number": 4344,
    "commits_list": [
      "056d4d7d78312fb795b7afb006fd1a29ea0bb0a3"
    ],
    "message_list": [
      "Prepare the code for the new repository implementation\n\nIn `v2.0.0`, the new repository implementation will be shipped, that\ndespite our best efforts, requires some slight backwards-incompatible\nchanges to the interface. The envisioned changes are translated as\ndeprecation warnings:\n\n * `FileType`: `aiida.orm.utils.repository` ->`aiida.repository.common`\n * `File`: `aiida.orm.utils.repository` ->`aiida.repository.common`\n * `File`: changed from namedtuple to class\n * `File`: iteration is deprecated\n * `File`: `type` attribute -> `file_type`\n * `Node.put_object_from_tree`: `path` -> `filepath`\n * `Node.put_object_from_file`: `path` -> `filepath`\n * `Node.put_object_from_tree`: `key` -> `path`\n * `Node.put_object_from_file`: `key` -> `path`\n * `Node.put_object_from_filelike`: `key` -> `path`\n * `Node.get_object`: `key` -> `path`\n * `Node.get_object_content`: `key` -> `path`\n * `Node.open`: `key` -> `path`\n * `Node.list_objects`: `key` -> `path`\n * `Node.list_object_names`: `key` -> `path`\n * `SinglefileData.open`: `key` -> `path`\n * Deprecated use of `Node.open` without context manager\n * Deprecated any other mode than `r` and `rb` in the methods:\n    o `Node.open`\n    o `Node.get_object_content`\n * Deprecated `contents_only` in `put_object_from_tree`\n * Deprecated `force` argument in\n    o `Node.put_object_from_tree`\n    o `Node.put_object_from_file`\n    o `Node.put_object_from_filelike`\n    o `Node.delete_object`\n\nThe special case is the `Repository` class of the internal module\n`aiida.orm.utils.repository`. Even though it is not part of the public\nAPI, plugins may have been using it. To allow deprecation warnings to be\nprinted when the module or class is used, we move the content to a\nmirror module `aiida.orm.utils._repository`, that is then used\ninternally, and the original module has the deprecation warning. This\nway clients will see the warning if they use it, but use in `aiida-core`\nwill not trigger them. Since there won't be a replacement for this class\nin the new implementation, it can also not be replaced or forwarded."
    ]
  },
  {
    "pr_number": 4317,
    "commits_list": [
      "1d86f04e38f5a5f372eb7690629cab50adb111c1",
      "4bb7dbefd7b5dd40d513632f6820d3108c8c708b",
      "92e3bc465613efad81224094885f8d95c9fbbe96",
      "177298e42aa190181eb764705b3a5b9a55782ddc",
      "ba1e43f98bac0efa85dc981af1e356f45aa2721b",
      "040b7e43dd54b90d08f263ef50e24123775bd962",
      "016aee56ff789630e9ad5196186319dcbf79f9ce",
      "80af135d771639fa5189d74f999c794e49e74436"
    ],
    "message_list": [
      "Engine: replace `tornado` with `asyncio`\n\nThe `plumpy` and `kiwipy` dependencies have already been migrated from\nusing `tornado` to the Python built-in module `asyncio` in the versions\n`0.16.0` and `0.6.0`, respectively. This allows us to also rid AiiDA of\nthe `tornado` dependency, which has been giving requirement clashes with\nother tools, specifically from the Jupyter and iPython world. The final\nlimitation was the `circus` library that is used to daemonize the daemon\nworkers, which as of `v0.17.1` also supports `tornado~=5`.\n\nA summary of the changes:\n\n * Replace `tornado.ioloop` with `asyncio` event loop.\n * Coroutines are marked with `async` instead of decorated with the\n   `tornado.gen.coroutine` decorator.\n * Replace `yield` with `await` when calling a coroutine.\n * Replace `raise tornado.gen.Return` with `return` when returning from\n   a coroutine.\n * Replace `add_callback` call on event loop with `call_soon` when\n   scheduling a callback.\n * Replace `add_callback` call on event loop with `create_task` when\n   scheduling `process.step_until_terminated()`.\n * Replace `run_sync` call on event loop with `run_until_complete`.\n * Replace `pika` uses with `aio-pika` which is now used by the `plumpy`\n   and `kiwipy` libraries.\n * Replace `concurrent.Future` with `asyncio.Future`.\n * Replace `yield tornado.gen.sleep` with `await asyncio.sleep`.\n\nAdditional changes:\n\n * Remove the `tornado` logger from the logging configuration.\n * Remove the `logging.tornado_loglevel` configuration option.\n * Turn the `TransportQueue.loop` attribute from method into property.\n * Call `Communicator.close()` instead of `Communicator.stop()` in the\n   `Manager.close()` method. The `stop` method has been deprecated in\n   `kiwipy==0.6.0`.",
      "`Process.kill`: properly resolve the killing futures\n\nThe result returned by `ProcessController.kill_process` that is called\nin `Process.kill` for each of its children, if it has any, can itself be\na future, since the killing cannot always be performed directly, but\ninstead will be scheduled in the event loop. To resolve the future of\nthe main process, it will have to wait for the futures of all its\nchildren to be resolved as well. Therefore an intermediate future needs\nto be added that will be done once all child futures are resolved.",
      "Unwrap the futures returned by `ProcessController` in `verdi process`\n\nThe commands of `verdi process` that perform an RPC on a live process\nwill do so through the `ProcessController`, which returns a future.\nCurrently, the process controller uses the `LoopCommunicator` as its\ncommunicator which adds an additional layer of wrapping. Ideally, the\nreturn type of the communicator should not change depending on the\nspecific implementation that is used, however, for now that is the case\nand so the future needs to be unwrapped explicitly one additional time.\nOnce the `LoopCommunicator` is fixed to return the same future type as\nthe base `Communicator` class, this workaround can and should be\nremoved.",
      "`Runner`: use global event loop and global runner for process functions\n\nWith the migration to `asyncio`, there is now only a single event loop\nthat is made reentrant through the `nest-asyncio` library, that monkey\npatches `asyncio`'s built-in mechanism to prevent this. This means that\nin the `Runner` constructor, we should simply get the global event loop\ninstead of creating a new one, if no explicit loop is passed into the\nconstructor. This also implies that the runner should never take charge\nin closing the loop, because it no longer owns the global loop.\n\nIn addition, process functions now simply use the global runner instead\nof creating a new runner. This used to be necessary because running in\nthe same runner, would mean running in the same loop and so the child\nprocess would block the parent. However, with the new design on\n`asyncio`, everything runs in a single reentrant loop and so child\nprocesses no longer need to spawn their own independent nested runner.",
      "Engine: cancel active tasks when a daemon runner is shutdown\n\nWhen a daemon runner is started, the `SIGINT` and `SIGTERM` signals are\ncaptured to shutdown the runner before exiting the interpreter. However,\nthe async tasks associated with the interpreter should be properly\ncanceled first.",
      "Engine: enable `plumpy`'s reentrant event loop policy\n\nThe event loop implementation of `asyncio` does not allow to make the\nevent loop to be reentrant, which essentially means that event loops\ncannot be nested. One event loop cannot be run within another event\nloop. However, this concept is crucial for `plumpy`'s design to work and\nwas perfectly allowed by the previous event loop provider `tornado`.\n\nTo work around this, `plumpy` uses the library `nest_asyncio` to patch\nthe `asyncio` event loop and make it reentrant. The trick is that this\nshould be applied at the correct time. Here we update the `Runner` to\nenable `plumpy`'s event loop policy, which will patch the default event\nloop policy. This location is chosen since any process in `aiida-core`\n*has* to be run by a `Runner` and only one runner instance will ever be\ncreated in a Python interpreter. When the runner shuts down, the event\npolicy is reset to undo the patch.",
      "Tests: do not create or destroy event loop in test setup/teardown",
      "Engine: explicitly enable compatibility for RabbitMQ 3.5\n\nRabbitMQ 3.6 changed the way integer values are interpreted for\nconnection parameters. This would cause certain integer values that used\nto be perfectly acceptable, to all of suddent cause the declaration of\nresources, such as channels and queues, to fail.\n\nThe library `pamqp`, that is used by `aiormq`, which in turn is used\nultimately by `kiwipy` to communicate with the RabbitMQ server, adapted\nto these changes, but this would break code with RabbitMQ 3.5 that used\nto work just fine. For example, the message TTL when declaring a queue\nwould now fail when `32767 < TTL < 655636` due to incorrect\ninterpretation of the integer type.\n\nThe library `pamqp` provides a way to enable compatibility with these\nolder versions. One should merely call the method:\n\n    pamqp.encode.support_deprecated_rabbitmq()\n\nThis will enable the legacy integer conversion table and will restore\nfunctionality for RabbitMQ 3.5."
    ]
  },
  {
    "pr_number": 2596,
    "commits_list": [
      "62b398bc3d16812cd587a1fa15f0e43eca303583",
      "4a11d5cb9d40d0f1f11fe11f3c58daf7901b7822",
      "1714c4dcab46ac3d54a7bae3298efdbc6fedb8ec",
      "5d3ca0b77fb7df61c6170cf40471dbc4c64ed767",
      "8ef11f2895fb86529a1665a73f657354f1a637ab",
      "f44c16d58c6791f8ba6b14553e0cb0c20d2907c8",
      "6e40e7d723ddfe397e825afca568b014316a9d12",
      "2ef18e660ac47e70328d96883c0e645fb29099ce",
      "1089287532f3dd67ee452561f6b9ca012e04cb40",
      "b236a657ea5973cede6855b4ad74b12f28123a1e",
      "104f4e4ff23d5d7c55db2b0977c1e22ddc582578",
      "d5371a5acba36acd9934f9497d4482e1b791b82a",
      "35685d31a1a926fbf434488ed934baab1bcdfda3",
      "4d02467e34637560020341a286da4f47c502ac6c",
      "2d85794f97348633f3202989890d600b913e1340",
      "4cdca70b655cd5f290069268c98fdb7ee6133cd3",
      "2ee6ca9b096d979eb602e3f7c7a3970cd0385025",
      "1f8a01c6166e4da7004cd20ecb66b0707835ce79",
      "5a111b5b8a51995292ec3a8a85b0eb140d4cf5a0",
      "07fe0f59ed9cc1ccc4e46a721ad0201accd8d75c",
      "28cc1b7f26cc558e247d1948c31074fae81b792b",
      "695de89fcce6dcf01be527d41e55e34227fd3ced",
      "661eba496718a3761255cb3ffb082f30f54c818c",
      "e3ce61663ca673a9aea351d22fd02d89fb7234cd",
      "ba9ea49452364a0574fff169907127c6dbcd2e8b",
      "ab8a4c3827bf71ee6d49307946515a7b43f889c5",
      "ae3f87385bb4736053dc132e1b74fc863a866a71",
      "aec83f390e3724eca47f5d3f86fba1178741ea95",
      "bc2d1cb37654062adbc4376a82c23df6e6038afe",
      "5bf1e2838f89dbd483bbcd7ed3e220df9d17efc0"
    ],
    "message_list": [
      "CJS fix",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "add main code",
      "added add_origins_to_targets method",
      "fix _cif_style",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into enhanced-graphviz",
      "updated to be compatible with v1.0.0b2",
      "test fixes",
      "convert docstrings to ReST style",
      "test fixes",
      "test fixes",
      "test fix",
      "docstring fixes",
      "Update cmd_graph.py",
      "default sub-label to node.get_description()",
      "move the process node styling to a separate function",
      "attempt to get default style from node.get_style_default()",
      "Merge remote-tracking branch 'upstream/develop' into enhanced-graphviz",
      "Merge remote-tracking branch 'upstream/develop' into enhanced-graphviz",
      "Merge remote-tracking branch 'upstream/develop' into enhanced-graphviz",
      "clear jenkins workspace",
      "change default link/node styles",
      "update Code node default style",
      "Update graph.py"
    ]
  },
  {
    "pr_number": 4532,
    "commits_list": [
      "d42aa88e693c19e75f8d7e0b090652157f6fbcd6",
      "e9ad3224fcb7d37b6f8a47500ac54f686a2c2ddb",
      "c2c9e6064b23a23afb3e06cd4a8d867236bec00a",
      "66021a4378814cec6b140c1cb64e17413342c516",
      "70c1fb0c24a5fe5939bfebc452ed401561710f61",
      "298ee420efa0b6f76586822c74d0c17ce162a90a",
      "cbcaec6e3a4e0f90922f3502f12798098011d915",
      "b44e3501df57c63f14cae1752d261a880e069086",
      "cf676551a089659d25e2abdc62e52921aebcfaa4",
      "ffb36990ec9069b8f078be2eb20dd6aece2193b7",
      "a80c5d9eefe978bd51ed96c6f8e536fb3025df2a",
      "7e04ff18cf496ccbcfbd9ae22b1ddb841d5e5f41",
      "072af2612df96b2ac2e6d22a3661ae6ce03cf53e",
      "d24481e35853dc79e8a91d95bbd2e51d31024932",
      "4356e0c8def5c53fdbdf5fd1f761cd36f39511cf",
      "f690b180ed6e20f8e206a2e0522b4635bf8a89e7",
      "641b7045ad16e5d630addf6037bd9e512df9dc8c",
      "4238cb2ec3a391e62b54efa596335025ee5b5f2d",
      "baf4eb1cba5e0558292e5c4b0892394fc4ce3339",
      "12960b45ecbb22623d1a6b0fd6891c03d680c4a5",
      "be1bbb461a9dc2aa580409d077979e278b5079a8",
      "6a8df4eecfb13423509061d2945be6a4b949c4f2",
      "cd7a27c646fe202b65b585c48b9ae091583ecace",
      "3b8b07f31a5b242c4ba6d3e0f5917b7b6be3d560",
      "f95b6aeed66bc1cb5c2652d92d8fbdedc309f783",
      "5d62743cd6c073a09db34369811e1d5bb2402b6f",
      "06fcd92919866c4594c9032a63013b90e158f350",
      "4dd7fa807c0b1652558be9f1454bfbe63c9eec00",
      "026b5cb5d755e249407ab2a463529e369523e3c4",
      "348242f0892e63c109a48a1db2e681396f1eaaba"
    ],
    "message_list": [
      "refactor (tests not yet fixed)",
      "update tests",
      "fix docs",
      "update verdi import with new migration process",
      "Apply suggestions from code review\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "fix tests",
      "Add CacheFolder",
      "Improve caching mechanism for json\n\nallow for only a single copy of a dict to exist in memory at any time",
      "minor fix",
      "simplify `verdi import` logic\n\n- fail on first error (do not continue loop of additional archives on click.confirm)\n- never ask user whether to migrate (just use --migration/--no-migration)",
      "fix pre-commit",
      "remove some pylint disables",
      "typo fix",
      "Update aiida/cmdline/commands/cmd_import.py\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "fix pre-commit",
      "update docs",
      "Apply suggestions from code review\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "apply review suggestions",
      "add extra debug statements",
      "fix tar extraction",
      "add more progress feedback",
      "move compression to common",
      "fix over-count",
      "Allow for no compression before import",
      "fix pre-commit",
      "fix pre-commit",
      "Update aiida/tools/importexport/archive/common.py\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "improve memory usage",
      "refresh migration description early",
      "Add tar migration test"
    ]
  },
  {
    "pr_number": 5058,
    "commits_list": [
      "2517a7a480a316845af242d577ff8c43e8a9bd87",
      "f943ee72a577db542d7da606e955ac0e6e70dbc8",
      "3f68491c16877c9e07dd44eacf9e0a0cbe6b6201",
      "74a9ebd6ad3537f8d503695da8a8cf5cca978802",
      "f709b5e2c9ae4f78aa6a39ea5cac7a4928aea803",
      "bf2b78912a18251b1c5ab1e5d685b272703855f8",
      "ab20f5e4e24cb921787ef3e84b8a65dee2296c04",
      "3aa73bdb82a481b8081e238e0f6d22e48bbc56ec",
      "e3612a3b100f8ba76f9a12b714a72fa23010991f",
      "09d9fe38c679c52b83b013da0c02ea9639dd0956",
      "27ef0ad37a0ed2d367ff822614a194def3175b94",
      "6cde07fc2a62a5101f9b0d779adddeefa53918c2",
      "52f460adc67be6d6bd410f00f54a765dcc9787e0",
      "48f8a1e4ec2604abadf7c8bf99d7c4ff1f440dba",
      "5bf24f5c2d03dd5439d71be4bfee7b8ff3766a4b",
      "80d1fcb368c82ee431fcd2bd3f531bfc4b767bf9",
      "8acbec12e23c13674c1c79ec92de01ba8f582e03",
      "dac9630699b111c27881d072ec8c9ded637966d1",
      "3437aef5550ab4cb8558a3befe362ccdc5ecdabf",
      "c6c060d8720ae61f382c77609c191bd48d51065c",
      "039c0c3664f233c16f2eb047e51b0a7ac837375e",
      "42903929f63ec7a74739165cf4fb198686f788b6",
      "6689fd253f7bd9ef3ca54011da6d15827ef7077c",
      "72c99d32c926d7884c70e87410ece54776e8ce50",
      "4a0dec3749942e449aa64cc95d26488fce3b973b",
      "3fd5b4c58d9d3553e59e7e35aace6ab6d699714a",
      "8362ef1854fa6dcbd49191ab508d6918514ccb7d",
      "418c1a52f2543b1b0653d7125163e54085137695",
      "d540e99666bd7410c368be499f1f88655692cead"
    ],
    "message_list": [
      "\u267b\ufe0f REFACTOR: Remove reentry requirement",
      "fix docs",
      "update",
      "Update nitpick-exceptions",
      "Update calcjob.py",
      "Update entry_point.py",
      "Update entry_point.py",
      "Update entry_point.py",
      "run load time for all python versions",
      "pin to `importlib-metadata~=4.3`",
      "Update environment.yml",
      "re-add fastentrypoints",
      "Remove pyproject.toml validation",
      "remove try/except",
      "Merge branch 'develop' into remove-reentry",
      "Update .github/workflows/ci-code.yml",
      "Merge branch 'develop' into remove-reentry",
      "Update .pre-commit-config.yaml",
      "Add caching for additional functions",
      "Merge branch 'develop' into remove-reentry",
      "Merge branch 'develop' into remove-reentry",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "cache `eps()`",
      "Update requirements-py-3.7.txt",
      "Update verdi.sh",
      "Update .github/system_tests/test_verdi_load_time.sh",
      "Merge branch 'develop' into remove-reentry",
      "Merge branch 'develop' into remove-reentry",
      "Update ci-code.yml"
    ]
  },
  {
    "pr_number": 3599,
    "commits_list": [
      "4ac3efdfa73f8943232d32ac4625337e4055bfc9",
      "44bf4a53b855ffb189683b08df35cfe493b0e57d",
      "f3b663ddd42aa7c4175b72c743dbf6b72a7b4ce5",
      "2f9a17fab940a849166c04c8372b81cbf56a3ee4",
      "db6a4600910352acd46c0b7b411227b28a647fc7",
      "219645f0417ac445716a599b3d8fdbb20298beee",
      "ff3230f17a74fc2c5fbde05b4ee708da2e4cd757",
      "2424946f8b276602bc72e978a9e2a018c99f68c9",
      "8f8ce553a9a52d3192fe96ad9ee02444b8b55c1b",
      "9136f27325f9ab44e13024d99d0b879dd36181f0",
      "e16d44a7f0fdb061e2f622e28610f06649d84547",
      "94dbc39bcbe9302f17d11f58b7b74ed78ee51cdb",
      "e712f780e1432e45df10c8fb48e8455ab241951b",
      "0ed38f7f39fd2695e3090d0c7f4f1088754c7476",
      "d21d7a31df3ff6f73132840d9d8b618126db3cbc",
      "a26d3345f2e478d6e73faf1f9d665e635e483499",
      "1ea98ea28dfda03c501b40836e6d3e5d003513af",
      "cb11a916bb39b0baec769f7d9641b7e100d9713a",
      "5cdeb70fba5f615a407872447d2fc4e2e08740b4",
      "3d3eabacdfa4d5be075b51ef3a9fb1adff70d621",
      "c68460cb9e82d536796666f53fdd7748910d4c25",
      "4aba3c2e10cc66ea0d5811891bd515a0408123df",
      "ce0276b5e534b991c15770b59c18fa884f42d38e",
      "ff70578ff150670ba511dc94dc6eba1aacefb901",
      "454a11db7c3e7d08fdace2badeb49f02236873c3",
      "d79021c0700f0968d43fdeb1a47562b8cd668f9f",
      "bf1a587a9cc9991214110d8b8ad839a02528d486",
      "3ddcb03c987310ff0832237780bf8bc1df75a597",
      "9b86b7b21f78004ee8aa53dcbf89bded042d5595",
      "79f3b34adf2c5114ab9fe2315c641f9189b38194"
    ],
    "message_list": [
      "Update folders doc-strings",
      "Introduce tqdm dependency",
      "Progress bar - archive and config",
      "Introduce debug param - import/export",
      "Progress bar - django",
      "Progress bar - SQLA",
      "Progress bar - export",
      "Print header - export",
      "Optimize Exception handling `verdi export create`",
      "More intelligent error handling `verdi import`",
      "Header - export",
      "Remove entity_sig from import sqla",
      "Optimize import of existing Node Extras",
      "Header - import",
      "Header re-write - ex/import - use tabulate",
      "Progress bar labels and refresh rates\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "Optimize export",
      "Minor cosmetic fixes - Export, Archive",
      "Explicitly set `refresh` for set_description_str",
      "Fix export inspect test\n\nSince `verdi export inspect` now prints the extraction information, the\ntest needs to take this into account when checking the output is\ncorrect.",
      "Update requirements files",
      "More explanatory debug messages\n\nUnify export header table headers.",
      "Minor tweaks to resetting progress bar\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "Remove unimportant .copy() statements",
      "Align django import with sqla import function\n\nSome initial logic and progress bar updates that were present in SQLA\nimport have been implemented the same way in Django import.",
      "Add --debug as common `params.options.DEBUG`\n\nUpdate it not just for the import/export click commands, but also for\nthe REST API command.\n\nKeep it hidden as default.",
      "Don't refresh new desc.s when extracting archives",
      "Being more descriptive\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "YAPF correction",
      "Pass and handle exception in _echo_error()"
    ]
  },
  {
    "pr_number": 1682,
    "commits_list": [
      "147d54c3022cb4b33a357fb14e900c790eab152e",
      "8f05c286e8a39e316a845fe3654afcf4a3451172",
      "0e9c10cbd7619105d1dc4958395671badfaededa",
      "4ac9aec6bf338c5f71caebce46542619fbde8aa9",
      "32665ac22e9f5c760fb2fcb9934055d748c7b8b7",
      "af4447ce15da2f1a38b5580d3c1f4ca5044304bc",
      "f4301144b06ddfbd25978b5213387eb23962c929",
      "ad5504fb2420ab28090b9854484caa91368ea068",
      "a1efa4eaabd4567c60d3e3f662c0316ba3700b98",
      "2c7fbe5e6790beb068482882f4124d86c32f5c95",
      "2dd73c3eb3e9d74b2811bfeb0fb8a803f889236e",
      "e7f1240da0663a3bcf4e4029240275e936455990",
      "611962c849d480aa43e2cbffdb28cfc3dd71fadb",
      "f8c6b8e0699acdb1e49eb3bf9646092cd8b1f6ea",
      "d38361bf9abb631ba4ed9c1d0ea93fa845618725",
      "b5384c05dfee2b122331ad331edc877cd336e1e1",
      "4047d14ea1d158e0690d8dffa441ba3e1852e81c",
      "b0acd2e6edb9cddd136b0e6dcda02d8171b34737",
      "b7db0dc70d3f373f69cc64b435d70f1d7cbec34c",
      "74e29d148aeebf3cffc29e749f9b9ea104bc3c7b",
      "187844265c2d00d9e13a497098ba64c5505fa4dd",
      "48848f316a815a786a933518d5f90950878ed093",
      "bf3ccef56b94ccba15aeea2f3779a44d13ca5b7d",
      "ba01a31bfc381727a698e811e925a2aba6e3eb0d",
      "c48c671bca38620b1bd8719cbfe04b60a54691bd",
      "0e9983e8f9b3b3f587fa92a179aaf7af7fad565b",
      "5f919e87e425a3411024a4f738441452e298ee81",
      "47b2bae6b922e32738ed34f9a55adb7f57949e20",
      "6af91531a696b22e48e7963d305c11aac53baf01",
      "2c4b061f64e911861fde1cded154e45b9faf46ca"
    ],
    "message_list": [
      "setup.py: make `setup.py` py3 compatible",
      "make `exec` calls Python 3 compatible",
      "travis: add python 3.6 for basic testing",
      "pre-commit-config: add python-modernize hook",
      "make imports absolute by default for Python 3 compatibility\n\nAt this point it is important to note that `.ci` and `.ci/polish` are no\nPython packages and adding the `__init__.py` will break 2to3/modernizes\nfixer. The directory of a Python script is always added to `sys.path`,\nalso in our uses of `exec()`.",
      "prospector: disable flake8/pep8\n\nmostly covered by pylint and pylint is properly configured already",
      "pylint: (re-)enable Python 3 relevant warnings\n\nthis is (partly) based on the list of language differences here:\n\n  http://python3porting.com/differences.html\n\nand PEP 3100 here:\n\n  https://www.python.org/dev/peps/pep-3100/",
      "python-modernize: enable all fixers without changes\n\nTo avoid any regressions.",
      "replace `print` keyword with `print()` function\n\nSome `print()` calls are consequently replaced by either `click.echo`\nor `echo.echo` (and variant) if either click or the echo utils package\nare already in use.",
      "replace old-style octals 0NNN with new style 0oNNN",
      "fix old-style except clauses",
      "replace old-style `a.has_key(b)` with `b in a`",
      "replace func_globals with __globals__",
      "pre-commit-config: mark optional 2to3 fixers as such",
      "use reduce() from functools instead of built-in\n\n`functools.reduce()` is the same as the `reduce()` built-in, see\n\n  https://docs.python.org/2/library/functools.html#functools.reduce",
      "replace/check current uses of `filter()`",
      "convert to new-style division where necessary\n\nthe changes by the 2to3 fixer needed to be carefully reviewed, since it\nconverted all `/` to `//`, which is definitely not what we want\n\nAfter adding `from __future__ import division`, the division operator `/`\nalways returns a float. To get the integer division, use now `//`.\n\nSince most of the time, the float division was intended, the commit\nconsists mostly of adding the import statement.",
      "Drop removed types.StringTypes",
      "replace all calls to `a.next()` with `next(a)`\n\nAlso replace some implementations where Python 3 style `__next__()` was\nimplemented based on Python 2 style `next()` with a copy of the Python 2\nimplementation instead.",
      "replace all calls to `map()` with list comprehensions\n\nmost of the time they are easier to comprehend, in some cases this is\neven an optimization since the intermediary list can be removed (for\nexample when inside `join` clauses which also takes generator\nexpressions).\n\nfor the `cod` and `icsd` there is also some refactoring of stringify\nfunctions to use `.format()` instead (increases readability further).",
      "make sure all uses of `zip()` are future-proof\n\nThe import `from six.moves import zip` resolves to `itertools.izip` on\nPython 2 and the built-in `zip` on Python 3.\n\nThis can actually give a speedup on Python 2 since a lot of `for` loops\nare now using iterators instead of full lists.\n\nThere is a nice idiom here. Instead of converting the iterator returned\nby new-style `zip` to a list and then get the first element by index\n`[0]`::\n\n  list(zip(a, b))[0]\n\none can use the `next()` function on the iterator instead::\n\n  next(zip(a,b))\n\nCare has been taken to ensure that every `for` loop over a `zip`\niterator does not modify the contents of the variables for the `zip`\n(which would have side-effects in Python 3, but worked before in Python\n2).",
      "enable the `filter` fixer after all\n\nThis makes sure that people are using the Python 3 style `filter` and\nsince we're using `six.moves` anyway we can also do it for this one.\n\nAlso fixes one usage of `filter` which was only discovered by running\n`python-modernize` using Python 3.\n\nCould be squashed with 'replace/check current uses of `filter()`'",
      "enable fixer for old-school `raise` clause to prevent regressions",
      "replace all tuples `(int,long)` with `six.integer_types`",
      "put tests involving `long` behind a `if six.PY2:`\n\nthis means we can't run the `long` fixer automatically since it doesn't\nunderstand this valid case (and the conditional)",
      "replace all calls into the reorganized urllib with six.moves\n\nThe fixer would originally replace calls like::\n\n  import urllib2\n  urllib2.urlopen(...)\n\nwith::\n\n  import six.moves.urllib.request\n  six.moves.urllib.request.urlopen(...)\n\nThis has been manually changed such that only the import line contains a\nreference to six::\n\n  from six.moves import urllib\n  urllib.request.urlopen(...)\n\nto be closer to the Python 3 variation of it::\n\n  import urllib\n  urllib.request.urlopen(...)\n\nInstead of importing the whole `urllib`, one could have imported only a\nsubmodule instead::\n\n  from six.moves.urllib import request\n  request.urlopen(...)",
      "replace references to itertools.ifilter with filter+six.moves",
      "replace all calls to `raw_input` with `six.moves.input`",
      "replace some more imports of renamed modules with six.moves\n\nthis involves:\n\n* `HTMLParser`\n* `cPickle`\n* `urlparse`",
      "make Meta Class spec Python 3 compatible via six\n\nThis uses the `@six.add_metaclass` decorator instead of\n`six.with_metaclass()` factory function. This should result in a cleaner\nMethod Resolution Order and avoid an intermediary base class.\n\n**Important**: the fixer rewrites the code using the factory function\ninstead."
    ]
  },
  {
    "pr_number": 3974,
    "commits_list": [
      "177c35349cca55c0b9fe866e51225082bef3da01",
      "11130a4d9a85978a4c15f89590f90236461da091"
    ],
    "message_list": [
      "Add possibility to pass keyword arguments to `create_engine`\n\nThe function `aiida.backends.utils.create_sqlalchemy_engine` now takes\nkeyword arguments that are passed straight to the SQLAlchemy function\n`create_engine` which creates the engine that connects to the database.\nThis utility function is used by the SQLAlchemy and by both backends\nfor the `QueryBuilder`. By allowing these keyword arguments to be passed\nand plumbing them all the way in to the `Backend` class, the parameters\nof the SQLAlchemy engine and the associated queue pool can be\ncustomized. This is not meant to be used by end users, but can be\nimportant for important use, for testing and performance reasons.",
      "Close SQLA session after every REST API request\n\nThis is needed due to the server running in threaded mode, i.e.,\ncreating a new thread for each incoming request. This concept is great\nfor handling many requests, but crashes when used together with AiiDA's\nglobal singleton SQLA session used, no matter the backend of the profile\nby the `QueryBuilder`.\n\nSpecifically, this leads to issues with the SQLA QueuePool, since the\nconnections are not properly released when a thread is closed. This\nleads to unintended QueuePool overflow.\n\nThis fix wraps all HTTP method requests and makes sure to close the\ncurrent thread's SQLA session after the request as been completely\nhandled.\n\nUse Flask-RESTful's integrated `Resource` attribute `method_decorators`\nto apply `close_session` wrapper to all and any HTTP request that may\nbe requested of AiiDA's `BaseResource` (and its sub-classes).\n\nAdditionally, remove the `__init__` function overwritten in\n`Node(BaseResource)`, since it is redundant, and the attributes `tclass`\nis not relevant with v4 (AiiDA v1.0.0 and above), but was never removed.\nIt should have been removed when moving to v4 in 4ff2829.\n\nConcerning the added tests: the timeout needs to be set for Python 3.5\nin order to stop the http socket and properly raise (and escape out of\nan infinite loop). The `capfd` fixture must be used, otherwise the\nexception cannot be properly captured.\n\nThe tests were simplified into the pytest scheme with ideas from\n@sphuber and @greschd."
    ]
  },
  {
    "pr_number": 4730,
    "commits_list": [
      "a6019007d7daa12e9f11588ba8181f2512b6daa7",
      "60297b38c3d6830fbaee95ae585c05760f735ed0",
      "9b050bdcba8ed93172b10d02de94e10bf61860ba",
      "b52ff0021d625364bd6caafe6c13dfd9373ada4e",
      "be54520c5010f1874e35a8dd387acb4e10884849",
      "c6ceb78717b8fbc1c1a04362e0f8d33cb79f180f"
    ],
    "message_list": [
      "Base `AiiDALoader` on `UnsafeLoader`, strip checkpoints on import.\n\nTo allow e.g. numpy arrays to be serialized to a process checkpoint,\nthe `AiiDALoader` is based on `yaml.UnsafeLoader` instead of\n`yaml.FullLoader`. Since this could pose a security risk when sharing\ndatabases with maliciously crafted checkpoints, the checkpoints are\nremoved upon importing an archive.\n\nFixes #3709.",
      "Rename the `deserialize` function to `deserialize_unsafe`.",
      "Fix copy behavior in `_strip_checkpoints`.",
      "Relax requirement on `pyyaml` version.",
      "Remove 'silent' keyword in call to 'export'.",
      "Merge branch 'develop' into fix_yaml_load"
    ]
  },
  {
    "pr_number": 4337,
    "commits_list": [
      "9e584dd8451e16f40ffbe98a643908c5c1ef67f1",
      "a4ec5122b9d2e79b183cd6fd73aa21a34c3f00a3",
      "4c9d44af4d8c2550444d9d528dce1b890c7772f6"
    ],
    "message_list": [
      "REST API fixes\n\n- Use node_type in construct_full_type().\n- Don't use try/except for determining full_type.\n- Remove unnecessary try/except in App for catch_internal_server.\n- Use proper API_CONFIG for configure_api.",
      "New /querybuilder-endpoint with POST for REST API\n\nThe POST endpoint returns what the QueryBuilder would return, when\nproviding it with a proper queryhelp dictionary.\nFurthermore, it returns the entities/results in the \"standard\" REST API\nformat - with the exception of `link_type` and `link_label` keys for\nlinks. However, these particular keys are still present as `type` and\n`label`, respectively.\n\nThe special Node property `full_type` will be removed from any entity,\nif its value is `None`. There are two cases where this will be True:\n- If the entity is not a `Node`; and\n- If neither `node_type` or `process_type` are among the projected\nproperties for any given `Node`.\n\nConcerning security:\nThe /querybuilder-endpoint can be toggled on/off with the configuration\nparameter `CLI_DEFAULTS['POSTING']`.\nAdded this to `verdi restapi` as `--posting/--no-posting` option.\nThe option is hidden by default, as the naming may be changed in the\nfuture.\n\nReviewed by @ltalirz.",
      "Use importlib in .ci folder"
    ]
  },
  {
    "pr_number": 4111,
    "commits_list": [
      "abd7004e81f20869f77a0e5091362006ac407647"
    ],
    "message_list": [
      "add improved search using rtd-sphinx-search\n\nincludes \"search as you type\"\n\nhttps://github.com/readthedocs/readthedocs-sphinx-search\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>"
    ]
  },
  {
    "pr_number": 4951,
    "commits_list": [
      "c3dbf09bddb56630c7e775ee75a1eda6b7ee9119",
      "eab722fbba3e7e05c0cba44d38d1371ee7c07e61",
      "78c98260efc27747ef8f3219b5b53e410dbba56d",
      "71c1ee38707df33c34b751b1f9cd0604ab012ec8",
      "cca94a5cf41b459ee3a82af5bc4184b50e543130",
      "b871855c1438a25154b133641ae67c42723e5004",
      "61a1b872c1de02dcf1fb05f152efde53717bd6ee",
      "c734bf2e5fd43016313d9029910a5aa37b421d5e",
      "054e300e1f41068911b29a38279d348fdf3b39e8",
      "04520e46e6b11c43f28de2c08366a7ce64400975",
      "9295bcfa8c503bf1dca441c807d0bf1e1100f664"
    ],
    "message_list": [
      "docs/ssh: add proxy_jump instructions",
      "transports/ssh: support proxy_jump",
      "Update docs/source/howto/ssh.rst\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update docs/source/howto/ssh.rst\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "docs/ssh: add proxy_jump instructions",
      "Merge branch 'feature/proxyjump' of github.com:dev-zero/aiida_core into feature/proxyjump",
      "docs/ssh: redo the proxy part",
      "Apply suggestions from code review\n\nCo-authored-by: Marnik Bercx <mbercx@gmail.com>",
      "Merge branch 'develop' into feature/proxyjump",
      "Merge branch 'develop' into feature/proxyjump",
      "Merge branch 'develop' into feature/proxyjump"
    ]
  },
  {
    "pr_number": 4753,
    "commits_list": [
      "8fc48ed0e41bf03b7b3f37b71cd4e9ed07a975b9",
      "e54acebe440eec805bc6604256a0a7aefb549123",
      "48393b9bcde72b38f5481a99b9011acf64a60cd0",
      "a07145066b66dc49033e190dc4367d0e7c693c2d",
      "0a03f65ad73d7dd77902dcd0076c62572cef5ca2",
      "295c92f1ba50ba077e327796fb5a56d0b2c4bf9f"
    ],
    "message_list": [
      "Add fallback equality relationship based on uuid",
      "Fix port test",
      "Fixed case delegation to superclass.",
      "Fix incompatible hash",
      "Apply PR corrections.",
      "Merge branch 'develop' into fallback_uuideq"
    ]
  },
  {
    "pr_number": 3771,
    "commits_list": [
      "b1c7270062da7c771bfa28f9aabf4e7f0b73d0bc",
      "070a78dd49c7115930948133a8f5d6d8d26f9127",
      "c7601ed68fc74a86c0baf35f9cf89411ef0e75c4",
      "5d552ea6458544f01d78cc7a43789fdd9facc2d7",
      "f6604bcd7c2ed0754287a06a7874c2b42d8f7999",
      "13af1d3e39c04383fe85bc97e31adc621f680aa7",
      "586adf7dbe6576f531d92f31ca6cb24b2c13de63",
      "c27d2bb0c09236b9d3268196ccf66348a2f551da",
      "a65a0c833ae08679e18d1c339c73282ec2f4ae4e",
      "78a34569d939ed15a8f5a18c1dae107f301a81d1",
      "f3b57024ad6cfeedec053b89b6bba51be8234ce0",
      "d9beaceb6b3e17674be6123048c66ed176e2723b",
      "0f5a60efb88ca568ab97b20830652ce3a9386e65",
      "59c64a3859500fc8b037cadcaeb8c24d4959950f",
      "ed8eaf0599a2d1d70d8c181410f726e716ff4cba",
      "364b34ce581872cad1090d0dc97cbfc7e2dd8dc9",
      "e502f4250e2f1133ca0f72881584c281651db7c3",
      "e98fd81b8972c3a30ddc3ef5b10cfdd286eed5e2",
      "46d4f2473a613689d11dec123937d5a7706bebe8",
      "420983f993361b1f945e0929a9b25c47fdb95f69",
      "9b86152d2adc2ac9ebfc1100832d5b12e58198ba",
      "21ed385094c375da679c44783909aaa5eb83b74d",
      "9b635bd2f56b8be6ebecd74167daa59bd0e3e397",
      "e03b77f298a470a361e26895ee9b544b2fff6a01",
      "f8892362db598ac8bf189d67246ea96a89c9622d",
      "79c962bdc4601e135c27a3b1b877459b01f9ec07",
      "5063076b852f86a8af1032db0bc9462cfca9965b",
      "1f13c4cc29ee11c3256a631545f20731d5fce20a",
      "240fcd025abd43b3ffa835c03ca10d21ddaf6f38",
      "b6a3c1a9f4b39599c74513fd8bcc9c71654a5bc4"
    ],
    "message_list": [
      "Add .github/CODEOWNERS file.\n\nTo automatically trigger a review request from the DM for changes\nthat modify files that are related to dependency specification.",
      "Rename script 'update_dependencies' to 'dependency_management'.\n\nTo better reflect the broader intended scope.",
      "Remove the 'etetoolkit' channel from environment.yml.\n\nAll, but one of the dependencies (sqlalchemy-utils) are available via\nconda-forge and mixing channels is generally not advisable.",
      "Implement validate command for dependency management util script.",
      "Add explicit dependency validation step to CI.",
      "Fix yaml requirement.",
      "Implement 'generate-conda-environment-file' for dependency_management script.\n\n- And remove equivalent function from validate_consistency script.\n- Update pre-commit-hooks to reflect change.",
      "Implement validation of rtd_reqs in dependency_management script.",
      "Implement validate-pyproject-toml in dependency_management script.",
      "Use pathlib for paths in dependency_management script.",
      "Use consistent exception type for dependency specification errors.",
      "Place loading of 'setup.json' file in function.",
      "Fix bug in 'validate-environment-yml' command implementation.",
      "Make implemenation 'validate-environment-yml' more robust.",
      "Improve error message for 'validate-rtd-reqs' command.",
      "Implement 'generate-pyproject-toml' dependency management command.",
      "Regenerate 'environment.yml' and 'docs/requirements_for_rtd.txt' file.\n\nRewrites requirements in 'canonical' form so that future changes are\nbetter tractable.",
      "Fix bug in 'generate-rtd-reqs' implementation.",
      "Improve 'dependency_management.py' script API consistency.",
      "Remove the seperate 'dependency' github action.\n\nThe relevant checks are carried out as part of the pre-commit checks.",
      "Ignore pylint error related to json.decoder module (false positive).",
      "Remove dependency management script main help.",
      "Implement 'pip-install-extras' command for dependency management script.\n\nTo install extra dependencies without the core package dependencies.",
      "Vendor toml package in utils directory.\n\nTo simplify the execution of the dependency management script.",
      "Add test for pip installation.",
      "Extend test for installation with conda.",
      "Add CI step to show test environment.",
      "Use 'source activate' instead of 'conda activate'.",
      "Refactor pip and conda install tests.",
      "Only run tests if installation succeeds."
    ]
  },
  {
    "pr_number": 4424,
    "commits_list": [
      "5070c5848d28f41454cd5e25cb5a8a1bf2a1ae69",
      "a721d4b712dacd39ed7b8ec5ea3f083a89ec6c86",
      "f05e9ea1d3448ea7965b2f3480db1f4ad3b76114"
    ],
    "message_list": [
      "`CalcJob`: add the option to stash files after job completion\n\nA new namespace `stash` is added to the `metadata.options` input\nnamespace of the `CalcJob` process. This option namespace allows a user\nto specify certain files that are created by the calculation job to be\nstashed somewhere on the remote. This can be useful if those files need\nto be stored for a longer time than the scratch space where the job was\nrun is typically not cleaned for, but need to be kept on the remote\nmachine and not retrieved. Examples are files that are necessary to\nrestart a calculation but are too big to be retrieved and stored\npermanently in the local file repository.\n\nThe files that are to be stashed are specified through their relative\nfilepaths within the working directory in the `stash.source_list`\noption. For now, the only supported option is to have AiiDA's engine\ncopy the files to another location on the same filesystem as the working\ndirectory of the calculation job. The base path is defined through the\n`stash.target_base` option. In the future, other methods may be\nimplemented, such as placing all files in a (compressed) tarball or even\nstash files on tape.",
      "Apply comments from review\n\nNote the big change in `execmanager.py` is simply moving the\n`stash_calculation` before `retrieve_calculation`. I did this because\nall actions are in order and stashing comes before retrieving. There are\nno actual changes in the code.",
      "Merge branch 'develop' into fix/2150/add-archive-transport-task"
    ]
  },
  {
    "pr_number": 3896,
    "commits_list": [
      "665f9226e25edea75f152b5cedbcb98ebb4dbb33",
      "08ab9da8a017a03372f54d4e8b642a48941e5bad",
      "45e5292fc0e12cfc6007b6b3fe4f590eb7c3a189"
    ],
    "message_list": [
      "add logger for verdi cli\n\nSo far, the verdi cli has been using a number of variants of\nclick.secho in order to write information to the command line\n(e.g. echo_info(), echo_warning(), echo_critical(), ...).\n\nThis automatically adds colored indicators for info/warning/... levels\nbut offers no way to control which level of information is printed.\nFor example, it is often useful to print information for debugging\npurposes, and so it would be useful to be able to specify a verbosity\nlevel when running verdi commands.\n\nHere, we use python's logging module to build a custom handler that\noutputs log messages via click.\nWe also add a click VERBOSITY option that is applied automatically\nto all verdi commands. Its default verbosity level can be controlled via\nthe new logging.verdi_loglevel configuration option.\n\nNote: The code added here is inspired by the click_log package (but\nneeded to be modified in order to fit the needs of AiiDA).\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "address reviewer comments",
      "let CMDLINE_LOGGER inherit from AIIDA_LOGGER\n\nLet CMDLINE_LOGGER inherit from AIIDA_LOGGER in order to be able to set\nthe verbosity level globally."
    ]
  },
  {
    "pr_number": 4712,
    "commits_list": [
      "2d02af92c52270af60eca5b48729128243384468",
      "9ffb8efabe956b2734df4d805ed7d4e8ac5a8113",
      "9211a0852a1aa1b15bfb382349024731b9cac349",
      "c18d169ed841c1650de91962e652cc3c7f01c581",
      "36d88f054868723ea343777cffdc2d618c31f000",
      "64b6f4b0529c81808c47b7e5e5a0382e963d37b5",
      "7e74b504f1ff8d737edfb9f77bf16befa2e66fbe",
      "363797cc6f73b72b21620f3c153a3626ad892ea5",
      "912f773dc3aa9cbfef0277c9e13f4ed6ee20ad1e",
      "94e1294b44bb046fafdad3c956fa54e0dac36204",
      "e4b7f9cc6585627f08ebf2d6465d7a6a5501717b",
      "a41c62e6591384e97a7a89b4e06d1d1f8496cb4d",
      "3fc11b6f7738a2063bd27bf5c30703c29abbe9a5",
      "e53f4502847445ef12e4a4b5d953612c116e9f9f",
      "affb604b7acca9c11cab5cfb3a18f95af22c686b",
      "477ca5f289ece6d03c61648e30a0ce01f55610e4",
      "0d535dddc2a99a491ee42971e2802bb3c6187e26",
      "6d07bf142b7c3d2a0406d5d80a38f4ef1570985b",
      "67275b6e5813d181dac547c25607f9498d36a718",
      "7e4a3e61164044787d76cb33b279c12b383204dd",
      "547f30210e82b3af24360ed21a607dd01f38b594",
      "5f2cf269ceda1d21fd02ed45a69e139fdade96f4",
      "6b026d3a961c4ad76426376e64d28b0b473f0c8a",
      "093ac73111d7a4aa963e751c30b478be643fb479",
      "2f38b6259430f9ea1b5db02d333fde5005919056",
      "d3291674c064165e147765708ae9cad1d363a279",
      "1a3a9586bd92adc0bac3892bbb61138c7f2b4106",
      "2b6370fd466bb6dfa4c4363a8eabc2eeaad8fc66",
      "e1561154db9cfbcfa05fb7b01d0d0bce1865c276",
      "13f3da585043465d38207906679ee6258328b324"
    ],
    "message_list": [
      "Initial implementation",
      "Update options.py",
      "Update config-v5.schema.json",
      "Add more tests",
      "Merge branch 'develop' into verdi-config",
      "Update nitpick-exceptions",
      "Merge branch 'develop' into verdi-config",
      "validate Config on __init__",
      "improve `ConfigValidationError`",
      "Merge `cache_config.yml` into `config.json`",
      "Merge branch 'develop' into verdi-config",
      "add `verdi config set --append/--remove`",
      "Add `verdi config caching`",
      "Improve `verdi config caching`",
      "Rename `cache_config.yml` after merging into `config.json`",
      "Merge branch 'develop' into verdi-config",
      "Improve pytest fixture use",
      "Update test_caching_config.py",
      "Merge branch 'develop' into verdi-config",
      "Apply suggestions from code review\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>\nCo-authored-by: Aliaksandr Yakutovich <yakutovicha@gmail.com>",
      "Merge branch 'develop' into verdi-config",
      "Merge branch 'develop' into verdi-config",
      "remove duplication",
      "combine `_enable_all`/`_disable_all`",
      "add warning for skipped profiles",
      "reuse config",
      "Add `rmq.task_timeout` configuration variable",
      "add validation to `profile.set_option`",
      "Merge branch 'develop' into verdi-config",
      "Update documentation"
    ]
  },
  {
    "pr_number": 4534,
    "commits_list": [
      "681803b58b40616c6a0fcfa72c28261def36f37f",
      "187725f0a6236338d1d4eee37488f0a96179cfc9",
      "6d3311ae9615381d1b98a46d88068330b298f7e8",
      "f9e265b76fae3641e86175c642bfb6db21e9e001",
      "e4772800e87f35387842f5db42ec73fe182dd7e6",
      "f9feb27069cc6623b7fac2ebaf65feba12192c0b",
      "fca7620c8129496b279b418220191e321e99eef4",
      "95b55a0edf44e91cbabb5cd0f71fba1a640d10c1",
      "b3227acfa3afe70999cfc0c7bd45dca70a8ac39c",
      "4f1cef7391e2f906b2c4393527c2e6a179188639",
      "6097589e0162a93e227bfaeb687f357c2f7a8914",
      "b3fd6bbf65ba1f871d6677c3cbb23a2d0841112e",
      "cd7a0ef6c26cd6beb713e7af65fb85b469783965",
      "b3c0ea2a4a6af42460600fdee7670e87897d9345",
      "1294692b3d3b0aa9b693ef2aa42f5595fd63c74d",
      "d4e9f3e05c7181d96244b56839b6b142ce2aa8a7",
      "68f4944752a3e33aab7df60e9b54390742ce6376",
      "3d22a7bb1611ba7846452fa9b1637a82c189dca6",
      "73c99aa46e744a56a492ffe8321c6a09a6e16268",
      "5b4c30696117af01c0ec30112affdbde8ba4ba06",
      "0f1cb3c073e1b973361c2d386ec82ca8d9aacce3",
      "78eb16113f63cd84c2471c36664bc7032ddc46db",
      "ae32cabd088fe0b00930ff4af012f18f6d830cc8",
      "7681f7902ecf70688c74ef8f53c682ca36208d61",
      "4ca4bcb7892c0f7b82927ac6d6f069d5b89f9acc",
      "3d76c152a2a107243a326ffb0e399b71b34cf9ea",
      "88d75bb9888e47edbc3309c75a13399f7a0d878d",
      "f5096855feebdb040b83637e7ec255a5cb88b83f",
      "d7021c8780f52f584160c3a4080d3d8d44ebd768",
      "2b96a3e2cc87691734e26b7817d7f94364132615"
    ],
    "message_list": [
      "add intital implementation",
      "minor updates",
      "implement zippath",
      "Update writers.py",
      "full implmentation",
      "Merge branch 'develop' into archive/export-refactor2",
      "fix pre-commit",
      "fix pre-commit (2)",
      "pre-commit fix (3!)",
      "fix `__all__`",
      "update ZipPath",
      "improve `verdi import` stdout",
      "fix pre-commit",
      "Add null writer",
      "convert `test_simple` to pytest\n\nplus add extra parameterization for file_format, and replace export_tree by export with file_format='folder'",
      "Apply suggestions from code review\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "Merge branch 'develop' into archive/export-refactor2",
      "commented out batch size --batch-size option",
      "cache at set",
      "Improve zip/tar read efficiency",
      "move compression code to archive-path module",
      "minor logging improvement",
      "minor update",
      "fix error",
      "Remove `safe_extract_tar` and `safe_extract_zip`\n\nUse `archive-path` `extra_tree` methods instead",
      "Add `zip-lowmemory` archive format",
      "change `_zipinfo_cache` default to None",
      "Merge branch 'develop' into archive/export-refactor2",
      "some extra typing fixes",
      "Merge branch 'develop' into archive/export-refactor2"
    ]
  },
  {
    "pr_number": 3613,
    "commits_list": [
      "ec6d7ea6293ac1d9dd84cbb7f1fda5f58155769f"
    ],
    "message_list": [
      "`GroupPath`: a utility to work with virtual `Group` hierarchies\n\nGroups can be used to store nodes in AiiDA, but do not have any builtin\nhierarchy themselves. However, often it may be useful to think of groups\nas folders on a filesystem and the nodes within them as the files.\n\nBuilding this functionality directly on the database would require\nsignificant changes, but a virtual hierarchy based on the group labels\ncan be readily provided. This is what the new utility class `GroupPath`\nfacilitates. It allows group labels to be interpreted as the hierarchy\nof groups. Example: consider one has groups with the following labels\n\n    group/sub/a\n    group/sub/b\n    group/other/c\n\nOne could see this as the group `group` containing the sub groups `sub`\nand `other`, with `sub` containing `a` and `b` itself. The `GroupPath`\nclass allows one to exploit this hierarchical naming::\n\n    path = GroupPath('group')\n    path.sub.a.get_group()  # will return group with label `group/sub/a`\n\nIt can also be used to create groups that do not yet exist:\n\n    path = GroupPath()\n    path.some.group.get_or_create_group()\n\nThis will create a `Group` with the label `some/group`. The `GroupPath`\nclass implements many other useful methods to make the traversing and\nmanipulating of groups a lot easier."
    ]
  },
  {
    "pr_number": 5270,
    "commits_list": [
      "7e088a9c3ea7ef4212c083cb805246736d899c77",
      "27bd603f97ce5c3fea25f487305d524c9f7f574e",
      "3c73bec3a1a684cda53981fd2922cf129b23d3ab",
      "1f64f805d1e40fdd8478b7aa0bd164c3e58788cf",
      "5e281af8640c911eb93f8b4e2a987eef907d3255",
      "5d2808862011f8247b5ff52e383d349d5a9ad4a3",
      "d723cdf75f68152320989fbe8d268335359b9e0b",
      "440b323b4a59e5139807c38fe0a3b79a66950932",
      "9a7b589075db7acd9b9fbdc700ce5400a803641e",
      "70b1674dccf340751a3a36ae3137dc16874f15cc",
      "64b6c3d82794e3d3f4c1ed1aec7bc7c3292e43f3",
      "e2467085961ebfd90a75459f3fe2cb5652750003",
      "0259d6df39cd470a2dbd26fbace43fa7cf8861f5",
      "72152e4dc658e0b95133ab76165c29b48df5e78c",
      "7e2c43b06c073c9da67ca957045206032715dec0",
      "5756e6d074ff5c23f4dcda369e742274adfccbfb",
      "d51165d267e47adc9e830f6b2b3fd3adb5ec8f30"
    ],
    "message_list": [
      "First prototype for the profile locking mechanism\n\nFrom now on aiida will keep track of all processes that request access\nto the profile by saving their PIDs inside:\n\n$AIIDA_PATH/access/profile_name/tracked/<process_id>.pid\n\nBefore returning control to the client, it will also check that there is\nno files of the form:\n\n$AIIDA_PATH/access/profile_name/locking/<process_id>.pid\n\nAs this would indicate that the profile is being locked by a process that\nrequires exclusive access for safety of its operations.\n\nFor a process to request such access it will first have to check that there\nare no active processes using the profile, so it will look at all files in\nthe `tracked` folder and compare those to the currently running processes\nin the system to check that none is actually active (it will also delete\nthose outdated tracking files in the process).\n\nThe design is as follows:\n\n - A ProcessData class was defined to store the information relevant to the\n   processes.\n\n     - It can be initialized either with a PID (of a process to be looked\n       among those currently running in the system) or with a filepath\n       (where the data of a previous process was stored, typically either\n       in the `locking` or the `tracked` folders). If none of these are\n       provided, it will load the info of the currently running process\n       that is calling the execution.\n\n     - The class also has a method to write the information to a given\n       filepath (typically in the `locking` or the `tracked` folders).\n\n - An AccessManager class to control the access to a profile.\n\n     - It can be initialized with a profile to which the class will control\n       the access to (by default it loads the one currenly in use).\n\n     - It has a couple of internal methods to distribute and modularize the\n       different tasks, but the most important externally are:\n\n         - profile_locking_context: a context that can be called to work\n           with the profile locked. It will raise LockedProfileError if the\n           profile is already locked or LockingProfileError if the profile\n           is being accessed by other processes.\n\n         - record_process_access: a method to record the accessing to the\n           profile. It is now being called in `load_backend_environment`\n           to make sure every process that loads the backend gets recorded.\n           It will raise LockedProfileError if the profile is already\n           locked.",
      "New general structure for the profile locking mechanism.\n\nIt introduces the class `ProfileAccessManager`, used to keep track and\ncontrol what system processes access the aiida profiles. It has the\nfollowing public methods:\n\n - `request_access`: to be called every time the profile is loaded\n   (for example, inside of `load_backend_environment` in the module\n   `aiida.backends.manager:BackendManager`). A file will be created\n   with the process ID as its name so as to keep track of who is\n   accessing (or has accessed) the profile.\n\n - `lock`: this context manager makes sure the process calling it is\n   the only one accessing the profile. It does so by checking that\n   there are not tracking files that correspond to currently running\n   processes and by creating a locking file that will prevent other\n   processes from accessing the profile.\n\n - `is_active` / `is_locked`: for clients to easily check if a profile\n   is being used or locked by running processes (respectively). I would\n   not recommend to use these as guards before calling `request_access`\n   or `lock` since running conditions are still possible, it is better\n   to use `try ... except ...` instead.\n\n - `clear_locks`: this removes any current lock by force-deleting the\n   locking file. This will not stop any process that was locking the\n   profile if it is still running: that process will still \"think\" it\n   has exclusive access to that profile, which could result in data\n   corruption. This method should be used with extreme care.\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "Merge branch 'develop' into lock_profile",
      "Merge branch 'develop' into lock_profile",
      "Add init to see if it helps with path recognition",
      "Fix tests and docs",
      "Small display adjustment",
      "Add tests",
      "Apply requested change for point of entry",
      "Simplified and re-organized tests",
      "Merge branch 'develop' into lock_profile",
      "Improved TestProcess class for testing",
      "Fix sub-process default profile dependency",
      "Merge branch 'develop' into lock_profile",
      "Fix aux class name and docstrings.",
      "Moved files around.",
      "Merge branch 'develop' into lock_profile"
    ]
  },
  {
    "pr_number": 2471,
    "commits_list": [
      "14c135b4f1e60c091eec0fdeeaa27288e72baae4",
      "b14e5518884d484b4f6f39ffd37c1c3dac5de033"
    ],
    "message_list": [
      "Providing a non-ORM way to add nodes to a group. This speeds up a lot the group addition procedure and import method of SQLA should benefit from it",
      "Merge branch 'release_v0.12.3' into issue_1319_for_v0.12"
    ]
  },
  {
    "pr_number": 3787,
    "commits_list": [
      "62462ddeeb6a129be12a5ba3635d7714c455718d",
      "8b58288fc3f0fe0cc743ce9e1473d68264369e36",
      "10a245ab0e15bc825154fa0bc25c4fd0c240bcb7",
      "c8cbb6e5bf8a009aa805584641e9d460c5a45b19",
      "406a3bc6ec3eca49efcb55e95bbefd82c5ca5a4c"
    ],
    "message_list": [
      "Avoid deadlocks when retrieving stdout/stderr via SSH\n\nFixes #1525\n\nThe simple shift of the line `retval = channel.recv_exit_status()`\nbelow `stdout.read()` and `stderr.read()`, while partially improving\nthe situation, is not enough (see discussion in #1525).\nThis instead reads in chunks both stdout and stderr, alternating.\nTests show that this does not hang anymore for large files.",
      "Major refactoring of the exec_commmand_wait functionality\n\nNow, I add a exec_commmand_wait_bytes that actually does the job, and this\nneeds to be implemented by plugins. The two plugins implemented by AiiDA\nalready define instead the new function.\nAlso, I improved in both the API for the stdin, so that these commands\ncan accept also bytes ot BytesIO objects, that makes more sense in general.\n\nI tried to keep the API (when calling them) backward-compatible, so using e.g.\na str/StringIO as stdin still works, and similarly `exec_command_wait` works\nas before, but adds an optional `encoding` parameter (the default is `utf-8` to\ngive the same old behavior, but give more flexibility now in using different\nencodings).\nTests have been added for these usecases.\n\nHowever, note that the interface of plugins have changed (plugins now have to define\ninstead the `exec_command_wait_bytes` function instead). The change is minimal\n(change the function name and avoid decoding the bytes into strings) but plugins\nextending the transport functionality should take care of this (and, if they drop\nthe `exec_command_wait`, they should be depending on AiiDA >=1.6; otherwise they\nshould copy the implementation from the general Transport class if they want to remain\nbackward-compatible).\n\nAlso, I changed a couple of places to call directly the _bytes version, to avoid\npossible issues with strange encodings on the remote computer.\nOthers are left in to avoid too many changes in the code, since until now noone complained\nbecause they had a weird encoding on the supercomputer.\nWhen such issue will arise we'll fix it, and thanks to this PR now the code is all ready\nto move to treat bytes directly (or use a custom encoding, that e.g. might need to be\ndefined in the `verdi computer setup` or `verdi computer configure` steps, in the future).",
      "Optimizing the parameters to have a high throughput\n\nI could get ~100MB/s when not using compression, ~33MB/s when using it\n(the second case is capped by the compression speed; the time is only\nmarginally larger of the one I get if I run, on the command line,\n`time ssh -C localhost cat /path/to/remote/file | md5sum`;\nthe first one is not as good as running md5 (via SSH) on the command line\n(which can go ~3x faster of a 4GB file)\nbut it's still acceptable for most usecases.\n\n(Note that here we're speaking of differences measurable only when\nsending hundreds of MB sent over stdout, which should not be anyway\nthe default way of transferring a lot of data).",
      "Adding comments from code review\n\nCo-authored-by: Francisco Ramirez <francisco.ramirez@epfl.ch>",
      "Merge branch 'develop' into fix_1525_hanging_ssh"
    ]
  },
  {
    "pr_number": 5507,
    "commits_list": [
      "c6ee523a737527d3f2b809e7113267d5beca22c7",
      "b833510ce0a99f871b3d7154b5dde2cf9fdf26c4",
      "e47766597bb6e0c03a58796376d95c9b7ecf3dd6",
      "63e7a2b2145ac451500e3ac3c58a372265f15217",
      "e3fade6ef11c6e568e858776d448d9c0d039ec0f",
      "4e422a4f26920b21fce629b27d20e99910c811bf"
    ],
    "message_list": [
      "Running containerized code in aiida-core\n\nThe containerized code is allowed to setting through cmdline and used in calculation.\nThe containerized engines supported and tested are docker, Sarus and Singularity.\nIt is shown below how to configure the code and running calculation.\nI will then move the example below into documentation.\n\nTest command line options for containerized code\n\ndocstring added\n\n[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nreview\n\n[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "CI test for containerized code\n\ntest docker run",
      "DOC and TEST: containerized code how to set and how to use\n\npytest fix\n\nAdd more docs in data_types\n\nUse localhost to test add-docker container",
      "Attempt of inner mpi call",
      "r",
      "Merge branch 'main' into design/containerized-code"
    ]
  },
  {
    "pr_number": 4208,
    "commits_list": [
      "05020d32e370834aedc1f37aa94326d3e9054910",
      "5782cad6c92a97ebd1d5678968972d4c09ed8ac8",
      "b378e06142ecc2ade6601512e306e8529b992e47",
      "1b0fb395274410d877c58a2b42861a28b6541e9a",
      "8cbf259c0f35488b13397f5ba42fe0fc86d04f93",
      "c06dfe40025aee9d4c4f3a712dbedbcbae9eff0e",
      "fdc8537be80419675a3d78233aa1654425048d5d",
      "c526e1715bec3afde1cb56c9579770def605b0f2",
      "d41c0b5564249a74f047b6bef80120e6801c4197",
      "771552e93bc1a9da4aea60a16ba9ec2df39864c4",
      "56ac60bd34060a653f1770c93c3967b1a8534b06"
    ],
    "message_list": [
      "docs: revise \"how to run external codes\"\n\nGive a pass to this relatively new section.",
      "Apply suggestions from code review\n\nthanks @sphuber!\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "address comments by @sphuber",
      "add back sentence on outputs",
      "Apply suggestions from code review\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "apply suggestions by @csadorf",
      "finish \"writing a plugin for an external code\"\n\n * add SimpleArithmeticAddParser to codebase (+ add one test)\n * add complete example of how to run the created calculation and parser\nplugins\n * add complete example of how to register entry points for these",
      "Apply suggestions from code review\r\n\r\nthanks a lot for the thorough read @sphuber !\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "Apply suggestions from code review",
      "remove section on running without entry points\n\nWorking without entry points just complicates things without providing\nmuch benefit to the user.",
      "Merge branch 'develop' into tutorial-running-external-codes"
    ]
  },
  {
    "pr_number": 3319,
    "commits_list": [
      "a45e62c5e787b0f5a28ebad215ea666219c9cfb5",
      "c1b3f5a52c6d7a38e60fe95cab55b55cdb8d427f",
      "5b1967c29692d6ef415d0997a6840a83a1d5740a",
      "84977bf3a1170bcbaabcb331bb5dd34bbde54188",
      "ca8f1c60c452917280cdebf1e9356812e679f41b",
      "ebd3f68a67e6d5898b775f5b30969953041452cd",
      "57d452ad77b9d2a905197e2997049fbb26bc198a",
      "8c0b53794eae7ef5dc0c622a87de8eebdfb7f6a8",
      "883ccd265512c4d3b833292b80d9240bd027addb",
      "d37cfbec7860b979ca0a20a5f88b59ca0e17e9bb",
      "4f80c13f1fb91c0c017fb525d6eea2a2d2123a39",
      "70da425f6b7606fe472b1188200ba738ab2fe875",
      "68113c26545056593b92565546c0995512dd3599",
      "597e4b4a6e508e73343b34565faa7a9a9b2cded2",
      "f593d9afaa37cf835e4be1cf8fc0fccbd62ba475",
      "8df9b90f4e56a6a240d8116e20d1e6c7c4058b60",
      "735dc2d5d2fca62c93510b9791926fb40087e477",
      "686b9c0b7f24b568cd0ea0b1fa6818c75fd980ab",
      "abcc353b0b35a8437fbd9fc3bc89c3328fe86277",
      "3b5e9ea24bca6c769331ac58ba6a2e850826b23a",
      "a6f558ccb03d36a696d8e0e0be76b8883a523b8c",
      "a9568f37c92e03a5ca9756668a78b507030e7786",
      "edc96004da02035be16ce8300eff70d7b59199d3",
      "43fea6c772930945c35621c24ec4b31e49b1b35b",
      "403fd63c019ee7fe423126f8ea90a90aa5816efb",
      "1a636d0f49059c9b3fda98d3a19ed5b47a6ccf1a",
      "61d696e9a2263f1a8d8dd1e032a900ee52821cdb",
      "2999853dddcf90f5d4be521599aabbd012421f51",
      "70c234a042e3b8401e216ba451cdb2ed32a22b47",
      "912c3c2faf11c02b00ed57fee9566fb162b0b942"
    ],
    "message_list": [
      "reorganize plugin testing code\n\nplugin testing code is split up from a single file fixtures.py (that\nactually did not contain a single fixture) over multiple files\n\n  * one for the manager, which is now called TestManager\n    (it has nothing to do with fixtures)\n  * one for test classes & runners for use with unittest\n  * one for (newly added) fixtures for use with pytest",
      "add simple test for pytest fixtures",
      "update docs",
      "add automatic backend handling\n\nboth unittest test classes and pytest fixtures will now use the backend\ndefined in the `TEST_AIIDA_BACKEND` environment variable.",
      "remove double underscore class variables\n\nfor consistency (used nowhere else in the code). + see\nhttps://stackoverflow.com/a/6930223",
      "make temporary test profile optional\n\nby specifying the environment variable TEST_AIIDA_PROFILE=profilename\none can use an existing profile instead of setting up a test profile\nautomatically.",
      "fixes for test_test_manager",
      "Merge branch 'develop' into add-pytest-fixtures2",
      "fix aiida_local_code_factory\n\n * fix name according to suggestion\n * fix bug in database query\n\nsince the code label no longer referred to the full label (including hte\ncomputer), `Code.objects.get(label='network@computer')` would always\nturn up empty-handed, causing the utility to create a new code.\n\nNow, we're searching all available codes with the `network` label and\nsimply take the first one (if it exists).",
      "fix linter and docs",
      "fix ResourceWarning issue\n\nResourceWarning introduced in python 3.2",
      "ignore undefined variable",
      "fix logic bug in TemporaryProfileManager\n\nProbably edited code at a wrong position at some point...",
      "add pytest-cov dependency",
      "fix running on existing profile\n\n * add reset function to reset default user cache in User.objects\n * simplify logic for initializing DB in ProfileManager /\n   TemporaryProfileManager\n * restrict Capturing to setup of AiiDa profile (instead of the full\ntest run using the PluginTestCase)",
      "Merge branch 'develop' into add-pytest-fixtures2",
      "fix issue when code not found\n\nThe current implementation would not throw an error if the code was not\nfound but rather an \"IndexError\" because the returned list was empty.",
      "Merge branch 'add-pytest-fixtures2' of github.com:ltalirz/aiida_core into add-pytest-fixtures2",
      "update plugin testing documentation\n\n * remove section of documentation on avoiding imports at the module\n   level - this is no longer an issue\n * add information on `TEST_AIIDA_PROFILE` environment variable\n   for controlling testing environment\n * reorder some parts of the plugin testing docs",
      "remove autouse=True from clear_database fixture\n\nWith `autouse=True`, there is no straightforward way for plugin authors\nto disable use of a fixture.\nWhile this makes sense for the `aiida_profile` fixture, we've\nencountered cases where resetting the database in between tests may lead\nto issues.\n\nThis problem is solved by keeping the fixture available but requiring\nusers to depend on it explicitly.",
      "Apply suggestions from code review\n\nCo-Authored-By: Sebastiaan Huber <mail@sphuber.net>",
      "implement suggestions by @sphuber",
      "rename TEST_AIIDA_PROFILE => AIIDA_TEST_PROFILE\n\n * rename TEST_AIIDA_PROFILE => AIIDA_TEST_PROFILE\n * rename TEST_AIIDA_BACKEND => AIIDA_TEST_BACKEND\n * add warning when setting AIIDA_TEST_PROFILE while running\n   `verdi devel tests`\n * make test profile configurable at the test_manager level,\n   so that the context manager can be used without having to mess\n   with environment variables.",
      "Merge branch 'develop' into add-pytest-fixtures2",
      "Merge branch 'develop' into add-pytest-fixtures2",
      "Merge branch 'add-pytest-fixtures2' of github.com:ltalirz/aiida_core into add-pytest-fixtures2",
      "change cache reset to work on User.obects only\n\nimplement specific cache reset method for User.objects",
      "Merge branch 'develop' into add-pytest-fixtures2",
      "rename TEST_AIIDA_BACKEND on travis",
      "add test for aiida_local_code_factory"
    ]
  },
  {
    "pr_number": 2421,
    "commits_list": [
      "b948b2aa8c2f7756edc877c85ae95219eaefee01",
      "8c063ba2b725527dd3c78f7aef51549d895436fc",
      "541bb8428cdf851df0aad54f96d978eb6aa58a31",
      "7892800ef822b4bb81d73fbd1337932ce25346c6",
      "fbc5ce2acf67e07f2f496888ee7112dda06b5115",
      "8dccfce510af89052cba5e8501373812e83a8962",
      "b6ef0e5f09a57342873eda0ee125de2ad8d49d24",
      "15e500bc8ad8e5358de0a8b7be6c27ccdaa56c7e",
      "77769dd33ba01eb9e05f90c481e7e509f53d4830",
      "c73fb4de236f2cb23dc03ab3a83737361a1f1e5f",
      "9971bc9de5acd564bb93fc7efe174af87a6c243b",
      "a949e36d036ce1a7e78f0357e0afbfaf39949cd3",
      "07f57bcda97d21c5a28040f8310ea113be0b566a",
      "df825ddcdc3b9a90c24345e1fb8648741077d014",
      "268270ae926c79d44f3b32a030f9bc47ce5e24b4",
      "51af14d695f08f1fab839e677363e55d10fe65d9",
      "aca20c01e37c577334edad10db49752169666868",
      "897a7a1dbe6a667b6dfe02eda2596e242f6cb58b",
      "3285a194344a65783665210e8c225b792618b294",
      "6411e941a4da3b0ac98bb84b036bea9b4dcd3b61",
      "ef6c839e77e9a119a81089189da724fe80b1ef3c",
      "b99f9a5cb893b3fcc75eeb625750519199498e7c",
      "d00eafe972ca546485cbf4b1085d939e29575bd4",
      "7485abe65264161e0c65ee4bf37d9e153c533d6e",
      "a0c8c52707d9272eec2e3f427f619116fa01c097",
      "c11d59d9af8e48203b04f95a4e898fbb61a22854",
      "c84a70ffac4b6b3de7765833ec0d798b8adfbc2c",
      "fe8b7e89334610af546bd88110e7d2eb7d73b6ea",
      "fc1c4915adc2910e2caef6ed3e6fe441509d78ed"
    ],
    "message_list": [
      "Implement support for process classes to `QueryBuilder.append`\n\nAny `Process` class will have an associated `ProcessNode` sub class\nassociated with it that it will use to store its provenance when ran. In\naddition with the `process_type`, determined by its specific process sub\nclass entry point, defines a set of nodes that correspond to that\nprocess class.\n\nFor example: the `ArithmeticAddCalculation` will use a `CalcJobNode` to\nstore its executions and has a process type according to its entry point:\n\n    `aiida.calculations:arithmetic.add`\n\nThis commit implements the functionality necessary that allows a user to\nappend the `ArithmeticAddCalculation` process class to a query builder\ninstance directly.",
      "Merge branch 'provenance_redesign' into issue_2400_process_in_querybuilder",
      "add subclassing for process_type\n\n + start adding tests\n\ndirty commit - first need to get a working test database",
      "minor refactoring\n\n * set_process_type => build_process_type\n * move call to build_process_type outside _set_process_type",
      "fix tests\n\n * check that warnings are raised when querying for process classes\n   that don't have an entry point (and are not built-in)",
      "Merge branch 'issue_2400_process_in_querybuilder' of github.com:ltalirz/aiida_core into issue_2400_process_in_querybuilder",
      "split qb documentation across multiple files",
      "add note on process class",
      "move teardown to testimplbase\n\nremove unnecessary code duplication by moving the teardownclass_method\nfrom the backend into the base class",
      "reset manager in teardownclass_method",
      "move file repository cleaning further up the chain\n\n + fix wrong reference in docs",
      "fix pre-commit",
      "Merge branch 'provenance_redesign' into issue_2400_process_in_querybuilder",
      "Merge branch 'provenance_redesign' into issue_2400_process_in_querybuilder",
      "Merge branch 'provenance_redesign' into issue_2400_process_in_querybuilder",
      "fix most query tests\n\none query test still failing - querying for workfunction nodes\ndoesn't seem to work anymore...",
      "Merge branch 'provenance_redesign' into issue_2400_process_in_querybuilder",
      "Merge branch 'issue_2400_process_in_querybuilder' of github.com:ltalirz/aiida_core into issue_2400_process_in_querybuilder",
      "get rid of _set_process_type\n\nuse setter from top-level Node class",
      "add test_query to pre-commit\n\n + use locally defined WorkChain to avoid problems with\n   references to ORM objects stored inside WorkChain class",
      "fixes for newer pylint version",
      "fix pylint versions\n\notherwise, pylint may update on travis but not locally\n(not even after pip install -e .[dev_precommit])",
      "hopefully final pre-commit fixes",
      "Merge branch 'provenance_redesign' into issue_2400_process_in_querybuilder",
      "Merge branch 'provenance_redesign' of https://github.com/aiidateam/aiida_core into issue_2400_process_in_querybuilder",
      "minor adaptations",
      "fix verdi daemon issue\n\nnot sure why this was not fixed automatically...",
      "fix string renames",
      "address comments by @sphuber"
    ]
  },
  {
    "pr_number": 4970,
    "commits_list": [
      "713656d4beba3bd5fe0c2a475af689464c57fc7e",
      "fc7926a340c97757480e1a09d03a907a94ace487",
      "4f9b9861f7f57aabb6c4b693e34cffa9c437bb9d",
      "0431dde1b2d952469dd62fa2f4732976442ba0fa",
      "297db0cbae3b21bc649d589a2ed0e42fdccf1bb9",
      "6530761e75b400c7c457ea5f5e572446a95ea4b9",
      "2b1fe53c42e45a1996992092c4ce71ac78aa0359",
      "91bdb979fc17465969e662f1864c3e5eb50e8706",
      "13084211d8e3b85dd40fb9f08f9d9b95cd19b2fb",
      "2903eac618500d44511ffaf66d14b885ebd501ff",
      "27005b5dc64407828697ae5ea47a00e6f0a5f83a",
      "54b9bda9dbd2f3cd6c3b18ea8f8857eda225be19",
      "b85abc61a363d30603107d26af79d0374ffd1c03",
      "ff48c17a04d1aa7a687568d4502f2385b0ecfc47",
      "9fddc580efddc97a01874b551e07cc48c1aa323f",
      "cd91b0ee9313fb755bd3cf6ab6775ae43c2a8f7d"
    ],
    "message_list": [
      "`ProcessBuilder`: Add HTML representation\n\nThe representation of the `ProcessBuilder` (i.e. `__repr__`) is\ndifficult to read since it just prints the heavily nested mapping on one\nline. Here we add a `_repr_html_` method to improve the readability\nwhen using the `ProcessBuilder` in e.g. Jupyter notebooks.",
      "Switch to using _repr_pretty_",
      "Fix pre-commit (now tested with tox!)",
      "Attempt to appease the CodeCov gods",
      "More tests",
      "Apply reviewer suggestions",
      "use Dict import; switch to using textwrap.indent",
      "Switch to using a `JSONEncoder`",
      "fix pre-commit",
      "Reorganise tests",
      "Fixed mypy (I hope)",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "more test coverage",
      "Apply reviewer suggestions",
      "Add `types-PyYAML` dependency for pre-commmit",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci"
    ]
  },
  {
    "pr_number": 4129,
    "commits_list": [
      "980d55a8ae8192c70d2ea4c7dae20f987830d1df",
      "3c5e1e5178e38e6eddd3045e9b1ad07edec616a1",
      "fa5c65355759a2bbba129caa923b21c0827ccf81",
      "b6c5b826ec8cba2800a821d4de3e28e66e15a6c3",
      "b4c4f1e1f2036025ac1fd147defebbdd89adf0eb"
    ],
    "message_list": [
      "add top level indexes",
      "undo unintentional change",
      "Move to pydata-sphinx-theme",
      "fixes",
      "Fix CSS of top bar link icons"
    ]
  },
  {
    "pr_number": 4362,
    "commits_list": [
      "27498d4506667f610fd754c4e28fe34460606d8e",
      "8ed6cd3f056d6cb688c2d671c8c6763500434ec3",
      "ae5e294899360ccf05cc32d28ab76c30aec8ad8c",
      "3146ba45ec3991ea5d8e949cd14be55b21da9b37",
      "8fc68ff6a7e853a9dd399a5c4b9aa1f83edcc248",
      "40d64ce39fdc9cdfbdccb7bacafa5ec82aba82f7",
      "21d419fb35c0bc99c544126e5d26f1360a2cafc2",
      "ce1cb761c21615168a9086e0159c7f32bc9776db",
      "92a4be5538ade572e34d7d9622e237b5ae52f8e4",
      "45b1114af9d1d9a2e3eef05c8ce3c89dd3313d85",
      "0b414aaffcca5feaf2cfc04e11361e6108d8aba4",
      "ef2b61d687a40047f36462c1cc7a7f1f99f63369",
      "9029b77eadfb7f9451d56d0386f4ad6d3b24caf3",
      "02bae99e1ea821f213361fa2640542f440dd17fb",
      "80cd7e00d5d9c1fe6cc27132f38573e184586239",
      "859210745e604cfc48afad5475e3170b5bf9f639",
      "8c5ac9c469ec447733fdc87d29b2021efe5f233f",
      "05bf66a25c2efa420aa5bd3a96119513e8e2f0ae",
      "ecf7da85874a49a4d3c136c04cf29b7007232963",
      "1572d7806548c3aa2aeeabc6bdbbf54ea6fa82da",
      "97558ec3114ddb678011452e6dcce6c5976df600",
      "b0750653ae327c1f7291c55c1a38f2dd035cbaa7",
      "957ae068c74712befb810d3cad1d815acbf8b9b5",
      "be94c5e2ac3e4c75181f03e2a7b711af340bc761",
      "c6120d5a55d50e60586a0cecb36ccd7db1efa190",
      "a883674a8379d12d72d1549af6d54e0b2c0a3d61",
      "2a11abe84756827a76e1709f1727985a6d5e470f",
      "bc2d43a9ed2320c9dbd4a3b89f2e26f494a86d63",
      "988b4e25cf168fb4e08fb3fcb9659fffa558dba3",
      "8328e07c352e3c84cb7f4a67fe1c4537eb983ce4"
    ],
    "message_list": [
      "\ud83d\udd27 Add tox configuration",
      "\u2b06\ufe0f UPGRADE: Move to tomlkit",
      "Merge branch 'develop' into add-tox",
      "\ud83e\uddea TESTS: Add initial benchmark code",
      "move to ` chrisjsewell/github-action-benchmark@v2`",
      "hotfix tests with rabbitmq",
      "temporarily disable CI",
      "group nodes [ci skip]",
      "update [ci skip]",
      "update [ci skip]",
      "update [ci skip]",
      "update [ci skip]",
      "update [ci skip]",
      "update [ci skip]",
      "update [ci skip]",
      "update [ci skip]",
      "Merge branch 'develop' into benchmark-test-cjs",
      "update [ci skip]",
      "Merge branch 'benchmark-test-cjs' of https://github.com/aiidateam/aiida-core into benchmark-test-cjs",
      "update [ci skip]",
      "update [ci skip]",
      "update [ci skip]",
      "Merge branch 'develop' into benchmark-test-cjs",
      "update [ci skip]",
      "fix rebase error [ci skip]",
      "update [ci skip]",
      "update [ci skip]",
      "update [skip ci]",
      "Merge branch 'develop' into benchmark-test-cjs",
      "update [skip ci]"
    ]
  },
  {
    "pr_number": 4093,
    "commits_list": [
      "fddb693332116c7fa5c2a6a6ff5e4e6ea94476a5",
      "022b576075caf979897290ab82a122b0e2749115",
      "a20429668a53260403e29f51fac6f616d933e50b",
      "19e6057d03e8ecaed3d0211915840bbf78924e07"
    ],
    "message_list": [
      "Add how-to interface codes section\n\nIt is now divided into how-to interface codes and how-to parse outputs.\nThe parse output section also contains a sub-section on how to handle\nerrors during parsing.",
      "Latest set of corrections.",
      "Quick-apply suggestions from code review\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "Final changes commited."
    ]
  },
  {
    "pr_number": 3944,
    "commits_list": [
      "04e01fa722a839907eb57283ed151ecb4daa8882"
    ],
    "message_list": [
      "update spglib==1.15.0"
    ]
  },
  {
    "pr_number": 3325,
    "commits_list": [
      "041ed69ea20e70b4df32462234c1c86cd6ac5c14",
      "99aa63b6b5c32c7f6e88b1e614115710a4b85360",
      "8db9ceb006e533c8b295c9d3d0c1f73c49e8216d",
      "bd00c484d115b1abae6615efddd1b0c83d1b75df",
      "9b5527de1582747b6348efd0559e5674f20c6d14",
      "2f4e20019ac528956fd66287ef9a683460185a5d",
      "2cc05fd47f5f0847d95ec9c51993df732bf4dab3",
      "57115756444e0c641586ae15590781e2f87a4053",
      "2917f8363810946686b7a9cc7f2d9427aef474ae",
      "2f5a26311655f8083cfa1c748257e20a0510d32e",
      "444153b677abd3ce9d5e66acdb677949c9b6ccf3"
    ],
    "message_list": [
      "Documentation added for delete and export features.",
      "Added a few paragraphs on how to use graph-easy.",
      "Corrections to the delete/export section applied: it was made more concise,\nwith less text and more images.",
      "Moved the import in some files because there where some problems\nwith them when making the documentation.",
      "There was a highlighted section (it was made this way as a reminder for review purposes) that I had to fix.",
      "Merge branch 'develop' into delete_docs",
      "Merge branch 'develop' of https://github.com/aiidateam/aiida_core into delete_docs",
      "Requested corrections applied:\n* The section explaining how delete/export work was renamed \"Consistency\" and moved as a subsection of \"Terms and Concepts/Provenance\".\n* A section named \"Deleting Nodes\" was included beneath \"Working with AiiDA\" with the command usage and a reference to the concept explanation.\n* A line was added in the \"Import and Export\" section to reference the concept explanation.",
      "Merge branch 'develop' of https://github.com/aiidateam/aiida_core into delete_docs",
      "Merge branch 'develop' into delete_docs",
      "Merge branch 'develop' into delete_docs"
    ]
  },
  {
    "pr_number": 4496,
    "commits_list": [
      "609876d205c98bb76733c0e7a3118a95940b64a5",
      "fc3309529701d66eb90b167c304b9af242a874ae",
      "3011bc73ca1b3778b01b794e243b7d1ef007baad",
      "0bed035aef324504babbda7662c6937321f30e8c",
      "b97c47968d65203d7d2b767d6d89f7a73e9a2ddc",
      "bd934139d7295e1dae4a0ecfa5914386abc7d914",
      "0867e09ee112d903d8d649ca4452abc65821164b",
      "b4113ca6e027d2bcac6115d6708a46b6cf7706a5",
      "f90328570f7adc49673573a090aa80ab66dba465",
      "778e31ae9dece1293969b13b566cf1dc9050e6b2",
      "a9c57c6195c843c7f77d179e3f28dbd1deb3aa7f",
      "9c6ff076678a4ed90ca1422504a6f37f522b2a68",
      "a7140eed4caa7186a8c6d5533ce9ee9f4aba81e5",
      "afa249a56f437f726d8baa04a499b2090c5dc594"
    ],
    "message_list": [
      "Revise \"Get started\" page.\n\n - Overhaul of page structure and installation flow.\n - Incorporate content from detailed installation page.",
      "Rename 'Detailed installation' page to 'Avanced configuration'.\n\nAnd remove all content covered by the get started page.",
      "Fix references.",
      "Add hint for differing commands for activation of virtual environments.",
      "Place individual setup routes on separate pages.",
      "Swap 'pip + venv' and 'Conda' tabs in system-wide install.\n\nThat makes it more likely for users to choose that method which is\noverall still better supported than Conda; the latter has no provision\nto install extras.",
      "Update WSL 1+2 installation instructions\n\n- Distinguishes between WSL 1 / 2 by how RabbitMQ is installed only\n- Adds instructions for setting up a task in Task Scheduler for\n  starting the services.\n- Removes the warning about the timezone problem, as it seems to\n  be fixed.",
      "Fix formatting of WSL install instructions.",
      "Tweak the WSL instructions.",
      "Extend docker instructions.\n\nTo retain more from the previous advanced installation instructions.",
      "Replace all 'hint' admonitions with 'tip'.",
      "Directly link to Quantum Mobile docs for VM route.",
      "Adjust the list of tested Ubuntu versions.",
      "Merge branch 'develop' into docs/issue-4254-installation-flow"
    ]
  },
  {
    "pr_number": 3618,
    "commits_list": [
      "a227bd30a1543de59cbdc970a9194a0cb805ac18",
      "588211275d8a9284a47a439a426a00dd68a3e407",
      "42d0cd56a58ce243384bcdd29dd2b982c8f052b6"
    ],
    "message_list": [
      "(re-)implement coverage reports via codecov.io\n\nfixes #3602",
      "Update README coverage badge\n\nOnly upload coverage for Py3.5",
      "Remove omissions from coveragerc and reorder tests"
    ]
  },
  {
    "pr_number": 2657,
    "commits_list": [
      "996dd01985181bff945ce39a1997aec03e92493b",
      "0b6e940b9095b21202b2508bf38ac81ddb6f1efb",
      "282b644669470b5ef5628a615c9983aa6ba56f78"
    ],
    "message_list": [
      "silence psycopg2 warning\n\nI think we have been warned enough:\n\nUserWarning: The psycopg2 wheel package will be renamed from release\n2.8; in order to keep installing from binary please use \"pip install\npsycopg2-binary\" instead. For details see:\n<http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.",
      "work around pylint bug",
      "Merge branch 'develop' into silence-psycopg2-warning"
    ]
  },
  {
    "pr_number": 4548,
    "commits_list": [
      "7f7d42075e7ca91a2c3654467e16b312f2218953",
      "cb19d49b90a260837c511455207bc96b337e0689",
      "c778a7d0fe4b3208225f5b75bcd7258ef9593a92"
    ],
    "message_list": [
      "docs: include ipython magics from source code\n\nIncluding the snippet to register the AiiDA ipython magics from the\naiida-core codebase instead of the (already outdated) copy-pasted\nversion.\n\nAlso revisit the corresponding section of the documentation, starting\nwith the setup, and removing some generic information about jupyter.",
      "add tests for IPython extensions\n\n * add test for AiiDA IPython extension\n * move some additional lines from the registration snippet to\naiida-core (where we can adapt it if the IPython API ever changes)\n * rename misnomer `load_ipython_extension` to\n   `register_ipython_extension` with deprecation far in the future",
      "apply suggestions from code review\n\n + try suggestion by @chrisjsewell\n + fix pre-commit failure"
    ]
  },
  {
    "pr_number": 3466,
    "commits_list": [
      "425d39d11470d920bd49a5974a89f742b344e122",
      "3f3dc0869c0587b5591bd06fb7c4bb8c68c41941",
      "69f40372bdb1cb82c0a7bae1fd3350073872a1e4"
    ],
    "message_list": [
      "Emit a warning when input port specifies a node instance as default\n\nUsing mutable objects as defaults for `InputPorts` can lead to\nunexpected result and should be discouraged. Implementing this for all\ntypes, including python builtins such as lists and dictionaries, is\ndifficult and not the most pressing problem. The biggest problem is\nusers specifying node instances as defaults. This will cause the backend\nto be loaded just when the process class is imported, which can cause\nproblems during the building of documentation and with unit testing\nwhere instances in memory to deleted nodes can be referenced, among\nother things.\n\nTo warn users for these complications, we override the `InputPort`\nconstructor to check the type of the default and emit a warning if it\nis a node instance.\n\nThe `CalcJob` implementation had to be adapted to conform with the new\nrequirements as the `mpirun_extra_params` and `environment_variables`\nmetadata options were using mutable types for their default. Just as\ncertain test process classes, the defaults are now prefixed with the\n`lambda` keyword.",
      "Merge branch 'develop' into fix_3143_warn_mutable_port_defaults",
      "Merge branch 'develop' into fix_3143_warn_mutable_port_defaults"
    ]
  },
  {
    "pr_number": 5184,
    "commits_list": [
      "37b79ad3f4dd990b21584a4c5ee67271596a5c7c"
    ],
    "message_list": [
      "`Code`: add `validate_remote_exec_path` method to check executable\n\nA common problem is that the filepath of the executable for remote codes\nis mistyped by accident. The user often doesn't realize until they\nlaunch a calculation and it mysteriously fails with a non-descript\nerror. They have to look into the output files to find that the\nexecutable could not be found.\n\nAt that point, it is not trivial to correct the mistake because the\n`Code` cannot be edited nor can it be deleted, without first deleting\nthe calculation that was just run first. Therefore, it would be nice to\nwarn the user at the time of the code creation or storing.\n\nHowever, the check requires opening a connection to the associated\ncomputer which carries both significant overhead, and it may not always\nbe available at time of the code creation. Setup scripts for automated\nenvironments may want to configure the computers and codes at a time\nwhen they cannot be necessarily reached. Therefore, preventing codes\nfrom being created in this case is not acceptable.\n\nThe compromise is to implement the check in `validate_remote_exec_path`\nwhich can then freely be called by a user to check if the executable of\nthe remote code is usable. The method is added to the CLI through the\naddition of the command `verdi code test`. Also here, we decide to not\nadd the check by default to `verdi code setup` as that should be able to\nfunction without internet connection and with minimal overhead. The docs\nare updated to encourage the user to run `verdi code test` before using\nit in any calculations if they want to make sure it is functioning. In\nthe future, additional checks can be added to this command."
    ]
  },
  {
    "pr_number": 5123,
    "commits_list": [
      "218af2f2796f13f3bc07332ad47b0a30626ffd53",
      "0c839e3d77616612de2f5aa0ff90f09764bbb579",
      "a7a27cae918adbd998826d79c7b061c0367c1404",
      "7e0cb192f5227f2075c1c9c646fcd8dbc01cb4b1"
    ],
    "message_list": [
      "\ud83d\udc1b FIX: Ensure Container DB always closed after access",
      "Add additional context managers",
      "Update disk_object_store.py",
      "Remove the `container` public property\n\nThis should not be used externally. The two tests that were using it\nshould also not have to use it directly."
    ]
  },
  {
    "pr_number": 3686,
    "commits_list": [
      "d6afc369f4c227efc69b4469b0cbb57d418b117b",
      "367dc0c8a741f20f6fffeccac2256851d2638092",
      "cadad4cf0fb930fba58a809f71e50caaa9202c7f",
      "19d9f5b3c5a09dc3f86748f57bfc0804f678da6a",
      "8ec079c1b17dbdc372ffe80ad53f0fffc444f427"
    ],
    "message_list": [
      "Add feature AGE\n\nThe AiiDA Graph Explorer (AGE) is a general purpose tool to perform\ngraph traversal of AiiDA graphs. It considers AiiDA nodes and groups\n(eventually even computers and users) as if they were both 'graph nodes'\nof an 'expanded graph', and generalizes the exploration of said graph.\nThe 'rules' that indicate how to traverse this graph are configured by\nusing generic querybuilder instances (i.e. with information about the\nconnections but without specific initial nodes/groups and without any\nprojections). The initial set of nodes/groups is provided directly to\nthe rule, which then will perform successive applications of the query,\neach on top of the results of the previous one. This cycle is repeated\nfor a specified number of time, which can be specified to be 'until no\nnew nodes are found'.\n\nThe current implementation works with the following (public) classes:\n\n* Basket: generic container class that can store sets of nodes, groups,\nnode-node edges (aiida links) and group-node edges. These are the\nobjects that the rule-objects receive and return.\n\n* UpdateRule: initialized with a querybuilder instance (and optionally a\nmax number of iterations and the option to track edges), it can then be\nrun with an initial set of nodes to obtain the result of the accumulated\ntraversal procedure described by the iterations of the query.\n\n* ReplaceRule: same as the update rule, except that at the end of the\nprocedure the returned basket contains not the accumulation of the\ntraversal steps but only the nodes obtained during the last step. This\nis rule is not compatible with the 'until no new nodes are found' end\niteration criteria.\n\n* RuleSequence: this can concatenate the application of different rules\n(it basically works like an UpdateRule that iterates over a chain of\nrules instead of a single querybuilder instance).\n\n* RuleSaveWalkers and RuleSetWalkers: rules that can be provided in a\nchain of rules given to a RuleSequence to save a given state of the\ncurrent basket (Save) that can later be used to overwrite the content\nof said working basket (Set). This is useful in the case where one might\nneed to do two operations 'in parallel' (i.e. on the same set of nodes)\ninstead of doing the second on the results of the first one.\n\nCo-Authored-By: ramirezfranciscof <ramirezfranciscof@users.noreply.github.com>",
      "Add feature traverse_graph and others\n\nThe function traverse_graph works as a simplified interface to interact\nwith the AGE that also removes the need to manually handle the basket\nand the querybuilder instance:\n\n * The price to pay for hiding the basket is that this function can only be\n   used with sets of nodes and links (so, no groups).\n\n * The price to pay for hiding the querybuilder is that complex traversal\n   procedures can no longer be specified, the user simply defines which\n   links can be traversed forwards and which backwards, and this criteria\n   is then applied in every iteration (so one could not, in a single call,\n   search only for all called calc nodes of the  called work nodes of an\n   initial workflow node, as one will also obtain the calc nodes directly\n   called by that initial workflow).\n\nBesides the starting nodes (pks) and links, the user can also provide the\nnumber of max iterations desired (which by default is None, which means\n'until no new nodes are found') and a boolean that indicates if the links\n(edges) should be returned.\n\nAdditionally, two other interfaces are included for ease of use when\ndeleting and exporting. These functions only take the starting set of\npks and the rules provided by the user (as 'rule_name_dir' = False/True)\nand can automatically check if the rule is toggable, set defaults (using\naiida.common.links.GraphTraversalRules), and also parse the ruleset into\ntwo lists with the links for forward and backward traversal. They will\nreturn a dictionary containing the 'nodes' list, the 'links' list (if\nthis was requested, else this will contain `None`) and a dict with the\nway in which all the rules were applied (using the following format:\n'rule_name' = True/False).\n\nCo-Authored-By: Leonid Kahle <leonid.kahle@epfl.ch>",
      "Add traverse_graph / AGE as engine for node delete\n\nThe node deletion function now uses the get_nodes_delete function (with\nthe traverse_graph underlying interface using AGE as main engine) to\ncollect the extra nodes that are needed to keep a consistent provenance.\nThe procedure is not very different than the one that was initially\nimplemented so no significant performance improvement is expected, but\nthis is an important first step to homogenize graph traversal throughout\nthe whole code.",
      "Add traverse_graph / AGE as engine for export\n\nThe export function now uses the get_nodes_delete function (with the\ntraverse_graph underlying interface using AGE as the main engine) to\ncollect the extra nodes that are needed to keep a consistent provenance.\nThis is performed, more specifically, by the 'retrieve_linked_nodes'\nfunction. Whereas previously a different query was performed for each\nnew node added in the previous query step, this new implementation\nshould do a single new query for all the nodes that were added in the\nprevious query step. So these changes are not only important as a first\nstep to homogenize graph traversal throughout the whole code: an\nimprovement in the export procedure is expected as well.",
      "Add traverse_graph / AGE engine for visualization\n\nThe graph visualization feature now uses the traverse_graph function\n(with AGE as the main engine) to collect the requested nodes to be\nvisualized. This was implemented in the methods of the graph class:\npreviously, `recurse_descendants` and `recurse_ancestors` used to\nwork by calling `add_incoming` and `add_outgoing` many times, which\nin turn have to load nodes during the procedure. Now these are all\nindependent and they all call the traverse_graph function, so the\ninformation is obtained directly from the query projections and no\nnodes are loaded. So these changes are not only important as a first\nstep to homogenize graph traversal throughout the whole code: an\nimprovement in the visualization procedure is expected as well."
    ]
  },
  {
    "pr_number": 4328,
    "commits_list": [
      "a26488f506099e0ddc6c9c5fc881b3e233184042"
    ],
    "message_list": [
      "`Group`: add support for setting extras\n\nAdd extras column to the database models for groups, with an empty\ndictionary as a default. Add the `ExtrasBackendEntity` mixin class\nto the `BackendGroup` to make sure the group backend classes for\ndjango and sqlalchemy have the extras methods. Add the\ncorresponding extras methods to the frontend `Group` class.\n\nFor SqlAlchemy: instead of using the constructor to set the default\nvalues of the attributes and extras for `DbNode` and `DbGroup`, use\nthe Column declaration to do so. Generate the required migration\nfiles and make adjustments where necessary.\n\nAlso fix some typos in the utils.py modules and add exception\nchaining for exceptions raised straight from the handling of a\nprevious exception."
    ]
  },
  {
    "pr_number": 3134,
    "commits_list": [
      "09c67a8059b95321150a573bf24ae7e1ed6ae53e",
      "52b944722ef8a238b52389acbfcd74f23aa2fd4a"
    ],
    "message_list": [
      "Add data migration for legacy process attributes\n\nAttribute keys that are renamed:\n\n  * `_sealed` -> `sealed`\n\nAttribute keys that are removed entirely:\n\n  * `_finished`\n  * `_failed`\n  * `_aborted`\n  * `_do_abort`\n\nThe `sealed` key is still used, the key has just changed, so existing\nattributes are migrated. The last four attributes are left over control\nattributes of one of the first process based engines, but are obsolete\nand are therefore deleted to clean up some space.\n\nFinally, any process nodes that still do not have a sealed attribute,\nget one, because these most likely correspond to legacy calculation\nnodes that were run before the sealed concept existed. However, any\nnodes that have a `process_state` that is active, are excluded as they\ncorrespond to actual active processes and are therefore not yet sealed.",
      "Up EXPORT_VERSION to 0.7\n\nAdded now illegal attributes (migration 0040) to process nodes in all\n\"migrate\" fixture export archives.\nMigration 0040 and the removal of 'Attribute' and 'Link' entities from\nmetadata.json and the import functions are the reason for the new\nEXPORT_VERSION.\n\nThe dependency on aiida-export-migration-tests has been upped to 0.7.0,\ntogether with the new release.\n\nDue to migration 0040, it is now mandatory for ProcessNodes to be sealed\nbefore they can be exported, hence this is checked in `export_tree`.\nBecause of this all tests including ProcessNodes has been updated."
    ]
  },
  {
    "pr_number": 4767,
    "commits_list": [
      "ae6d7a89c398ebfa45a28cddf98ec90d2570d2c7",
      "1f702b0a09a1ea4f0a00396f62a67114091de0e3",
      "0e4a7f612a5c06e987505bd9e8c86e32e73c06b3"
    ],
    "message_list": [
      "\ud83d\udc4c IMPROVE: Garbage collect on process termination",
      "fix pre-commit",
      "increase async sleep to 1"
    ]
  },
  {
    "pr_number": 4707,
    "commits_list": [
      "774968713bddb04038084b8dde32e7eb5d6327b0",
      "3ed7ca05fb133cfbb13812ad192cd12037a901c5",
      "42882b56f19bcb7eed75ffc3bf1dd1862e316157",
      "dd51a4d1fec7f1d50fdfa872b3e07e371ac14fbc",
      "b116c3420f3a0aeaee0bfba52ca18df4d5874b5a"
    ],
    "message_list": [
      "Allow setting `rerunnable` from `metadata['options']`.",
      "Add tests for `rerunnable` option in scheduler plugins.",
      "Add documentation for the `rerunnable` flag.",
      "Fix the direct scheduler test.",
      "Replace manual caplog handling with `aiida_caplog` fixture."
    ]
  },
  {
    "pr_number": 4277,
    "commits_list": [
      "b21b33f01be67723d9fa65fd60ba6a8ee264836f",
      "b29e9555130e99aa38304d331931d2e80f7ddea9"
    ],
    "message_list": [
      "REST API: Modify assumptions for `process_type`\n\nThe `process_type` attribute has changed over the years; currently it\nmust have some descriptor for processes and be None for data types.\nApparently this has not only been the case, and thus old databases may\nhave both data and process nodes with either empty strings ('') and/or\nNone entries in their `process_type` attributes.\n\nAdditionally, there were some problems with how the unregistered entry\npoints were considered that made it impossible to query for them.\n\nIn order to consider all of this when filtering and doing statistics,\nit has been decided to:\n\n1) Group all instances of a given node_type that have either '' or None\nas their process_type in the same `full_type` (`node_type|` ) and hence\nalways query for both when the `process_type` is missing.\n\n2) Remove the `aiida.descriptor:` and the `no-entry-point` from the\n`process_type` part of unregistered processes. This was interfeering\nwhen the `full_type` was given to return the filtering options to query\nfor these processes.\n\nTests were adapted to test this new compatibility aspects.",
      "REST API: Add full_types_count as new entry point\n\nThis feature returns a namespace tree of the available node types in the\ndatabase (data node_types + process process_types) with the addition of\na count at each leaf / branch. It also has the option of doing so for a\nsingle user, if the pk is provided as an option."
    ]
  },
  {
    "pr_number": 4869,
    "commits_list": [
      "40cb067505ca31010540fe9badabb210911891c7",
      "dc9ec6f91e0b23608c58ece6e3283a13780dcf47",
      "f0f72a8927f700fd7bee6f0a75ec0469db6e0ec2",
      "146e436965897efd9bfe5ecee38f3e8d495e5c08",
      "13f95caac724fbad1d268d0e7258e97a3bf0503b",
      "28ee0cea2224642b2e747f516204007d09de8096",
      "5ab86cca5fc2070937cfa90b1a479d06878e68dd",
      "2bb18e015c32cdb23e6aab2476e9b9a217194aa3",
      "f3c335965b301af97b2d91e50b67ae57d3c5cacc",
      "fa8b24379f5731cef43ca1648d18142d0c55de97",
      "5407799b602837252400f918118d328ac1d3fda0",
      "f1aab1fd5015e9b4d203845d57136fca2aed6281",
      "42455ac4319f8de399048e1f87acc0d99852130a",
      "c8ee9e605663fbabc9f7f2484fa0f88668a3b322",
      "7295bb64e8bf1f6c5a6dfa75dd3764c45b9a603a",
      "372f5cbf057dc22f26e4c200d64f1a3f2ffcd2cc",
      "fac7c75eeaec7f5f9a1ffd6bef6b200f117231af"
    ],
    "message_list": [
      "\ud83d\udd00 MERGE: `master` -> `develop` (#4844)",
      "\ud83d\udcda DOCS: Minor fix to Changelog (#4845)\n\n\ud83d\udcda DOCS: Minor fix to Changelog",
      "\ud83e\uddea TESTS: Bump `urllib3` 1.26.3 -> 1.26.4 (dependabot) (#4849)",
      "FIX: dependency conflict (#4851)\n\nThe following dependecies have been pinned:\r\n\r\n* Constrained jupyter-client to <6.1.13: to avoid conflict for nest-asyncio required by\r\nboth plumpy (dependency) and  jupyter-client (sub-dependency).\r\n\r\n* Constrained pytest-cov to <2.11: to avoid conflict for coverage required by us with\r\nconstrain <5 and pytest-cov.\r\n\r\nAdditionally, a utility function to analyze the package dependencies has been added\r\nas part of this fix for future use.",
      "DOCS: add help in intro for when quicksetup fails (#4838)\n\nSometimes AiiDA will have problems identifying the PostgreSQL configuration,\r\nand when this happens, `verdi quicksetup` will fail. In case the user runs into\r\nthis, the following additions have been made:\r\n\r\n* A section in the troubleshooting FAQ explaining the alternative procedure\r\n* A warning in the corresponding sections of the intro that points to the FAQ",
      "\ud83d\udc4c IMPROVE: Add `account` option to `LsfScheduler` (#4832)\n\n\r\nCo-authored-by: Chris Sewell <chrisj_sewell@hotmail.com>",
      "DOCS: Add \"How to extend workflows\" section (#4562)\n\nSplit the section on \"How to run multi-step workflows\" into one that focuses on running\r\nthe workflows and one on \"How to write and extend workflows\". Add a subsection on\r\nhow to extend workflows to the second.\r\n\r\nThis subsection continues with the `MultiplyAddWorkChain` example and covers:\r\n\r\n* How to submit the `MultiplyAddWorkChain` within a parent work chain.\r\n* How to expose the inputs using the `expose_inputs` method and a proper namespace.\r\n* How to use the exposed inputs with the `exposed_inputs` method.\r\n* How to expose outputs and pass them to the outputs of the parent work chain.",
      "CI: install default `postgresql` instead of specific version (#4860)\n\nCI: Use `postgres` since update to Ubuntu Focal 20.04\r\n\r\nThe CI configuration uses `ubuntu-latest` which recently became Ubuntu\r\nFocal Fossa (20.04) which no longer has `postgres-10` available, but\r\ninstead provides `postgres-12`.",
      "Revert \"CI: Notify slack on failure of the test-install workflow. (#4690)\" (#4862)\n\nThis reverts commit dcc80618368f405c02c9eaa6d122177e78d70a4b.\r\n\r\nThe slack integration didn't work as intended and I currently don't have the time to properly fix this.",
      "CLI: Use DB `proxy_command` for `verdi calcjob gotocomputer` (#4761)\n\nCurrently the `verdi calcjob gocomputer` command only works for\r\ncomputers accessed via a proxy in case:\r\n\r\n* the `Host` is set up in the `.ssh/config` file. \r\n* the `Host` is the same as the `HostName`, also configured as the host\r\n  name for the AiiDA computer.\r\n\r\nIf both conditions are met, the `verdi calcjob gocomputer` command\r\ngenerates an `ssh` command (via `SshTransport.gotocomputer_command()`)\r\nthat can rely on the configuration in the `.ssh/config` to use the proxy \r\ncorrectly, because the generated command is executed via `os.system()`\r\nand hence it parses the `.ssh/config` file. However, since typically users set\r\nan alias `Host` instead of using the full `HostName`, this will often not work.\r\n\r\nHere we add the `proxy_command` that has been configured for the\r\n`Computer` in the database to the ssh command string that is created for \r\n`verdi calcjob gotocomputer`. This fixes the `verdi calcjob gotocomputer`\r\ncommand for computers which are accessed by a proxy in case the above\r\nconditions are not met and the user has configured the `proxy_command`\r\nfor the `Computer`.",
      "DOCS: Update ssh proxycommand instructions (#4839)\n\nIt was not obvious how to handle cases where the SSH key for the proxy\r\nserver needs to be specified.\r\n\r\nAlso, remove the note regarding redirection.\r\nThis was relevant for the old approach of using netcat directly and is\r\nmore confusing than helpful without it.",
      "build(deps): bump django from 2.2.18 to 2.2.20 in /requirements (#4854)\n\nBumps [django](https://github.com/django/django) from 2.2.18 to 2.2.20.\r\n- [Release notes](https://github.com/django/django/releases)\r\n- [Commits](https://github.com/django/django/compare/2.2.18...2.2.20)\r\n\r\nSigned-off-by: dependabot[bot] <support@github.com>\r\n\r\nCo-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>",
      "FIX: respect nested output namespaces in `Process.exposed_outputs` (#4863)\n\nThe `exposed_outputs` method was detecting nested namespaces in the\r\ndictionary of outputs of a node by using the namespace separator\r\ncharacter of the port namespace class. However, this character, which is\r\nthe `.`, gets converted to `__` when the link is stored in the database.\r\nThis transformation is necessary since the `.` is a reserved character\r\nto enable attribute based dereferencing, e.g.\r\n\r\n    node.outputs.some_output\r\n\r\nSince link labels can contain underscores, we use a double underscore\r\nwhich, together with the guarantee that link labels don't have leading\r\nor terminating underscores, can uniquely determine the namespaces in any\r\ngiven link label.\r\n\r\nThe solution is to not manually reconstruct the namespaces in the output\r\ndictionary but simply use the `get_outgoing().nested()` method which\r\ndoes it for us.",
      "NodeLinkManager support dot separated namespace attributes retrieving (#4625)\n\nThe `NodeLinksManager` is used by the ``inputs`` and ``outputs``\r\nattributes of the ``ProcessNode`` class to allow users to quickly access\r\ninput and output nodes by their link label. Nested namespaces in link\r\nlabels are converted to double underscores before it is stored in the\r\ndatabase because it can only store a flat string. This is an\r\nimplementation detail, however, and the user should not have to know\r\nabout it and should be able to use the nested namespace. However, up\r\ntill now, one had to pass the link label as stored in the database, i.e.\r\n\r\n    node.inputs.nested__sub__namespace\r\n\r\nAfter this commit, it is now possible to use the more intuitive\r\n\r\n    node.inputs.nested.sub.namespace\r\n\r\nFor backwards compatibility, the old flat link is still supported but\r\nwill emit a deprecation warning.\r\n\r\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "Fix `aiida.cmdline.utils.decorators.with_dbenv` always loading the db (#4865)\n\nThe premise of the method `with_dbenv` is that it will load the profile\r\nand corresponding database environment if not already loaded. However,\r\ndue to a bug in `load_backend_if_not_loaded` it was actually *always*\r\nloading the environment for every call, even if the database env was\r\nalready loaded, leading to very expensive calls for nothing.",
      "Fix `IndexError` in `override_log_formatter_context` utility (#4873)\n\nIn the `aiida.common.log.override_log_formatter_context` utility\r\nfunction, the formatting of all current AiiDA handlers is temporarily\r\nchanged for the duration of the yield, and then reset. The problem is\r\nthat the function assumed that the number of handlers would not change,\r\nwhich is not necessarily the case. Additionally, it also relied on the\r\norder of the handlers, which isn't guaranteed either.\r\n\r\nThe solution is to simply cache the handlers' formatters as a dictionary\r\nand directly use those handler references to reset the cached formatter.\r\nNote that we copy the `handler.formatter` return value because it may be\r\nreturned by reference and we want to keep the original formatter if it\r\nis changed during the yield. Note also that it doesn't matter that the\r\nAiiDA logger may have gained additional handlers during the yield, after\r\nwe recorded the cached formatters, but that is ok, because we didn't\r\ntemporarily change their formatter either.",
      "Release `v1.6.2`"
    ]
  },
  {
    "pr_number": 5017,
    "commits_list": [
      "9537ef4df2f6155d7186f0b8adbc953e8d0d89e3"
    ],
    "message_list": [
      "Add the `JsonableData` data plugin\n\nThis data plugin is designed to make it easy to wrap instances of\narbitrary Python classes and store them as a node in the database. The\nonly requirement is that the class implements the `as_dict` method,\nwhich should return a dictionary that represents the instance which is\nJSON-serializable. The latter means that it can be serialized by the\n`JsonEncoder` of the `json` built-in module.\n\nIf the class also implements a `from_dict` method, that can consume the\ndictionary that is returned by `as_dict`, to reconstruct an instance,\nthe original instance that is wrapped by the node (loaded from the\ndatabase), can be recovered through the `obj` property."
    ]
  },
  {
    "pr_number": 4965,
    "commits_list": [
      "a0543a0974f715c3a9fc1e847c64a142d0c4715d",
      "3bfc85636314bb75cfcc9fef61be4188ae8485f9",
      "5ee2fa6bce869a095d6f21cf560fde1338964298",
      "2784998797531e4910fa86ce8852f55cbb7934d6",
      "766709db662d8bd04b5098a4d8202bf0d550efaf",
      "df3bfd41149eee1acc8225e6540c06ec4ee0cb5b",
      "5b5b38ed0404213bf13b61ba799a2e6b6613851d",
      "a4ec0d1e1f15ed46a8880f87172cfe82dc4195d9",
      "9fa51eaf212fd48c905fe62685bf33a962fd6bee",
      "fd071a996148cae6fe213f0c3bb8a802953403fd",
      "aa5908959a6ddbde52aaf09a7073045fdabb1de3",
      "2501cde0b25683491e0f4f126f8d510fbe5a606d",
      "fa7d5ffeca3a1341466680352fc9a22351d8cede",
      "8cb095109143cc708366f91e9359a2dfb567a6f8",
      "7a6a939ba3a81b4c10c3a5b45a108f1f0d936519",
      "2402bf7c217577335e7b97f698ba453388717ad4",
      "1432dcf28c44331f523d8efecd6b9bf3b7b20f00",
      "50376650efed4a6ba0aaca548cca6f8b43cd4feb",
      "f0aa923a0380dee1e45af5c04885300ac346a5a5"
    ],
    "message_list": [
      "Add proposal for repository maintain functionality\n\nIt would in principle consist of a single verdi command with one\noptional flag and one optional argument:\n\n  > verdi repository maintain [--live] [--pass-down <STR>]\n\nThe base command will perform all the available and necessary\nmaintainance operations but will require to lock down the profile\nin order to proceed (it will ask the user to confirm before).\n\nOptionally, the `--live` option can be passed so that only the\nmaintainance operations that are safe to perform while actively\nusing AiiDA will be applied.\n\nFinnally, the user can also `--pass-down` a backend specific string\nthat will be interpreted by the backend method for even further\ncustomization of the process.",
      "Apply PR corrections.",
      "Apply PR corrections - moving control from storage to backend",
      "Apply PR corrections after rebase",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "Add corrections from PR - method naming",
      "Apply PR corrections - Method defaults",
      "Fix test problems",
      "Modifications due to PR review\n\n - Added a logger for immediate info / feedback\n - Modified returned variables from the methods to be more\n   \"machine readable\" (dicts).",
      "Fixing auto generation",
      "Re-adapt cli tests",
      "Major re-structure of the features",
      "Merge branch 'develop' into repocli",
      "Fix control tests",
      "Apply suggestions from code review\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "Apply latest corrections from PR",
      "Last PR corrections",
      "Merge branch 'develop' into repocli",
      "Add mypy ignores"
    ]
  },
  {
    "pr_number": 3722,
    "commits_list": [
      "ed72892a20dbe1ccba55d583b1c3587d64442e01",
      "ecc19e0cdbe9a91ee5fb4b32889452d9e5515570",
      "7e599dc8eb564922882fd5b53bab34222c9d6f0d",
      "d0e076fc2812e57404bd785f0264a999fbf452fc",
      "c2178d1c5421a6eafadad271e57496bbfc6d2371",
      "062b7aa3acab3eac9e1474b4af236a6f629f0e17",
      "812c4cf57abbe5167633d5f561ac13ac0452a4b9",
      "a06e06b823ded577fd79818d381f8df9dbb3d21c",
      "5d8971b34d058f062a029c6a73ae24ed7a0a34b8",
      "6047d2a201c08fee12cc1be285cbe5fba2278efb"
    ],
    "message_list": [
      "Build docker image based on aiida-prerequisites",
      "Adress the comments by @ltalirz and @sphuber",
      "Replace verdi setup with verdi quicksetup",
      "Add docker action",
      "Fix typos and improve comments\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "trying to fix docker action",
      "Fix github actions\n\n* Use `wait-for-services` script to wait till all scripts in /etc/my_init.d executed.\n* Replace `verdi profile list` with `verdi provile show default`",
      "Update .github/workflows/ci.yml\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update .github/workflows/ci.yml\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "implement suggestions of @sphuber"
    ]
  },
  {
    "pr_number": 4301,
    "commits_list": [
      "e804d33682ba3b68c491896751ce0f3fb5d2e7fd",
      "6ed6dc7f02a738d6888df146da22ace2ce02fcd9",
      "726c7b4a66e91ff519460138f99788bc1e74f7b4",
      "0ff1ebfb95a51fce4a9e69b874c8978e55c0d2b5"
    ],
    "message_list": [
      "CI: Update test-install workflow to include Python 3.9.",
      "Add Python 3.9 to tox configuration.\n\nAnd add requirements file for Python 3.9.",
      "Add 'Python :: 3.9' classifier trove.",
      "Merge branch 'develop' into ci/python-39"
    ]
  },
  {
    "pr_number": 5142,
    "commits_list": [
      "ded9b56ab629a5c46a99e11dbf5e027fd2ed84e0",
      "2c8f7f33278dcad7f3033b50a7b08c9948be8eae"
    ],
    "message_list": [
      "`Dict`: implement the `__eq__` method to compare plain dictionary\n\nThe `__eq__` method is overridden to not only let a `Dict` node compare\ntrue to itself, but also to the plain dictionary that represents its\nvalue. That is to say, the following:\n\n    node = Dict(dict={'a': 1'})\n    assert node == node.value\n\nnow passes the assert. This brings the behavior of `Dict` on par with\nall the other base types, `Bool`, `Float`, `Int`, `Str` and `Bool`.",
      "`List`: register the class with the `to_aiida_type` serializer\n\nThis will allow users to pass a plain list as input for a port that\naccepts `List` nodes and specified the `to_aiida_type` dispatch as\nserializer. The value will be automatically converted to a `List` node.\nThis brings the functionality for `List` on par with the other base type\ndata node classes."
    ]
  },
  {
    "pr_number": 4832,
    "commits_list": [
      "73dac1b0ab0aedb20d73adc0c317e93d1edf0718",
      "2f86d6963fd13f79ef6f23fc6b89b346fa5dd104",
      "63d78eb63b8ebf04be74cf5bc3bf3141122c3cf8",
      "dce1de99d589ec66262b8ec20ab10556b465ef84",
      "d7bbafe830e64c74a33a6e6a6100213bcdb96632",
      "ba0c0b1a15d21f83d7b4802d39d0b57a70186882",
      "5047e4e444f3764c9066ee88572e8e494eece9de",
      "a6c542e4ddf5c90a8ec58fa7b8c1eaafe3802d19",
      "574639e9257075654cbed8302f0cd3b11734719a",
      "b213324f5be80a16c6e233c72283605d77ef9ad8",
      "dc219932f723281cf3b3b0ae5a1c55cfead4ba05"
    ],
    "message_list": [
      "`LsfScheduler`: add support for `account` option",
      "Add tests for lsf scheduler plugin",
      "Improve error messages.",
      "Add test + code comment for edge case",
      "Improve check on kwargs of resources",
      "Merge branch 'develop' into fix/lsf-scheduler-add-account-option",
      "Merge branch 'develop' into fix/lsf-scheduler-add-account-option",
      "try pinning pg8000",
      "revert change",
      "Merge branch 'develop' into fix/lsf-scheduler-add-account-option",
      "Merge branch 'develop' into fix/lsf-scheduler-add-account-option"
    ]
  },
  {
    "pr_number": 4922,
    "commits_list": [
      "e8370c94a89998b1bb874f199f4a8e797a022769",
      "7e0e7bc03142e9b62d7e9f0957eb15427a259745",
      "a91f1c875f3803b331dcea4d6dae81f22b109a46",
      "0826ed2a950a1c8908b4379d703ddabf5df4135b",
      "51e4dd55316f13edc614ea556be2f1d6bcc0e79f",
      "3da1e343ec34f52b9deb6a18ea9df75999f734ef",
      "2e65008ebede45df1a5b7b96134eecdd9de21db3",
      "30599c9855647252d20489a0982be0b40fdb91cb",
      "9bf95612a80e12ce0f2f1ead814bf2218cabd84a",
      "3aff8f4a395ab9ba8f4abc6fdce86e500848106f",
      "39cc793cc9e746b43f3b748f3b411efc47e759ca",
      "f28adb0549f2efdecd49f4170452fd9c5c7c01f8",
      "4da2a591ced6cb1adf140e57e04a2bd5ef02e96b",
      "9e6c07f1e874ab0b952393113288d302982ea401",
      "dcadcdbfb95f231216cc68efdf2ffc3657a31e9c",
      "2579d0431c73c6493c9605e7db98bf3b5dddc599"
    ],
    "message_list": [
      "\u2728 NEW: Add `Node.objects.iter_object_keys`",
      "fix pre-commit",
      "Update node.py",
      "Merge branch 'develop' into iter_object_keys",
      "Merge branch 'develop' into iter_object_keys",
      "Merge branch 'develop' into iter_object_keys",
      "Update aiida/orm/nodes/node.py",
      "update and add tests",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "add batch_size",
      "Merge branch 'develop' into iter_object_keys",
      "fix linting",
      "Merge branch 'develop' into iter_object_keys",
      "Apply suggestions from code review",
      "Apply suggestions from code review",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci"
    ]
  },
  {
    "pr_number": 3906,
    "commits_list": [
      "e72a4a3f04970033871e721887cdfe8989946493",
      "8562f5e95a594b3f0f8edf27db4c9cbaf52a06bc"
    ],
    "message_list": [
      "Add infrastructure to parse scheduler output for `CalcJobs`\n\nAdd a new method `Scheduler.parse_output` that takes three arguments:\n`detailed_job_info`, `stdout` and `stderr`, which are the dictionary\nreturned by `Scheduler.get_detailed_job_info` and the content of\nscheduler stdout and stderr files from the repository, respectively.\n\nA scheduler plugin can implement this method to parse the content of\nthese data sources to detect standard scheduler problems such as node\nfailures and out of memory errors. If such an error is detected, the\nmethod can return an `ExitCode` that should be defined on the\ncalculation job class. The `CalcJob` base class already defines certain\nexit codes for common errors, such as an out of memory error.\n\nIf the detailed job info, stdout and stderr from the scheduler output\nare available after the job has been retrieved, and the scheduler plugin\nthat is used has implemented `parse_output`, it will be called by the\n`CalcJob.parse` method. If an exit code is returned, it is set on the\ncorresponding node and a warning is logged. Subsequently, the normal\noutput parser is called, if any was defined in the inputs, which can\nthen of course check the node for the presence of an exit code. It then\nhas the opportunity to parse the retrieved output files, if any, to try\nand determine a more specific error code, if applicable. Returning an\nexit code from the output parser will override the exit code set by the\nscheduler parser. This is why that exit code is also logged as a warning\nso that the information is not completely lost.\n\nThis choice does change the old behavior when an output parser would\nreturn `None` which would be interpreted as `ExitCode(0)`. However, now\nif the scheduler parser returned an exit code, it will not be overridden\nby the `None` of the output parser, which is then essentially ignored.\nThis is necessary, because otherwise, basic parsers that don't return\nanything even if an error might have occurred will always just override\nthe scheduler exit code, which is not desirable.",
      "Remove superfluous `ERROR_NO_RETRIEVED_FOLDER` from `CalcJob` subclasses\n\nThe `ERROR_NO_RETRIEVED_FOLDER` is now defined on the `CalcJob`\nbase class and the `CalcJob.parse` method already checks for the\npresence of the retrieved folder and return the exit code if it is\nmissing. This allows us to remove the similar exit codes that are\ncurrently defined on the calculation plugins shipped with `aiida-core`\n`ArithmeticAddCalculation` and `TemplateReplacerCalculation` as well as\nthe check for the presence of the `retrieved` output from the\ncorresponding parsers. The fact that is now checked in the `CalcJob`\nbase class means that `Parser` implementations can assume safely that\nthe retrieved output node exists."
    ]
  },
  {
    "pr_number": 3597,
    "commits_list": [
      "a67fc3e3301f0108dd1a6d8fca92b84220ef1c20",
      "6f320faa7d6836039ce29b6fe1fe7f13d0196bac",
      "69096b938cb2128575fe2634a3953daa3c0ab4c2",
      "ed11e58a640db9b1cff61f9854db07e4b5325a3a",
      "1a07adcee43f5eca1ac0642928a9b013b16b3380",
      "8885d215f920a124ce6eb1784f6ffa38c489fdc8"
    ],
    "message_list": [
      "Relax requirements of required and optional dependencies\n\nThe version requirements of all `install_requires` and `extras_require`\ndependencies are reevaluated. Where we used to require explicit version\nby pinning to exact versions using the `==` operator, we now use the more\nliberal operator `~=`. This is the compatible release operator:\n\n    https://www.python.org/dev/peps/pep-0440/#compatible-release\n\nFor a given release identifier `V.N` the compatible release clause is\napproximately equivalent to the pair of comparison clauses:\n\n    >= V.N, == V.*\n\nWith this rule we define our dependency requirements using the minor\nversion for packages with a major version of 1 or higher and using the\npatch version for packages whose major version is still at 0. The reason\nis that when packages have released `v1` they can be expected to respect\nsemantic versioning and not break backwards compatibility in minor\nversion whereas this is typically not the case for `v0` versions where\nbackward incompatible changes are introduced in minor versions as well.\nNote that this approach will not prevent from breaking our builds\nbecause packages break their API in minor versions, but at least we have\na consistent dependency requirement policy.\n\nList of specific restrictions and changes:\n\n * `pymatgen==2019.7.2` is the last to support python 3.5\n * `jinja2` moved to `install_requires` as it is used explicitly by `verdi`\n * `astroid==2.2.5` required by `prospector`, will cause crash if wrong\n\nExplicit lower patch requirements are set for a few packages because\nthey are known to contain critical and required bug fixes:\n\n * `psycopg-binary>=2.8.3`\n * `sqlalchemy>=1.3.10`\n * `pgtest>=1.3.1`\n * `seekpath>=1.9.3`\n\nFinally, an exception is made for packages of the `dev_precommit` extra\nwhich can affect pre-commit behavior such as linting warnings or code\nformatting. To prevent updates in these packages from unexpectedly\nfailing the pre-commit hooks, they are pinned to an explicit version.",
      "Remove unnecessary explicit dependency `mock`\n\nThis is a rolling backport of the module `unittest.mock` which has been\npart of the standard python library since v3.3.\n\nNote that one has to do `from unittest.mock import patch` and use it\ndirectly as for some reason `import unittest` followed by using\n`unittest.mock.patch` will result in an `AttributeError`.",
      "Remove unnecessary explicit dependency `uritools`\n\nIt was only used to parse the repository URI of a profile, which can be\naccomplished just as well with the built in `urllib.parse.urlparse`.",
      "Remove unused explicit dependency `passlib`",
      "Deprecate `verdi node tree` and related utilities\n\nThey will be removed in `aiida-core==2.0.0` which will allow to drop the\n`ete3` dependency. This CLI command was they only code using this lib.",
      "Update dependency requirement for Django to 2.2"
    ]
  },
  {
    "pr_number": 5197,
    "commits_list": [
      "62bda3a61cccc1a6b3426cf0e6f6d82a1cb6c799",
      "9d9b556e53534e0765b2d7bad35864bdc5183a0b",
      "680c2d3d27d603b8477759649f6c27fdde868770"
    ],
    "message_list": [
      "caching: order and use latest node when cache more than one",
      "doc: add notes for order caching nodes",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci"
    ]
  },
  {
    "pr_number": 4194,
    "commits_list": [
      "b31d6dc5198706d4149dc02ca1b39e76f8239b7c",
      "cea90020a80f30c599bd6440b30204365b53b7d9",
      "d615f44b86b042f965af947e2d53466ecf2b8ad9",
      "f515dae072a4a21b95b59c5bb7762285fce7fef1",
      "5c8f97ba02ffca4a37bc8100a1e3d2614f90b7e5",
      "6524743b8aa5e0ce528ccf06dda98d6d5f7961dd",
      "65f0872a89047ae08773a8603c4fe370a0aa2123",
      "a71d929e03cf00f7ccb9d7c34c861c2bec0a3e1c",
      "01eee1671cf0e562933ee310e69386d64d0f2853",
      "00e53ed680bd0f9ca622880439c8f460191c3d5f",
      "caeecb82812b8a0e3a40678013e64adfd5fa91a5"
    ],
    "message_list": [
      "Add transfer calcjob\n\nThis calcjob allows the user to copy files between a remote machine and\nthe local machine running AiiDA. More specifically, it can do any of the\nfollowing:\n\n* Take any number of files from any number of RemoteData folders in\na remote machine and copy them in the local repository of a single\nnewly created FolderData node.\n\n* Take any number of files from any number of FolderData nodes in the\nlocal machine and copy them in a single newly created RemoteData folder\nin a given remote machine.\n\nThese are the main two use cases, but there are also other more complex\ncombinations allowed by the current implementation.\nPlease check the documentation for more details.",
      "Add tests for transfer calculation.",
      "Add skip_submit option to calc_info\n\nThis is used by the process handler to decide wether to do the normal\nsequence of steps (upload, submit, update, retrieve) or just skip the\nmiddle two and immediately retrieve after uploading.",
      "Add integration test to test skip of submit step\n\nI am using TransferData as a way to check that the whole process of\nsetting and executing the run steps works correctly when using the\nCalcInfo setup to skip the submit/update steps.",
      "Initial PR corrections.",
      "Remove code from TransferCalcjob",
      "Add documentation on TransferCalculation",
      "Fix exception in docs class reference",
      "Doc fix",
      "Apply suggestions from code review\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "Add test for validators + other pr corrections"
    ]
  },
  {
    "pr_number": 5320,
    "commits_list": [
      "fe28f48816128b96305223561ac7153861fd4c4a",
      "297da50a21a6626e62d76c0465fbc769dbcb7b1a",
      "05aa2d16b1c9af35778e923744e0c43b1e8efb2b"
    ],
    "message_list": [
      "\u267b\ufe0f REFACTOR: Profile storage backend configuration",
      "\u267b\ufe0f REFACTOR: Profile rabbitmq configuration",
      "\u267b\ufe0f REFACTOR: Profile configuration top-level keys"
    ]
  },
  {
    "pr_number": 2420,
    "commits_list": [
      "eb7a5b555dbd11fb7df7dbbbd8f5fc6dabf18012",
      "6b5c8ff34f0ef1d95bae5939b2cb7726151b9a3a",
      "06e97ca168dbecf5be27f9f04834b79b7fac3397",
      "c4d9724fcd0d57536e244a52aa33cca86603fa42",
      "65e3728bd749150e43328d06fc4c7104587cb2cd",
      "ca81f9c410e5270363461ecb87c608f1ee0dad9b",
      "636cd2596f05bc413e34353800640ad86120106c",
      "6a384193b73b3002cddb8783e0c2757d9cfbe7eb",
      "262e418f4c7bfe7a62d2c097e64f07536aa493a5",
      "5e8d411ecff3b0e5556caa44d66a5b0dbe7bcab6",
      "381e9d0c7f87d84cf011d599a06cd69419768361",
      "e7d6630a2f6c3b9b36a3ba6a4fc9976b72e6d530",
      "d7e29530acb40065a7951b21277672c655c1c8b7",
      "7a667b75d4df5f9de17e6cee7f1e928d5497372f",
      "5fa4594c4017af091dedd5ce42c1af2b33d3f76a",
      "7df750da93414a9ec7bc150008c5cc7ae25c2749",
      "3705e5e2e75215e95481efd9ccb361578210daa7",
      "37c83f1dacc78e51479b9579598efa485c0fe411",
      "88d9126610bea7d9f0692552f88b54e550f4c2a1",
      "bcf2b4c751ecf68cef8f2dd05c469a7eed262481",
      "5cbce6c750297d9f8cdb88e51dfc92dad8626783"
    ],
    "message_list": [
      "add instructions when verdi import fails\n\nwhen verdi import fails because of an old/new export file version,\nadd a hint on what to do",
      "add fastentrypoints to build requirements",
      "try fixing pip issue",
      "missing flag for regular install",
      "update pre-commit to fix ruby issue\n\ndiscussed in #2362",
      "try bumping numpy version",
      "downgrade numpy because of 1.16 issue\n\nsee https://github.com/numpy/numpy/issues/12749",
      "try upgrading pymatgen",
      "go back to 2018.12.12 for py2 support",
      "trying cython version\n\ntrying cython>=0.29 from here:\nhttps://github.com/numpy/numpy/issues/12785",
      "revert back to pip 18.1",
      "try reverting other packages as well",
      "try fixing scipy version",
      "try adding numpy to build requirements",
      "try uninstalling numpy",
      "add --yes",
      "travis: add pip freeze + verbose flag",
      "tone down verbosity for travis",
      "new numpy version",
      "cleanup",
      "add missing exception"
    ]
  },
  {
    "pr_number": 4699,
    "commits_list": [
      "a216b1959e53a820df5454c80b80a609ba200f38",
      "a27aa944742cdcadef585035a22cdeaae8769b71",
      "69aa6b9cbcb2f72319f9d24951cb89f673141ce1",
      "20d43c952221f11779a5f26bf4cafad4c7a897bb",
      "cda8e5cdaacfdad12417d01ee8c4ba5e30ae2963",
      "99bb97c004c0d83dfa2993c78c70248af15bfe9e",
      "9842fc5a7904a4ddec5b83a51e8a3152ed2d61b4",
      "e27e802dac59a98bbddc5138b476938043ce600e",
      "df7af0fa8973199f31e8ec289cb4c8b8e63a7316",
      "b9ce49653ec5c5df944b3c061594258b31f5c5ed",
      "05750be255ccf834803d5695c154e34ff19cb04f",
      "fdd6b24bfc2594732b7dddbb27cd3e3f875057e2",
      "a1cf65e139c5ed38d52056958a7f87646a554a5a",
      "4d90829e8a46443a952f2905e7d9250f0968c669",
      "3e51b5631a585323f122753bb69325251427cbb7",
      "04d19d2070bbe57f75cebcd14affa3b8c477ed78"
    ],
    "message_list": [
      "memory leaks: don't pass process stack via context",
      "add test against memory leaks in engine.run\n\nTest that no refernce to AiiDA processes remains in memory after all of\nthem have exited.",
      "add pympler dependency also on CI",
      "refactor test to get rid of local variables",
      "try freeing results dict",
      "try adding more empty contexts...",
      "add memory leak test in test suite",
      "Merge branch 'develop' into issue_4698_context_var",
      "fix memory leak in test suite",
      "try fixing daemon tests",
      "move test to .ci",
      "Merge branch 'develop' into issue_4698_context_var",
      "fix mypy",
      "Merge branch 'develop' into issue_4698_context_var",
      "Merge branch 'develop' into issue_4698_context_var",
      "Merge branch 'develop' into issue_4698_context_var"
    ]
  },
  {
    "pr_number": 4357,
    "commits_list": [
      "9b4eb1711109690c70a760c6213f09946f4b867d",
      "45ed3b27d951b9a079a527fb130f31a44e3b39e0",
      "1d061a638cb060339f8b744e9eeb412cb34f46b2",
      "2830206176d82265ebdb9471f9019b82abfb98f7"
    ],
    "message_list": [
      "docs: clarify `verdi node delete` docstring",
      "remove unnecessary Group.clear()",
      "deprecate --clear flag and remove any functionality",
      "Merge branch 'develop' into issue_4356_group_delete_docstring"
    ]
  },
  {
    "pr_number": 5331,
    "commits_list": [
      "4e76e515bc19ff819f824d6ebe4bc8c0f7f62aa5",
      "c98fb5c629fdd41ff5932cbf4154c22778803999",
      "98f6bc41671bc6c78f04f3a1fe290fe0a02485d1",
      "d98e3014eb96dbb2674bc30c6cfbf9b5fa9136d4",
      "1b0c41315ed8b74f7cbe8c5696be74becbd2d03d"
    ],
    "message_list": [
      "Improve storage maintain related methods\n\nMoved the general purpose functions into methods of the storage class\n\nUse the lock mechanism for the maintain operation",
      "Apply feedback from PR review",
      "Merge branch 'develop' into incorporate_lock",
      "Generalize the get_info method.",
      "Final details"
    ]
  },
  {
    "pr_number": 4060,
    "commits_list": [
      "34d7f13b87fd0dc48c9f5a10d193231b7fc34419",
      "ed13abc5c8496af8d8dcb8ec64441ea80c65af42",
      "e9062c5b6e826f5111190159fb21995e808162bb",
      "18d62266433cfa4dd3c3e5682f3d4c270476eab1",
      "ccc90918b8d9c1456005c9fa853825a79abfe095",
      "c22979c78a5433f850ce8013f56f38e26fad2ec3",
      "081a94a5593624aeb646572a201de39a34c853bb",
      "db673cb55922899b8d057b45ed25cb1c6c4d2ec6",
      "f88ce4aa5b477d5c7a9befe27db6da4c0f8cc874",
      "4bd06e4cf9fabe969dd9743915b56128a52627a4",
      "5903f9e755582e54d0da89feac74eda0d96734d0",
      "160e49539fd4641a6e1f6d2dcdf44dca0b022f75",
      "9befbc9b82d64f8ab3a9418543465c2f6cfb4c12",
      "8766481566e6a68bf52c376284f68517703bfe20",
      "a53f5b54174e7ce3bb6621dd9fbf965e2c514fe1",
      "ec576b4f036cf00af2b926f23e2e24527b4856c6",
      "1e0869f8c4b13304b88682a9afd70a29605caa86",
      "179beda373b41ba481f279221ebec23b16e9a93a",
      "191d122b98b2c57f954eb57c20bb85cf6f29a15d",
      "3ed97b92e8abd83891c50e18b755666beef8b53b",
      "bfd98e35cc188e2463cb1b1c91e9a76e5de66a08",
      "80e1dd1ab1d1ae8be7d68fc03fb1252e4ce70a1d",
      "b83385cdb1bb7b1e1bb86976a9ea28f95a68f994",
      "aa3e59f4f01dae4aa4f0f901e195824d2872e2e2",
      "7dad928be7ae71db9621a8e7613dd874b9a47af5",
      "43aebd7fd7dba777538f411b774a03e849e352d0",
      "bf42e53c8cd0fdb57eafc880ce55c65b3c3e9b96",
      "39b344504fc213ef14a82240233dd40a694722d3",
      "ec3120055c91be4ffaf49a1ac0f7c75ca7c8d6fb",
      "11176819d4a49a0b7ef21ec45b6089f43ebcc327"
    ],
    "message_list": [
      "Update dpendency sphinx-panels=0.2",
      "Update install instructions",
      "Update sphinx-panels dependency",
      "Major update",
      "circleci fix",
      "circleci fix",
      "Update config.yml",
      "Update config.yml",
      "Update config.yml",
      "Finish docker section",
      "Add updating AiiDA sections",
      "typos",
      "Move other sections",
      "Fix pre-commit",
      "fixes",
      "fix title",
      "code block fixes",
      "Add further reading links",
      "Add sphinx-copybutton",
      "Add `copybutton_prompt_text`",
      "Update get_started.rst",
      "Add front page links",
      "Update docs/source/index.rst\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Update docs/source/intro/get_started.rst\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Update docs/source/intro/configuration.rst\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Update docs/source/intro/get_started.rst\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Installation -> Detailed Installation",
      "Update config.yml",
      "Merge branch 'docs-revamp' into docs-revamp-intro-install2",
      "Address some review comments"
    ]
  },
  {
    "pr_number": 3650,
    "commits_list": [
      "374543fc3517520d2527719d49e21422687e4745",
      "43d852ebf0048b0a623061d4b22007ce9dd3a576",
      "ba4b2174d1a08348eb914492d4f7c8714dae478a",
      "31d0470543c2213bd0ca0dbd99cf92b49e09c100",
      "a63cc52b67b503e54a2283d9d699a7551750987e",
      "9a681009d2d3095fc54c0f25aa0d129e7ab97d7f"
    ],
    "message_list": [
      "Clean-up of the autogroup logic\n\nIn particular:\n\n- deprecated use of 'group-name' in favour of 'group-label'\n  both in the internal API of AutoGroups and in the CLI parameters\n  of `verdi run`\n- Improved the Autogroup implementation and API\n- add various tests of the functionality of autogroups\n- fixed the CLI parameters to include/exclude groups, that were badly\n  broken (and untested). Now it accepts any entrypoint string (before\n  it was accepting the first part of an node_type string). Also added\n  click validation of the parameters.\n- fixed the --group flag to activate/deactivate autogrouping (it was\n  wrongly defined and thus always True)\n- linter fixes",
      "Centralised the creation of the Autogroup\n\nThis also remove an overzelous isinstance check, and\nmoves additional checks in a cached function that is run only when\nstoring the very first node (that needs to be put in an autogroup),\nmaking storing of nodes faster (even if times oscillates so it's hard\nto estimate exactly by how much).\n\nAlso, added logic to allow for concurrent creation of multiple groups\n(and test). This fixes #997",
      "Addressed comments by Sebastiaan.\n\nIn particular, the most important changes include:\n- now the auto-group flag is called `--auto-group`, is\n  a flag and is False by default\n- only kept `--include` and `--exclude` options, checking\n  typestrings and allowing to end with `%`. One benefit\n  is that while reimplementing I replaced some `isinstance`\n  with string comparisons, with potential further benefits.\n- `--include` and `--exclude` are now mutually exclusive\n- improved documentation of the main point of the issue",
      "A few further comments addressed.\n\nMost notable: now the % sign can be anywhere in the string\nof included or excluded classes, and not only at the end.",
      "Merge branch 'develop' into fix_997_parallel_autogroups",
      "Minor pre-commit fixes after merge."
    ]
  },
  {
    "pr_number": 5804,
    "commits_list": [
      "19cf0a662342f8683106a4c89c50e1845554f0e5",
      "192bcc62441868655f8ad7fb37286ccab695b968",
      "a19d76c7b7c9a973d6fb8bdde79e61c4956d658c",
      "29a30670eefaf548d3a38ec034597bfe2f6aa97f",
      "24921527b1fe7ca23e90fda9d03a1f7bbcddd952"
    ],
    "message_list": [
      "`PsqlDosBackend`: Use transaction whenever mutating session state\n\nStoring a node while iterating over the result of `QueryBuilder.iterall`\nwould raise `sqlalchemy.exc.InvalidRequestError` with the message:\n\n    Can't operate on closed transaction inside context manager.\n\nThe problem was that the `Node` implementation for the `PsqlDosBackend`,\nthe `SqlaNode` class, would not consistently open a transaction, using\nthe `PsqlDosBackend.transaction` method, whenever it mutated the state\nof the session, and would then straight up commit to the current session.\nFor example, when storing a new node, the `store` method would simply\ncall save. Through the `ModelWrapper`, this would call commit on the\nsession, but that was the session being used for the iteration.\n\nThe same problem was present for the `SqlaGroup` implementation that had\na number of places where sessions state was mutated without opening a\ntransaction first.\n\nThe problem is fixed therefore by consistently opening a transaction\nbefore making changes to the session. The `transaction` implementation\nis slightly changed as any `SqlaIntegrityError` raised during the context\nis now converted into an `aiida.common.exceptions.IntegrityError` to make\nit backend independent.",
      "`SqliteTempBackend`: Fix the `transaction` method\n\nWhen trying to store nodes, for example by running a calcfunction,\nSqlAlchemy would throw an exception complaining that the transaction had\nalready been closed and the context manager had to be closed first. The\nimplementation is fixed by matching it with that of the `PsqlDosBackend`\nstorage backend.\n\nThe error was only discovered through the execution of the `tutorial.md`\nnotebook in the documentation. An explicit unit test is added to make it\nmore obvious and easier to debug in case a regression is introduced.",
      "Add test for `SqlaGroup.add_nodes` to test `disable_expire_on_commit`\n\nThe test passes, but it seems independent of the use of the utility\n`disable_expire_on_commit`. It looks like this is because now a\ntransaction is properly used to add or remove nodes, and so the\ninstances in the parent session are not expired. It seems that the\nproper use of transactions essentially makes `disable_expire_on_commit`\nobsolete. When the same test is run on `v2.1`, where transactions were\nnot properly used, removing the context manager fails the test.",
      "`PsqlDosBackend`: Remove `disable_expire_on_commit` utility\n\nThe implementation of the backend `Group` ORM class for the\n`PsqlDosBackend` used the `disable_expire_on_commit` context manager in\nthe `add_nodes` and `remove_nodes` methods. By default, the engine used\nsets `expire_on_commit=True`, which means that all database model\ninstances in the session are expired once the session is committed,\nwhich forces the instances to be refreshed from the database the next\ntime they are accessed.\n\nThis is a sensible and important setting, as it ensures that we are in\nsync with the database when multiple processes are committing to the\ndatabase. However, expiring instances comes at a cost because they will\nhave to be refreshed at some point. It is unknown exactly why his change\nwas originally applied, but it is probably because expiring all nodes in\nthe session upon addition/removal of nodes from a group affected\nperformance badly and the probability of inconsistencies by omitting it\nare less problematic.\n\nThe behavior was never tested though and with the recent refactor of the\ntransaction control in the `PsqlDosBackend` it seems the utility to\nexplicitly disable the expire on commit no longer seems necessary. In\nthe previous commit a test was added to test the utility correctly being\napplied, but it works whether the utility is used or not at all. This\nsuggests that with the current proper use of a nested transaction in\n`add_nodes` and `remove_nodes`, the nodes in the outer session are no\nlonger expired, making the utility obsolete and therefore it is removed.",
      "Docs: Fix the build on ReadTheDocs\n\nThere was a backtick missing in the `help` attribute of an input port\ndeclaration of the `CalcJob` class. This class is rendered automatically\nthrough our sphinx plugin in the `topics/calculations/usage.rst` file in\nthe \"Options\" section. No idea why this didn't fail before this because\nthis mistake was not introduced in this PR.\n\nAlso add config option for `myst-nb` to `conf.py` so that it shows the\nstacktrace if an exception is raised during notebook execution."
    ]
  },
  {
    "pr_number": 4516,
    "commits_list": [
      "559abbaab690bc7f94c84ece63ad4810500592bf",
      "01845181740c2768ce3c31165a3f80e18d241a9f",
      "5e1c6fd965bc8cdeea8bc0c37ee19a71de5986f3",
      "dac81560647b2ffaa170ee87f673bd9f89db2b41",
      "ff30ebdb8860dc69bcbfec5e7a19e8b6e15a4f42",
      "1310abaa7f765866f636c9af2d3332e3eaf74ced",
      "e2b5385044076f135e5b769aa8fd24f7950738f5",
      "65ad067b18cffeb639994efe9a372ec1475e1615",
      "91449241ff2e12dd836b29882e32201cc7841716",
      "af91a8b10f2fe68360483a951d5c578863d38b76",
      "4544bc49a50c9aa3abebd2837efb5626958ee2b4",
      "02248cf3686a0ab89faf1625e0da24d9e33d8cde",
      "29331b558b45ba74acf1ca633a2d8bfabc1bdd05",
      "1e1bdf2dee779970654ed9b22eb996b78e9c4149",
      "bd6903d88a4d88077150763574784a1bc375c644",
      "16bc30548f7f1c686d200935174533535e850fd5",
      "e840ab62615bf18dbfbd24bf9b821489f2adc37b",
      "0fb6f72333b0e553590326fc348acbee3ef0763b",
      "4791046a58a1dfd0948f02cea6cf4b13eb1be4a5",
      "9ceb7b3b6a133227f8b1b46bd34ce41f3eb1357c",
      "2174924e40c3c9f00263f2e892e63a07eb90b40f",
      "eed191785da51b3c8ae105ee2073e3330deb0790",
      "db659ddf8a36db77963e2955df5066876c0d8017",
      "6202fac632f2e9c5c440b34695effc94f78e9492",
      "cda695007feca41c01c93bc0936e5463b3aa73dd",
      "5e0de12cd21d18da1e57c87f8f5e0130edda508d",
      "ce65988b7a90f0245467dae626d9f695181b1dba",
      "e63fdebb9d13a203adc23920fb036234baed6942",
      "cb268d1dd90a2aaeb94e7fd0ddc4091a6bf9ebc9",
      "58eb4d3a395294589e18a2cceb3a99415256e757"
    ],
    "message_list": [
      "Drop support for Python 3.5 (#4386)\n\nPython 3.5 is EOL as of September 13 2020. CI testing will now only be\r\ndone against Python 3.6 and 3.8.",
      "`LinkManager`: fix inaccuracy in exception message for non-existent link  (#4388)\n\nThe link manager was always referring to an 'input link' while it should\r\ninstead refer on an 'input link label' or 'output link label' depending\r\non the value of the link direction, determined by the `self._incoming`\r\nattribute.",
      "Implement `next` and `iter` for the `Node.open` deprecation wrapper (#4399)\n\nThe return value of `Node.open` was wrapped in `WarnWhenNotEntered` in\r\n`aiida-core==1.4.0` in order to warn users that use the method without a\r\ncontext manager, which will start to raise in v2.0. Unfortunately, the\r\nraising came a little early as the wrapper does not implement the\r\n`__iter__` and `__next__` methods, which can be called by clients.\r\n\r\nAn example is `numpy.getfromtxt` which will notice the return value of\r\n`Node.open` is filelike and so will wrap it in `iter`. Without the\r\ncurrent fix, this raises a `TypeError`. The proper fix would be to\r\nforward all magic methods to the wrapped filelike object, but it is not\r\nclear how to do this.",
      "Dependencies: increase minimum version requirement `plumpy~=0.15.1` (#4398)\n\nThe patch release of `plumpy` comes with a simple fix that will prevent\r\nthe printing of many warnings when running processes. So although not\r\ncritical, it does improve user experience.",
      "`verdi setup`: forward broker defaults to interactive mode (#4405)\n\nThe options for the message broker configuration do define defaults,\r\nhowever, the interactive clones for `verdi setup`, which are defined in\r\n`aiida.cmdline.params.options.commands.setup` override the default with\r\nthe `contextual_default` which sets an empty default, unless it is taken\r\nfrom an existing profile. The result is that for new profiles, the\r\nbroker options do not specify a default, even though for most usecases\r\nthe defaults will be required. After the changes of this commit, the\r\nprompt of `verdi setup` will provide a default for all broker parameters\r\nso most users will simply have to press enter each time.",
      "`verdi setup`: improve validation and help string of broker virtual host (#4408)\n\nThe help string of the `--broker-virtual-host` option of `verdi setup`\r\nincorrectly said that forward slashes have to be escaped but this is not\r\ntrue. The code will escape any characters necessary when constructing\r\nthe URL to connect to RabbitMQ. On top of that, slashes would fail the\r\nvalidation outright, even though these are common in virtual hosts. For\r\nexample the virtual host always starts with a leading forward slash, but\r\nour validation would reject it. Also the leading slash will be added by\r\nthe code and so does not have to be used in the setup phase. The help\r\nstring and the documentation now reflect this.\r\n\r\nThe exacti naming rules for virtual hosts, imposed by RabbitMQ or other\r\nimplemenatations of the AMQP protocol, are not fully clear. But instead\r\nof putting an explicit validation on AiiDA's side and running the risk\r\nthat we incorrectly reject valid virtual host names, we simply accept\r\nall strings. In any case, any non-default virtual host will have to be\r\ncreated through RabbitMQ's control interface, which will perform the\r\nvalidation itself.",
      "Merge branch 'master' of github.com:aiidateam/aiida-core into develop\n\nMerge after release of `v1.4.0`.",
      "CI: move `pylint` configuration to `pyproject.toml` (#4411)\n\nThis is supported by `pylint` as of v2.5.",
      "`verdi process show`: order called by ctime and use process label (#4407)\n\nThe command was showing the called subprocesses in a random order and\r\nused the node type, which is often uninformative. For example, all\r\nworkchains are always shown as `WorkChainNode`. By using the process\r\nlabel instead, which is more specific, and ordering the called nodes by\r\ncreation time, the list gives a more natural overview of the order in\r\nwhich the subprocesses were called.",
      "Dependencies: update requirement `pytest~=6.0` and use `pyproject.toml` (#4410)\n\nStarting from v6.0, `pytest` supports using the `pyproject.toml` instead\r\nof a `pytest.ini` to define its configuration. Given that this is\r\nquickly becoming the Python packaging standard and allows us to reduce\r\nthe number of configuration files in the top level of the repository, we\r\nincrease the version requirement of `pytest`.\r\n\r\nNote that we also require `pytest-rerunfailures>=9.1.1` because lower\r\nversions are broken in combination with `pytest==6.1`. See the following:\r\n\r\n   https://github.com/pytest-dev/pytest-rerunfailures/issues/128\r\n\r\nfor details.",
      "CI: add coverage patch threshold to prevent false positives (#4413)\n\nThe project diff percentage is the change in coverage w.r.t. all lines\r\nin the project, whereas the patch diff percentage is the change in\r\ncoverage w.r.t. only lines touched by the PR. The patch threshold is\r\ncurrently defaulting to 0%, hence it is very easy to fail. By raising it\r\nto 0.1% it should now only fail when there is a significant reduction\r\nin coverage. Number may need to be further tweaked.",
      "Replace old format string interpolation with f-strings (#4400)\n\nSince Python 3.5 is no longer supported, format string interpolations\r\ncan now be replaced by f-strings, introduced in Python 3.6, which are\r\nmore readable, require less characters and are more efficient.\r\n\r\nNote that `pylint` issues a warning when using f-strings for log\r\nmessages, just as it does for format interpolated strings. The reasoning\r\nis that this is slightly inefficient as the strings are always\r\ninterpolated even if the log is discarded, but also by not passing the\r\nformatting parameters as arguments, the available metadata is reduced.\r\nI feel these inefficiencies are premature optimizations as they are\r\nreally minimal and don't weigh up against the improved readability and\r\nmaintainability of using f-strings. That is why the `pylint` config is\r\nupdate to ignore the warning `logging-fstring-interpolation` which\r\nreplaces `logging-format-interpolation` that was ignored before.\r\n\r\nThe majority of the conversions were done automatically with the linting\r\ntool `flynt` which is also added as a pre-commit hook. It is added\r\nbefore the `yapf` step because since `flynt` will touch formatting,\r\n`yapf` will then get a chance to check it.",
      "CI: use `-e` install for tox + add docker-compose for isolated RabbitMQ (#4375)\n\n* Using `pip install -e .` for tox runs improves startup time for tests\r\n   by preventing unnecessary copy of files.\r\n\r\n* The docker-compose yml file allows to set up an isolated RabbitMQ\r\n   instance for local CI testing.",
      "Merge remote-tracking branch 'origin/master' into develop",
      "`ProcessBuilder`: allow unsetting of inputs through attribute deletion (#4419)\n\nThe builder object was already able to delete set inputs through the\r\n`__delitem__` method, but `__delattr__` was not implemented causing\r\n`del builder.input_name` to raise. This is not consistent with how these\r\ninputs can be set or accessed as both `__getattr__` and `__setattr__`\r\nare implemented. Implementing `__delattr__` brings the implementation\r\nup to par for all attribute methods.",
      "`CalcJob`: support nested directories in target of `remote_copy/symlink_list` (#4416)\n\nThe `upload_calculation` transport task would fail if either the\r\n`remote_copy_list` or `remote_symlink_list` contained a target filepath\r\nthat had a nested directory that did not exist yet in the remote working\r\ndirectory. Instead of inspecting the file system or creating the folders\r\nremotely each time a nested target path is encountered, which would incur\r\na potentially expensive operation over the transport each time, the\r\ndirectory hierarchy is first created in the local sandbox folder before\r\nit is copied recursively to the remote in a single shot.",
      "Docs: link to packaging plugins howto (#4439)\n\nThe how to on supporting external codes in AiiDA did not link to the\r\npackaging guide. While the how to is great for learning how AiiDA works,\r\ndevelopers who want to get started as quickly as possible are better off\r\nusing the AiiDA plugin cutter.",
      "Docs: Add docs live build to tox configuration (#4460)\n\nAdd docs live build using sphinx-autobuild. This dramatically speeds up the process of checking the rendered documentation while editing.\r\n\r\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "Docs: Add redirects from old documenation (#4457)\n\nUses the `sphinxext-rediraffe` Sphinx extension to automatically create\r\nredirects when documentation pages are moved and therefore their URLs\r\nchange. New redirect rules should be added to `docs/source/redirects.txt`",
      "Docs: add \"How to install plugins\" section (#4468)\n\nThere was no centralized location yet that explains how a plugin package\r\nis installed, yet this is one of the most critical first steps for new\r\nAiiDA users. A new how-to section is created \"How to install plugins\"\r\nthat details this information. Since a file `plugins.rst` already\r\nexisted, which contains information on how to develop a plugin package,\r\nit is renamed to `plugins_develop.rst`.",
      "Docs: add the \"Transport plugins\" topic section (#4465)\n\nContent is mostly moved from older existing documentation.",
      "Docs: \u2b06\ufe0f Update sphinx + extensions versions (#4470)\n\nThis commit primarily upgrades the sphinx dependency from sphinx v2 to v3, allowing for other upgrades of sphinx version pinning.\r\n\r\nIt also moved the `aiida/sphinxext` testing to the official sphinx testing infrastructure, and fixes an issue with the automodule writer. However, the automodule functionality cannot yet be re-instated, due to issues with referencing of these objects.",
      "REST API: list endpoints at base URL (#4412)\n\nThe base URL of the REST API was returning a 404 invalid URL response\r\nwithout providing any guidance to new users as to how to use the API.\r\nWe change this to return the list of endpoints formerly available only\r\nunder /server/endpoints.\r\n\r\nDocumentation of where to find the list of endpoints -- which seems\r\nto have been entirely deleted -- is added.\r\n\r\nCo-authored-by: Giovanni Pizzi <gio.piz@gmail.com>",
      "Docs: move the \"Daemon as a service\" section (#4481)\n\nInstead of copying the template file inline, links are provided to the\r\nsame template on the MARVEL NCCR repository that is used for the Quantum\r\nMobile.",
      "Docs: move the cookbook to its own how-to page (#4487)\n\nThe scripts were updated to work with the AiiDA v1.0 interface. The\r\nexample for `AuthInfo` was also simplified as the `Computer` class now\r\nprovides the `get_authinfo` method.",
      "Docs: add info on archive format to internals section (#4467)",
      "Docs: move how-to write data plugin to topics (#4482)\n\nThis information is felt to be too detailed for the how-to section and\r\nit breaks the flow too much.",
      "Docs: add \"How to share data\"\n\nThe \"how to share data\" section includes instructions both for dealing\nwith AiiDA archives (e.g. for publishing AiiDA graphs alongside your\npublication) and for using the AiiDA REST API.\n\nCo-authored-by: Casper Welzel Andersen <43357585+CasperWA@users.noreply.github.com>",
      "replace all occurences of \"export file\"\n\nWe have agreed on the terms \"AiiDA archive (file)\" and \"AiiDA archive\nformat\".\n\nCo-authored-by: Casper Welzel Andersen <43357585+CasperWA@users.noreply.github.com>",
      "Docs: add \"How to interact with AiiDA\" section (#4475)\n\nThis section gives a high-level overview of the various methods with\r\nwhich can interact with AiiDA. For details it refers to relevant\r\nsections with more information."
    ]
  },
  {
    "pr_number": 5046,
    "commits_list": [
      "f7c0097d3613a4fb46167926c159773841a870e8",
      "00dcbf7c309b58d316fe32e345e63391e4661cd4",
      "42ed72748657f9fdd3b2af622fe13267d367fe2e",
      "d9bca0aec61bdfffe7dfb495f14fc5b0415da31a",
      "56d6a49e05b8f61e22282ee41082af9935860680"
    ],
    "message_list": [
      "Dependencies: Bump jinja2 to version 3.0",
      "Dependencies: Bump pydata-sphinx-theme to 0.6.3.",
      "Automated update of requirements/ files. (#5057)\n\nCo-authored-by: csadorf <csadorf@users.noreply.github.com>",
      "Merge branch 'develop' into dm/bump-jinja2-3.0",
      "Merge branch 'develop' into dm/bump-jinja2-3.0"
    ]
  },
  {
    "pr_number": 4219,
    "commits_list": [
      "6219600e9204cb9b02dd52f14dd531eb204ef663"
    ],
    "message_list": [
      "rename test fixtures => static\n\nThe name \"fixtures\" is currently used both for the pytest fixtures and\nfor test data in tests/fixtures, such as AiiDA export files.\nThis is confusing and makes searching in the codebase unnecessarily\ndifficult.\nHere, we rename the test data folder to \"static\", which indicates the\nstatic nature of the files residing there, while avoiding a clash of\ndefinition with the pytest fixures residing in aiida.manage.tests."
    ]
  },
  {
    "pr_number": 4539,
    "commits_list": [
      "a549d673d4a73f5e245f9880ad88a04c5b0cbae7",
      "61a92b024110b401b40c33a9393aa28e5d658e02"
    ],
    "message_list": [
      "make process function submittable\n\nFix #2965, let the process function can be submit to the runner, means shutdown the daemon will not fail the function process (`calcfunction` or the `workfunction`). So they can restart when the daemon is restart. Since the process function still can only running on the local machine, it blocking the daemon even we 'submit' it to the daemon.",
      "Merge branch 'develop' into feature/2965"
    ]
  },
  {
    "pr_number": 4554,
    "commits_list": [
      "ce1ad67b9e58cd8bede80de8a92fef373b6bccce"
    ],
    "message_list": [
      "Process functions: Add the `get_source_code_function` method\n\nThis method returns the source code of just the process function itself,\ni.e., the wrapped function plus the decorator itself.\n\nThe existing method `get_function_source_code` would return the source\ncode of the entire file in which the process function was defined, but\nthis can at times include a lot of other code that is not always useful\nto have. The `get_function_source_code` method is deprecated and replaced\nby the `get_source_code_file` method to have consistent naming.\n\nThe `get_source_code_function` implementation retrieves the source code\nof the function by calling `get_source_code_file` and extracting just\nthe lines of the function. This is accomplished by using the starting\nline of the function, which already used to be stored, combined with the\ntotal number of lines that make up the function, which is an attribute\nthat is stored from now on for process functions."
    ]
  },
  {
    "pr_number": 3912,
    "commits_list": [
      "08e531001e62d12db1e981c28e56a8e1ce55fe0b"
    ],
    "message_list": [
      "Add export archive migration for `Group` type strings\n\nThe `Group` entity class is now subclassable, for which the type string\nof existing entries in the database had to be changed through a\nmigration. Here we add the corresponding migration for existing export\narchives. The only required change is to map the type string of groups.\n\nTo test this migration we use the existing export archives in the module\n`tests/fixtures/export/migrate`. They did not contain any group entities\nso `export_v0.1_simple.aiida` was updated first and had four groups\nadded, one for each of the migrated type strings. This initial archive\nwas then migrated to each subsequent version, step by step, using the\ncommand `verdi export migrate --version`.\n\nAlso fixed and migrated `tests/fixtures/graphs/graph1.aiida` which was\ncorrupt and could not be migrated automatically, because it contained a\nprocess node with an active process state."
    ]
  },
  {
    "pr_number": 5718,
    "commits_list": [
      "da51c93e2dd9b9e44ab9d761bb382753872ec52a",
      "fecf04dec7637557ce21cccd8d9c3061dacbc3a5",
      "82d4f74111a1bfa9022709a14f39922f67645b85",
      "26f82a8a65e108f18b24e9a7311ec345f3cef353",
      "60342fef8fda59e507a24ac1673bf706f22e788b"
    ],
    "message_list": [
      "Refactor: Turn `aiida.manage.external.rmq` into a package\n\nThis is in anticipation of more code being added soon and the file was\nalready getting large and difficult to read. Turning it into a package\nwith individual modules helps readability greatly.",
      "RabbitMQ: Remove support for v3.5 and older\n\nRabbitMQ 3.5 has been EOL since 31 October 2016:\n\n    https://www.rabbitmq.com/versions.html\n\nThis version required to explicitly run `support_deprecated_rabbitmq` on\nthe `pamqp` library (which is used all the way down the stack by\n`aiormq` to handle interactions with RabbitMQ) to enable compatibility.\nIn removing this, the explicit dependency on `pamqp` is also removed.\n\nThe `aiida.manage.manager.is_rabbitmq_version_supported` function which\nis used to check compatibility when a profile is loaded is updated to\ninclude the lower bound.",
      "Add a client to connect to RabbitMQ Manamegement HTTP API\n\nRabbitMQ provides a plugin `rabbitmq_management` that provides an HTTP\nAPI to manage a RabbitMQ instance. For example, it can be used to\ninspect existing queues, create new ones, or delete queues.\n\nFor the client to work, the plugin should first be enabled with:\n\n    sudo rabbitmq-plugins enable rabbitmq_management\n\nIf the API cannot be connected to a `ManagementApiConnectionError`\nexception is raised.\n\nIn the Github Action workflows, the command above won't work, since\nRabbitMQ is installed as a service in a Docker container. However, it\ncan be enabled in the Docker container by adding the `-management`\nsuffix to the version specification of the RabbitMQ service. Since the\nAPI is exposed on port 15672, that is also added to the forward list.",
      "CLI: Add the `verdi devel rabbitmq` command group\n\nThe command group has three subcommands:\n\n * `server-properties`: Report properties of RabbitMQ server\n * `queues`: Interact with RabbitMQ queues\n * `tasks`: Deal with messages stored in tasks queues\n\nThe first command is a simple utility that can come in handy during\ndebugging as it reports various server settings, more so than the\nversion of RabbitMQ that is reported by `verdi status`.\n\nThe `queues` command is also mostly useful for debugging. It currently\nallows to list, create and delete queues.\n\nThe most important command is the `tasks` command. Currently, there is a\nbug where it is possible that tasks in the process queue (which are\nconsumed by daemon workers to continue submitted processes) can get\nlost, despite the persistence requirements on the queue and the\nmessages. When a tasks gets lost, the corresponding process will become\na \"zombie\" and seems stuck. Currently there was no easy way to confirm\nthat a process had become a zombie due to its task having been lost.\n\nThe `list` method fixes this problem as it will introspect all task\nmessages in the process queue and collect the process ids that they\ncorrespond to. This can be used to see which active processes no longer\nhave a corresponding task and so are a zombie.\n\nThe `analysis` command is a utility command that automates this process.\nIt will get all process ids of the existing tasks in the process queue\nand cross-reference those with the active processes as stored in the\ndatabase of AiiDA's storage backend. By default it reports if there are\ninconsistencies between the two sets. If there are, the user can run the\ncommand again with the `--fix` flag and the command will automatically\ndiscard tasks that are incorrect and recreate those that are missing.",
      "CLI: Move `verdi devel revive` to `verdi devel rabbitmq tasks revive`\n\nThe command was recently added before the `verdi devel rabbitmq`\nsubcommand existed. At this point it makes more sense to group it there\nbecause it is directly related to RabbitMQ and if this service is ever\nremoved, this command also no longer serves a purpose."
    ]
  },
  {
    "pr_number": 544,
    "commits_list": [
      "ec5806de7d5ab83f2d650dbcea6e131fc18fa8c3",
      "2a8d936282f354bb99ab9e15ee724adbfad51537",
      "c3ce662359db0c10ea3736d3d9c3f483f8db1248",
      "b45a0a631c60485d25e8d738161ed224e498b608",
      "e59a37654fa70d74437461fd75b1b7eef39cb55e",
      "cbe037813796050857feca97cc214e1e0d2d19e0"
    ],
    "message_list": [
      "Implement inheriting input parameters from other processes",
      "Add tests for processes which inherit inputs",
      "Merge branch 'develop' of github.com:aiidateam/aiida_core into workchain_inherit_inputs",
      "Merge branch 'develop' of github.com:aiidateam/aiida_core into workchain_inherit_inputs",
      "Add 'namespace' to spec.inherit_inputs",
      "Allow inheriting from a Process's inputs with multiple namespaces"
    ]
  },
  {
    "pr_number": 4070,
    "commits_list": [
      "f8cbb354e5db6c70262d25fc734487469a7b3487",
      "844da351c8d7b234014e33f49456c8012966b36d",
      "f229ea8e808743f3034af59b1c4489b4a4132016",
      "1bc5aaa0f558e8dd7ba89a1419f578dd2c6f3370",
      "9f9a74d9e020915eda131fae7000bacb814cc771",
      "298ece32ff300a1be5ddad735cc346a8a057aff5",
      "0aca8517c2e6427d6895954d7d5be3754b4d249c",
      "8cd38edd82f67883d48451168465ca89e4808710",
      "f77f9f0d533cce55341be53b0296f98a97e88386",
      "ebe46b87865ab21e62ecd53dda15457139dc569f"
    ],
    "message_list": [
      "docs: add first draft of aiida tutorial\n\nFirst draft of the AiiDA tutorial. Next to adding the tutorial itself, I've also:\n* Changed the ArithmeticAddCalculation to set defaults for the resources and use a basic bash execution on the input files.\n* Added the MultiplyAddWorkChain for demonstration purposes.\n* Set the background-color of bash code snippets to aliceblue.\n* Added the sphinx-copybutton extension. This still needs to be improved, as currently all code snippets are provided with a copy button, and e.g. those with a Python prompt also copy this prompt.",
      "Remove new processes and sphinx-copybutton\n\nRemove the MultiplyAddWorkChain class and the changes to the\nArithmeticAddCalculation class from this branch, so they can be merged\nseparately into the develop branch.\n\nAlso remove the sphinx-copybutton extension for now, as this addition\nis being dealt with in another issue (#4062).",
      "Replace MultiplyAddWorkChain include with codeblock\n\nReplace the literalinclude for the MultiplyWorkChain with a code-block\nas the module has been removed from this branch.\n\nAlso, clean up the trailing whitespace.",
      "Update docs/source/tutorial/basic.rst\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Apply suggestions from code review\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Docs: Fix code-block formatting\n\nUse proper code-block formatting for the different code-block types.\n\nAdd aliceblue background color for console code-blocks.\n\nAdd output for `verdi daemon status` command, as well as small fixes\nto the text.",
      "Apply suggestions from review",
      "Fix: change accordion by dropdown",
      "Apply suggestions from code review\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Docs: apply reviewer suggestions"
    ]
  },
  {
    "pr_number": 3205,
    "commits_list": [
      "1128c8cbcd8b2c8fbc0126e3c63e9dfb8ec695ef",
      "80f72007f2bfbfeb7f68db133487eb161885ead8",
      "af7ba8aa81212983d754dbe74d2fe996a9dc30fb"
    ],
    "message_list": [
      "Run through backup docs on Ubuntu 18.04\n\nUpdating some lines according to going through most of this page on an\nUbuntu 18.04.02 LTS setup using AiiDA v1.0.0b5.\n\nAdding a few helpful lines elsewhere in docs according to some more\nadvanced information in the backup documentation.",
      "Satisfy (new) pylint",
      "Merge branch 'develop' into fix_3028_docs_working_with_backups"
    ]
  },
  {
    "pr_number": 4861,
    "commits_list": [
      "bfba8e978327d2aa08e13cc8addece4c5bdd681d",
      "4fbaea2423c22f70563d4f8ae89ba59ff354c2e7",
      "46bb0116eeb4090b9d24a5e344f4117642e1800a"
    ],
    "message_list": [
      "Added a remotecat command for printing remote file\n\nThis can be usedful for monitoring the calcjobs while they are\nrunning.",
      "Various minor improvements\n\nAlso adds a test for the `--monitor` option.",
      "Remove the `--monitor` option"
    ]
  },
  {
    "pr_number": 3958,
    "commits_list": [
      "b2af3f3869d33d6837ce2c7f9e1a2d80d2ff45de",
      "341327e99cdf6d87ec31205cf24a460fa1b0d4e4"
    ],
    "message_list": [
      "add github action of transifex upload",
      "Merge branch 'develop' into transifex-action"
    ]
  },
  {
    "pr_number": 3738,
    "commits_list": [
      "1c369a327be2286e67739a705430c16f7e514273"
    ],
    "message_list": [
      "Cleanup the top-level directory of the repository\n\n * Include `bin/runaiida` through `console_scripts` of `setup.json`\n * Remove the outdated examples from `examples` directory\n * Remove obsolete `utils/plugin_tpl/calculation.tpl` superseded by\n   plugin cookie cutter package\n * Move `conftest` to the `tests` directory"
    ]
  },
  {
    "pr_number": 5250,
    "commits_list": [
      "ccc5329dbc7b9b9c7a231acf0f5de2eff8db7952",
      "581695693b9758ba21ee84e6aebe188a8193188d",
      "e61fd73abb5a4a962fc3887dffb182f6c6f4cbe1",
      "56fd860373781395a7a754feba48f04c3d86ff63",
      "034e77b382f759e22d5f4f08d3dc324d02e143f4",
      "54ad5688825b63f8fabfcc46996b98bc1bee6fca",
      "77e996dbcb101d86c565afccb1d7f649b1ee6c61",
      "2e968b43adc39e3e269da36f7b5f7efba7978c30",
      "a8c5ce1985dcdd107815c2da7c909de2a2259493",
      "2f5b53f48f6cf70a62ee933077ac76e58188defa",
      "02067c33f4a4586401d7a80e6e9b5948f0e06eda",
      "549f43f73bbe8f11f2cdba81a5b516e9011a63f6",
      "e1e758d256f5604ae00c4aadff40182f10eddfaf",
      "9e23623321e5f0ecc1913c84ef74f9c3d5c88e1b",
      "ba40f5aec77eb0eed8f31d5bee0bf09607735e00",
      "afa69fc84cee587a889482b2c1e1f2dd566de15f",
      "26191fb322baf04c4b9674e1bd6d300045cd8144",
      "60049cea35e21f029046c46ecfdfbfa8d4a2bf39",
      "7d99acee7c48fb76764766942264f5e8de5d168e",
      "cba54e51d04c949099001eadc842a4a0c8f0376e",
      "1ea9e5e6093a52f8fc39c91a4516ba9ab8072a95",
      "00aa846668a40efbbdc5dfb4c4ab29be27e1835b",
      "235b67be3e3d29ba1c43233afc667584100c15bc",
      "aa1bd86317bb9f5b48c59f1648387a5f3b41f273",
      "b54e6e27b355c74e34388e7f29407698f96984bd",
      "c915cb25d1766b8b98e01885cbcba0ea34865644",
      "34812cc26d6ffe6d2ce2858abd82594b5ca74e65",
      "32845807748495b71b0ae829057fc0ed25975944",
      "ee8b20bd927f70941006ece863d1686492fb9198",
      "97d994a4d971275cca0671a0cd0d2a7fc7509e1f"
    ],
    "message_list": [
      "escape bash with double quotes as an optional",
      "address escape issue for ENV variable",
      "review",
      "double quotes set from computer and to exec command line",
      "update test",
      "seperate std input output",
      "computer_cmdline_params",
      "add use_double_escape to Code and CodeInfo",
      "unittest",
      "add computer_cmdline_params to code_info",
      "Add custom string for code info to maximum flexibility",
      "rename to prepend_cmdline_params",
      "code_info use_double_quotes as a tuple",
      "explicitly set use_double_quotes in every scheduler test",
      "code setup",
      "computer setup",
      "draft container code class from code",
      "test for container code by calcjob",
      "ugly implementaion of code setup CMD for container code",
      "add template for the sarus cmd parameters",
      "use tmpl to set sarus command",
      "do not escape '$' in bash script",
      "rename parameters of ContainerCode init",
      "cmdline for sarus",
      "clean ContainerCode class",
      "rollback to only set image in tmpl",
      "dont escape when there is $ cmdline param",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "rename to ContainerizedCode",
      "rename the variables\n\nuse `container-engine-command`, `is_containerized`, `container_command`,\n`ContainerizedCode`."
    ]
  },
  {
    "pr_number": 2846,
    "commits_list": [
      "ffe1664fa0551aca2f00cf218277850c57bc707e",
      "ab02daa770a2797bf97809bb35045009964a68e6"
    ],
    "message_list": [
      "Separate core of `aiida.manage.external.postgres.Postgres` utility\n\nRemoving the core of the `aiida.manage.external.postgres.Postgres` into\na separate class (PGSU for postgres superuser), which only has the\npuprose to connect to a given database as the postgres superuser and\nexecute SQL commands.\n\n * Make sure psycopg2 will connect to 'localhost' and not try to\n   go via sockets\n * Minor changes in the `Postgres` API\n * Add new options to `verdi quicksetup`:\n   As mentioned in #2836, `verdi quicksetup` actually deals with two\n   users and databases: the database & user to be created for the AiiDA\n   database, and the superuser (& template database) to be used for\n   creating those.\n * Fix construction of `Postgres` instance from aiida profile\n * Switch to providing sensible defaults in `DEFAULT_DBINFO`\n   and dealing with special cases (psql vs psycopg) in the respective\n   functions",
      "Merge branch 'develop' into issue_2836_quicksetup"
    ]
  },
  {
    "pr_number": 3052,
    "commits_list": [
      "9dbb85f65d7659ffe67e4007cb79cb4fe26bf0d6",
      "8b36757f585f20c6c5ca06cebcc284b303fd295f",
      "f8d79c367ce9351096fb940387fcb7fce6fe01b3",
      "707bf6dd91967aaa66555f5f74c388438d241d34",
      "3c1d52e0220ce3a880cfe0157af27765dd71a4df"
    ],
    "message_list": [
      "Refactoring importexport module.\n\nSplit up and then remove `aiida.orm.importexport`.\n\nMoved to `aiida.tools.importexport`.\nSplit up into structure (relative to `aiida.tools.importexport`:\n./dbexport/__init__.py\n./dbexport/utils.py\n./dbexport/zip.py\n./dbimport/__init__.py\n./dbimport/backends/utils.py\n./dbimport/backends/django/__init__.py\n./dbimport/backends/sqla/__init__.py\n./dbimport/backends/sqla/utils.py\n./config.py\n./utils.py\n\nImport/export is now considered a tool, and has nothing to do with the\nspecifics of AiiDA.\nIt should be in a state, where it would be easy to remove it and save\nit in a separate repo.\n\nImport with a django or sqlalchemy backend has been split up.\nFor export, this is not needed, since it is already backend-independent.\nTo further reflect the split of backends, the actual import functions\nhave been placed\nunder `aiida.tools.importexport.dbimport.backends.[django/sqla].`.\nIn the future, `dbimport` should reflect the situation from `dbexport`\nand be backend independent, creating the imported nodes through the\nnew backend-independent ORM entities.\n\nExport to a zip-folder/file has been split up in a separate file.\nThis should later be implemented in another place, since a ZipFolder\nclass exists here.\nFor example `aiida.common.folder`. See also issue #3100.\n\nNote: Be careful with wildcard imports in `__init__` files for backends\nspecific functionality.",
      "Split up and move importexport tests\n\nOriginal file: `aiida.backends.tests.test_export_and_import.py`\n\nSet up tests in `aiida.backends.tests.tools.importexport.`\nmatching the newly moved import/export module, which has been moved to\n`aiida.tools.importexport.`.\n\nTests are split up into separate files according to the test classes in\nthe original file.\n\nAll tests related to ORM entities have been put under\n`aiida.backends.tests.tools.importexport.orm.`.\nWhile `aiida.tools.importexport.orm.` does not exist, this simplifies\nthe overview of the tests.\n\nThe tests were moved under `aiida.backends.tests.` and not\n`aiida.tools.importexport.tests.` because we are currently looking into\nchanging to a pytest API, and it is therefore easier to do this later\nchange having all tests in a single place.\nFurthermore, other tests, which are not backend dependent, are also\nfound here.\nThis is more-or-less for the same reason.",
      "Move export archive migration\n\nMoving module at `aiida.cmdline.utils.migration` to\n`aiida.tools.importexport.migration`.\nUpdate `verdi export migrate` to point at new location.",
      "Add import/export test of `Attributes`",
      "Remove LINK_ENTITY_NAME and ATTRIBUTE_ENTITY_NAME\n\nThese are removed from the importexport module,\nsince they were/are never used."
    ]
  },
  {
    "pr_number": 2487,
    "commits_list": [
      "6f7039e31a9210bd4e1eab12d31226f81fe5182e",
      "99abc2b7ac78dc7fa62e97a3d900ad19f7616b52",
      "bead9e5340d747cf97cabc88c4d90100e47cb1c6",
      "e00a56526ea1e0ed3ea74c93a8b8aac432647606",
      "ac36a176712781cadd7485e44b9ac52c4980cfdf",
      "654cfe730b346b73445e56d085eb0926b92bdebc",
      "7c03278e9f013450876e230dc91c84e06e650da4",
      "07fcabf2dddc78fd0b2ec019dc7c83f159a02d87",
      "434b964457f967d98252096f267ecb70776d679b",
      "9327359a091f4328bce845c1d8f70281006e663a",
      "f5cf79521eb01306a4b4a3a1dde40d9afee36a95",
      "ccdfc3b06502b6f2335d8f3df643624deb526d88",
      "a791f2f630b876f2420d2a066f16c509c5a56c13",
      "d70a5c8ed1157c6dfb8191129da615d119151f8a",
      "8b21ca40176d68d93090adc633d8ed49599efbc1",
      "a339462f4aeb7e49c34b80b73b4ca3ece66fd0c5",
      "24e457c82779e491f51473b9875a4f22e0094c8a",
      "30c6e14cd746fcd98ed36f8e9cb821b2148c91d8",
      "31d37dc736f01e026a4b8dc6681fea3a85ffd494",
      "3aec2a7d285be775523dca281a1be7cbaf758e16",
      "ed5266c3e30a07ff222ed6958b141bb483ad0bc4",
      "e59121aca4e6d574f1b0905103d1c8640275ae92",
      "6202948c8616660c1d65d84d474fb69d801b0d85"
    ],
    "message_list": [
      "add verdi status\n\nShows\n * profile status\n * daemon status\n\nTo add:\n * rmq status\n * postgres status\n * file repo status",
      "add more status messages\n\n + status for postgres\n + status for rabbitmq\n + status for file repository\n\ntodo:\n\n + document this\n + get rid of load_dbenv (needed for rmq at the moment)\n + file repo always created automatically. do we want this?",
      "Merge branch 'provenance_redesign' into issue_2409_verdi_status",
      "move aiida.work.rmq => aiida.manage.external.rmq\n\n + remove mandatory dependency on aiida.common.serialize\n   since it requires the dbenv",
      "Merge branch 'issue_2409_verdi_status' of github.com:ltalirz/aiida_core into issue_2409_verdi_status",
      "remove export of rmq to aiida.work",
      "fix rmq import",
      "add documentation of verdi status",
      "Merge branch 'provenance_redesign' into issue_2409_verdi_status",
      "Merge branch 'issue_2409_verdi_status' of github.com:ltalirz/aiida_core into issue_2409_verdi_status",
      "add basic test",
      "Merge branch 'provenance_redesign' into issue_2409_verdi_status",
      "add different symbols for UP/DOWN\n\nso that you can paste them into forums/on github without losing\nthe meaning",
      "fix capturing + test error\n\n * capture stderr as well\n * stop communicator after creating it",
      "Merge branch 'provenance_redesign' into issue_2409_verdi_status",
      "switch to frozendict",
      "Merge branch 'issue_2409_verdi_status' of github.com:ltalirz/aiida_core into issue_2409_verdi_status",
      "fix layout",
      "move printing outside try/except",
      "rework postgres connection determination\n\n * use parameters specified in profile to check connectivity\n * fix #2519",
      "move away from frozendict\n\n * move back to regular dict (I thought frozendict was used elsewhere\n  but apparently it isn't)\n * make Postgres.from_profile more robust to be able to handle\n   test configurations that are missing data",
      "Merge branch 'provenance_redesign' into issue_2409_verdi_status",
      "add fixes by @sphuber"
    ]
  },
  {
    "pr_number": 5691,
    "commits_list": [
      "36118c8a741784ccbfd85d60ef1d517f6aa9c7f6"
    ],
    "message_list": [
      "`ProcessFunction`: Add support for variadic arguments\n\nUp till now, variadic arguments, i.e., arguments defined as `*args` in a\nfunction signature which collects any remaining positional arguments,\nwere not supported for process functions. The main reason was that it\nwasn't immediately clear what the link label should be for these inputs.\n\nFor normal positional arguments we can take the name of the argument\ndeclaration in the function signature, and for keyword arguments we take\nthe keyword with which the argument is passed in the function invocation.\nBut for variadic arguments there is no specific argument name, not in\nthe function signature, nor in the function invocation. However, we can\nsimply create a link label. We just have to ensure that it doesn't clash\nwith the link labels that will be generated for the positional and\nkeyword arguments.\n\nHere the link label will be determined with the following format:\n\n    `{label_prefix}_{index}`\n\nThe `label_prefix` is determined the name of the variadic argument. If a\nfunction is declared as `function(a, *args, **kwargs)` the prefix will\nbe equal to `args` and if it is `function(*some_var_args)` it will be\n`some_var_args`. The index will simply be the zero-base index of the\nargument within the variadic arguments tuple. This would therefore give\nlink labels `args_0`, `args_1` etc. in the first example.\n\nIf there would be a clash of labels, for example with the function def:\n\n    def function(args_0, *args):\n\nwhich when invoked as:\n\n    function(1, *(2, 3))\n\nwould generate the labels `args_0` for the first positional argument,\nbut also `args_0` for the first variadic argument. This clash is\ndetected and a `RuntimeError` is raised instructing the user to fix it."
    ]
  },
  {
    "pr_number": 4792,
    "commits_list": [
      "09463c4542bb0439b6353a70dcf22d5fafa82565",
      "9d765ce50a225282df2c5e02d3809ab105b3d848",
      "43236cc01f1186796fb5a326c3ad3c04428bdd2d",
      "68974a635a947842f240db06dcf103e1c6f4ad82",
      "533c42ed7c8ae36a76a021707b5f1df44070bf8b",
      "4feccf65327499a6177e5918b3de761fff298386",
      "6f041a56f7a185beefe7dc2f95f18eff9da68fcc",
      "87be4170869f68b447171024e8c1265c6ae53261"
    ],
    "message_list": [
      "\ud83d\udc1b FIX: Task.cancel` should not set state as EXCEPTED",
      "Merge branch 'develop' into fix-task-cancel",
      "Apply suggestions from code review",
      "Merge branch 'develop' into fix-task-cancel",
      "Merge remote-tracking branch 'upstream/develop' into fix-task-cancel",
      "Update kiwipy/plumpy versions",
      "add test",
      "fix pre-commit"
    ]
  },
  {
    "pr_number": 5054,
    "commits_list": [
      "2f87d7431b5120486753aa6bd676684b1c70a6d3",
      "9e95849eb517c7fec23edd744ac1950a83bac36f"
    ],
    "message_list": [
      "REST API: make the profile configurable as request parameter\n\nTo make this possible, after parsing the query string but before\nperforming the request, the desired profile needs to be loaded. A new\nmethod `load_profile` is added to the `BaseResource` class. All methods\nthat access the storage, such as the `get` methods, need to invoke this\nmethod before handing the request.\n\nThe `load_profile` method will call `load_profile` with `allow_switch`\nset to True, in order to allow changing the profile if another had\nalready been loaded. The profile that is loaded is determined from the\n`profile` query parameter specified in the request. If not specified,\nthe profile will be taken that was specified in the `kwargs` of the\nresources constructor.\n\nNote that the parsing of the request path and query parameters had to be\nrefactored a bit to prevent the parsing having to be performed twice,\nwhich would result in a performance regression.\n\nWhen the REST API is invoked through the `verdi` CLI, the profile\nspecified by the `-p` option, or the default profile if not specified,\nis passed to the API, which will be passed to the resource constructors.\nThis guarantees that if `profile` is not specified in the query\nparameters the profile with which `verdi restapi` was invoked will be\nloaded.",
      "Add global config option `rest_api.profile_switching`\n\nThis global config option is set to `False` by default. When set to\n`True`, the REST API will allow requests to specify the profile and it\nwill switch the loaded profile when necessary. If a request specifies\nthe profile query parameter and profile switching is turned off, a 400\nBad Request response is returned."
    ]
  },
  {
    "pr_number": 5454,
    "commits_list": [
      "6244f60987c2c8d3d3913c753e61bbc3c7f93267"
    ],
    "message_list": [
      "Made singlefile initializable empty"
    ]
  },
  {
    "pr_number": 4470,
    "commits_list": [
      "f53b3e9c1a4ee6a69c64c31bf9563d01423639fe",
      "6c9578e213ca418fcb77cd04bbe31c663b423d24",
      "e6a76c42e3f6f0b5f29009b3073932f55fa819a2",
      "648f68dfc60741a1475a7094dc7ead61ca827a0b",
      "772b40f520d57ea2741e511d825252ba8f3e57a3",
      "cd37991111912f6299e71f6274ba441bdebe75bd",
      "0b38ff1da22675b38d47008abe6f3043ee7b323b",
      "1ab9af8287521399ab3ec3e5fda0e9b960f7d1e5",
      "29954d44dc35ecb09da35671b6fb9dc6dcdf9eb4",
      "1b3d522872c0c381c2e78d63b4dd2531e36525e6",
      "446d1a3f41529d8d74bf4d53f8a79b544e1c3140"
    ],
    "message_list": [
      "Docs: \u2b06\ufe0f Updata sphinx version + extensions\n\nAlso moved sphinxext testing to the official sphinx testing infrastructure",
      "Merge branch 'develop' into docs/update-versions",
      "update",
      "Fix tests",
      "Merge branch 'develop' into docs/update-versions",
      "Update setup.json",
      "Fix autodoc bug",
      "Merge branch 'develop' into docs/update-versions",
      "fix test",
      "roll-back autodoc",
      "fix List.pop"
    ]
  },
  {
    "pr_number": 3957,
    "commits_list": [
      "9d4cfc4230d8424b43310bc5a69e5175f4988be1",
      "08cdff4b67f17351bab645b5ac2948132df3ba64",
      "3b39e23ac51c1c19b2081fb2d53273d7fc0f7626",
      "bfa8ccfe63ef2b49050c9d883a58f8c6b08d02c8",
      "0f650b0fdc9f58f09b1158ece93f643612252192",
      "4aae357d1d07472da25ce835b3ce61695f4447a7",
      "5c3e8f4a1162b0d7b0633a89006a1a7d8306121c",
      "2fef6856f86dee89704e6e6982bc60d1c4772b82",
      "ba632c8d3b790b72830e08f88a424a21356e5b17",
      "f2440b5f670556bb30bb83dc7eb1dd44bd667fb5",
      "59bb168f4951a11dba8cf70cba2664fd1ef1c504",
      "232703f4426242604a36df14bbb6cac933b1ffbd",
      "b91c51d295b5aeccdb58792f4b398bf3e0e5cc73",
      "412411b2137f5add7a0611da9c9dc5f3a750795c"
    ],
    "message_list": [
      "Update 'test-install' workflow to address requirements/ issues.\n\nIdentify and attempt to remedy inconsistencies of the requirements/\nfiles with setup.json.",
      "Update commit comment for inconsistent requirements/ files.",
      "Use Python 3.8 consistently for dependency checks.",
      "Require check-requirements job to succeed to execute 'tests' jobs.\n\nWith inconsistent requirements/ files those tests are misleading.",
      "Provide detailed error message for missing matches of dependencies.\n\nWith some refactorization of the utils/dependency_management.py script.",
      "Allow to turn off file annotation as part of GitHub actions workflow.\n\nTo avoid the duplication of the warnings as part of the 'test-install'\nworkflow.",
      "Add README.md for requirements/ directory.\n\nTo provide some context for the purpose of the contained files.",
      "Ignore unrelated files for check-requirements command.",
      "Only create commit comment for expected check-requirements errors.",
      "Simplify comments in test-install workflow to be more concise.\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Improve commit and PR comments.\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Improve wording of requirements/README.md\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Fix line number in commit warning message referring to setup.json.",
      "Remove obsolete update-requirements workflow."
    ]
  },
  {
    "pr_number": 4564,
    "commits_list": [
      "39cabfad20daf2cd41ae9d6f8ec5381885a48eff",
      "7fbc4ce10f2781cf2140fd658bbfe684e12c7a02",
      "60ebfaaf4033f8d1acde442286772644cba20017",
      "e2675087dc77ad1f55a07c53cbd91b0407bc6da5"
    ],
    "message_list": [
      "Add .dockerignore",
      "Update .dockerignore",
      "Merge branch 'develop' into add/dockerignore",
      "Merge branch 'develop' into add/dockerignore"
    ]
  },
  {
    "pr_number": 2049,
    "commits_list": [
      "1f5b79ff4c241abfefb3615576c8f3c0586bbaac",
      "d8cea6c00d749ffb8ca12499cd809d643369acd8"
    ],
    "message_list": [
      "Fix several issues with the hashing method\n\n* dict items are sorted _after_ hashing, fixing the behavior for\n  keys which can not be sorted\n* unicode/str and str/bytes are explicitly handled, with a different\n  type salt\n* in general, the combination between two hashes is done via the\n  method of boost::hash_combine, not by concatenating the hexdigest",
      "Add docstrings, remove dead code."
    ]
  },
  {
    "pr_number": 3327,
    "commits_list": [
      "d98976e03d986e120c698b435757eef2f4f461c6"
    ],
    "message_list": [
      "Fix the identifier format of the caching configuration file\n\nThe old caching configuration file implementation expected the module\npath of node sub classes that should be either enabled or disabled for\ncaching, for example:\n\n    `aiida.orm.node.calculation.job.TemplatereplacerCalculation`\n\nThe old caching implementation in the `Node.store` method would\nthen compare these directives to the node type of the current node being\nstored to determine whether it should be considered for caching. This\nmethod broke after the separation of the \"process\" and \"node\" concepts.\nSince calculation job classes, for example, are now no longer a sub\nclass of `Node`, but of `Process`, the corresponding node that will be\ncreated will all be `CalcJobNode` instances, regardless of the exact\nprocess sub class. That particular bit of information is stored as its\nentry point string in the `process_type` column of the node. For example:\n\n    `aiida.calculations:templatereplacer`\n\nThis entry point string is now also the identifier that determines the\ncaching behavior and should be used in the configuration file."
    ]
  },
  {
    "pr_number": 5319,
    "commits_list": [
      "c5e9d192466a1e50a044918a420e28c6fa6b73b9",
      "5f64ec17b46b78d481a436ec60ddda0c53523bd6",
      "e483a74b37a326e6b3900d6bac243a5efcc601f9",
      "fc142ab110b516216c8ddd9d36ce17f2c80ebc08",
      "16a2703384bf481cee88153cd342e3ec64163bca",
      "e3c059a8f0ef43fb65f7708072bbe81f6146ea6e",
      "532f3152feee2c736a80f80d5ee505455cc233de",
      "bd3a6df7cb879c7fdf9db1f0edab861d24d34423",
      "baea9cec3080d138407b5b5262b3bbaa727d6a56",
      "4b797eb519f6066167dfd27eb0603621f23f05ef",
      "2f9fa47f977c6e315beb5ff82b2f5094c7b83def",
      "0cfae86eba6a77e701d57313b8ddad20ab6e1dd4",
      "8e4b70a40d49ad2117d07f56e0678ee650fccf83",
      "e34d0410b95cf70d68a7975cff902c0628b225ff"
    ],
    "message_list": [
      "\ud83d\udc4c IMPROVE: Configuration migration\n\nAllow for both upgrades and downgrades, and expose this in the CLI.\n\nThis will allow a route for user to downgrade their configuration,\nif they try a new version of aiida, but then want to return to an old version.",
      "Merge branch 'develop' into storage-config",
      "address review comments",
      "Merge branch 'develop' into storage-config",
      "add new line between class attributes",
      "Update verdi_config_downgrade and add test",
      "Merge branch 'develop' into storage-config",
      "fix docstrings",
      "Add failure tests",
      "add typing",
      "Update migrations.py",
      "remove options",
      "add choice",
      "Merge branch 'develop' into storage-config"
    ]
  },
  {
    "pr_number": 4229,
    "commits_list": [
      "82faf15391b54cd1649e9110608acdf37b6736c2"
    ],
    "message_list": [
      "ci: run all on python 3.8\n\nFor unknown reasons, the python3.7 environment used for the \"verdi\" and\n\"pre-commit\" jobs was broken."
    ]
  },
  {
    "pr_number": 4457,
    "commits_list": [
      "10b44d9100a162c631672e415c39d7a1ac3b5437",
      "b7494f0349cf57bcda56fb42252215302f648ae6"
    ],
    "message_list": [
      "Docs: Add redirects from old documenation",
      "Merge branch 'develop' into docs/redirects"
    ]
  },
  {
    "pr_number": 2015,
    "commits_list": [
      "cb29f50aaa447e87af1bf76d1aa934f1522f5fae",
      "91b7ca8f00444b8ca2f3096189cbeb99f586b26c",
      "7751c2a3e766c94c65139b21034aeb3675ed993d"
    ],
    "message_list": [
      "Adding representer and constructor for AttributeDict.",
      "Add a docstring to 'add_mapping_representer'.",
      "Use single '!' for tag."
    ]
  },
  {
    "pr_number": 652,
    "commits_list": [
      "ce539d8b97a5059e2a9306b9cfc99737c9f29da4",
      "7589afdbb535a814e2a10b3c8bb916ae2d85e091",
      "13caff274d9965613dd0b374c87bf3a6463a125e",
      "ca5f0477021a27fe98e7ec503fff05fd9abec222",
      "87e546fc8fbd3856464db3f02deb5c8453b3f1f3",
      "39d378e3bb1cf1487237079a448a01c1540cf7b5",
      "f7e6fe039ffa4e317615edc4f0cdf2663e590cb2",
      "28af8ed96a3ec8c47dbf0873ca4decf2b37f0915",
      "f12c3cb8bd87f38e82b89fac12ca81330fd6ed6a",
      "219d53ba0b826de0e6438492e908e3fe2466fe93",
      "76acbb6e17d8dce555cba1820502fe6f5693b648",
      "2de9bc82e2a99c70fb9e51cd74a0959dc0a66fc2",
      "a4ae949e5fcce8de3e6a798085bfa5667ffacb52",
      "516c44e89531d93eea2de55cd8f7f9cae5e48e58",
      "dcb4a38b7c432edfba957c9598fbda6cc7547c7f",
      "bbe4c33ad582e45885679df00e3fdb83612a1257",
      "fb182ccb3116173f1a7df46f179a35d003066359",
      "dc939620a5c0bd1ec90024134fc86c818f9419ce",
      "5fef7f2fe1a26c20509ff947278398ada372073f",
      "f04d3bb8c44310746ed73815879383b9addbd9d8",
      "1bc7c90b38c22fb7e9338f5423653c77846393a2",
      "c5df6bf6c54dade644304bd51b5df34dbc18a9e6",
      "e5c72a186600b6c8407427f879ea03794a269a99",
      "a896eb102d983742f3b27f050dea9fb389a02c81",
      "d13f8dd37ac7fb60eb5f31feacf3f1fb3aeeae7a",
      "533e9684851dd3954e6c4ce6d6bd4f891af2401b",
      "55d1ad0c41286786787e4f2f405a9eca4f0def82",
      "29f14bae175ab70c35b05dbdcb5abe0eaabcced6",
      "7a0d4efbf2da476cbd41f2765dc8442a26a60f0a",
      "148770df25dcbd2b66d14a2d276dac961f0b2c8a"
    ],
    "message_list": [
      "#119 added first small test to check for node hashing",
      "#119 Added small functions get_hash and get_same_node to Node",
      "#119 Added functionality to store method (only for Django backend) that replaces the _dbnode member if a similar node already exists",
      "#119 Try-except clause if hashing fails, and also check for None for hash, which obviously should not be checked in the DB",
      "#119 Fix in test that was failing because there is now an additional extra (hash)",
      "* Failing test for two small but unequal floats\n* Failing test for two ArrayData with unequal content\n* (Accidentally) passing test for two ArrayData of different size, with same str representation\n\nFor the ArrayData, we need to take the actual array into account when creating the hash,\nnot just the shape which is return in get_attrs()",
      "Merge pull request #591 from greschd/node-hashing\n\nAdd more node hashing tests",
      "Merge branch 'develop' of github.com:aiidateam/aiida_core into node-hashing",
      "Merge branch 'node-hashing' of github.com:aiidateam/aiida_core into node-hashing",
      "Give name to '_' in loop",
      "Update hashing algorithm",
      "Fix get_hash for special case of ArrayData",
      "Fix ArrayData issue by hashing folder content for 'pathlib.Path'",
      "Explicitly raise error when non-directory path is hashed",
      "Add pathlib2 requirement",
      "Print modulename to check failing Travis test",
      "Try moving ArrayData import inside the test",
      "Move numpy import to test",
      "Add comma after checksumdir requirement",
      "Add find_same to store_all, add to SQLAlchemy",
      "Simplify get_same_node",
      "Set hash extra on SQLAlchemy nodes",
      "Change how extra is stored on SQLAlchemy node",
      "Use Folder interface for hashing the repository folder",
      "Add logic to ignore attributes",
      "Add tests for FolderData with empty files and folders.\nClose files after reading them to create the hash.",
      "Add functionality to set the default of caching True / False",
      "Merge branch 'develop' of github.com:aiidateam/aiida_core into node-hashing",
      "Add contextmanager to enable / disable caching",
      "Change caching default in store methods to False.\n\nThe caching.defaults.use_cache parameter should be used by\nplugin developers to mark whether a specific .store() call\nCAN actually use caching. The user decides whether to use it\nin the end by setting the default to True / False."
    ]
  },
  {
    "pr_number": 3057,
    "commits_list": [
      "6f000cbf015fe60fbca4ed82cfeb632020b907b0"
    ],
    "message_list": [
      "Add aiida-core version to docs home page (1.0 branch)."
    ]
  },
  {
    "pr_number": 5330,
    "commits_list": [
      "97c96955f3bc89604aea46a2bb185d7c0f9510b4",
      "0e5099d7dd11fb68add06e2fbe4da8839640cd6f",
      "8603fb1154ed18f82a008c54582efd26b94ed55e",
      "5f983291e815361de9ae5f51690842649ce06537",
      "3c3fdf0ff34fcae62a9f2a2131ffc649e2c37b36",
      "7593d5bfc2b7b0211eb539d29282cefd0c79ceb1",
      "7d20946207e942b54bd7cc6b6ca697d620541a26",
      "c1e0e6511c957210ad1612a4f24a203fa5512520"
    ],
    "message_list": [
      "\ud83e\uddea TESTS: Add migration schema regression files\n\nThis commit pre-emptively adds auto-generated YAML files,\nwhich fully describe the database schema at each migration revision,\nfor both the django and sqlalchemy migrations.",
      "\u267b\ufe0f REFACTOR: Remove Django storage backend",
      "\ud83d\udcda DOCS: Update verdi CLI outputs",
      "\ud83d\udc4c IMPROVE: ArchiveReadOnlyBackend\n\nSo it can be used via a profile",
      "\ud83d\udc1b FIX: Account for variable legacy django database schema\n\nSee `aiida/backends/sqlalchemy/migrations/utils/reflect.py` for details.",
      "\ud83e\uddea TESTS: Fix sqlalchemy deprecation warnings",
      "\u267b\ufe0f REFACTOR: Implement naming conventions for PSQL schema\n\nAs described in https://alembic.sqlalchemy.org/en/latest/naming.html,\nnames are now auto-generated with specific conventions,\nspecified in `aiida/backends/sqlalchemy/models/base.py`.\nThe renaming, in the final migrations of the django and sqlalchemy branches,\nhave also been re-written to use the same code,\nensuring consistency.",
      "\ud83d\udd27 MAINTAIN: Add `NotImplementedError` for migration downgrades\n\nDowngrading storage version is not explicitly supported in aiida-core,\nor exposed for the user.\nPrevious to #5330, some downgrade functionality was required for testing,\nsince migration tests involved migrating down the global profile,\nthen migrating it back up to the target version.\nThese migrations though were often incomplete,\nmigrating database schema but not the actual data.\n\nNow, migration tests are performed by starting with a separate, empty profile,\nand migrating up.\nSince these downgrades are no longer required and are un-tested,\nwe replace their content with an explicit `NotImplementedError`."
    ]
  },
  {
    "pr_number": 2478,
    "commits_list": [
      "326bbccf0474a48cf838f7ba7082f70337ecb0b6",
      "1e7d93cf9535b7bf63f60fc884060ceb8bfcf3ef",
      "0599dabf0887bee172a04f308307e99e3c3f3ff2",
      "f379547ff79a6d877a5124e7415084733c0164b9",
      "e9f71a597b7aeb669ad72e7929fef068cab6b8b1",
      "cf5ee1da71ae5ce8bc148b551eb765c27dc0d8b8",
      "6142c1def7d9deba65892c2067ad5a25adb4523a"
    ],
    "message_list": [
      "Implement changes of database migrations for export archives\n\nThe following database migrations have been translated into a migration\nfor the contents of existing export archives with version 0.3:\n\n 9, 14, 16, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28\n\nThe ones that have been skipped were not applicable for export archives",
      "Setup the tests for export migrations of `v0.3` to `v0.4`",
      "Add latest migrations:\n\n- Rev. 0029\n- Rev. 0030\n- Rev. 0031\n- Rev. 0032\n- Rev. 0033\n\nIntroduce changes to `metadata.json`:\n\n- Add `Log` to `all_fields_info` and `unique_identifiers`\n- Add `Comment` to `all_fields_info` and `unique_identifiers`\n- Changes to key values of `all_fields_info`\n\nAdd extra dicts to `data.json`:\n\n- `node_extras`\n- `node_extras_conversion`\n\nBoth will be empty dicts.\nIf `Node`s are present in the exported file, key-value-pairs will be added,\nwhere the key is the `Node` pk/id and the value is an empty dict.\n\nRenamed new test file by prepending `test_`\n\nUpdated `test_v3_to_v4` in new test file to migrate `export_v0.3_no_UPF.aiida`\ninstead of `arithmetic.add.aiida`.\n\nRemoved UPF node attributes information from `data.json` files for:\n\n- `export_v0.1.aiida`\n- `export_v0.2.aiida`\n- `export_v0.3.aiida`\n\nRenamed them by appending `_no_UPF`.\n\nMigrated `export_v0.3_no_UPF.aiida` to export version 0.4.\n\nMigrated other existing export files under `fixtures` to export version 0.4.\nBut only for those that are expected to be the newest version.\n\nUpdated `test_calcjob.py` and found an issue for `inputcat` and `outputcat`.\nThis has been reported in issue #2611.\n\nUpdated all tests that use exported files from `fixtures`:\n\n- `test_export_and_import.py`\n- `test_calcjob.py`\n- `test_export.py`\n- `test_import.py`\n- `test_archive.py`\n\nAdded test `test_no_node_export` to check migration of export file with no Nodes succeed.\nAdded `export_v0.3_no_Nodes.aiida` export file for this test.\n\nUpdated test utils file `fixtures.py` to incorporate the  `get_archive_file` function\nfrom `test_export.py`.\n\nAll instances of reimplementations of this function have been updated\nwith the new `get_archive_file` util function.\n\nInserted key checks, if migration of an export file should have been partially performed,\nfor one reason or another.\n\nUpdate arithmetic.add.aiida",
      "Moving all migration functions out of `cmd_export.py` and into `aiida.cmdline.utils.migration`.\n\nFollowing files have been created:\n- `__init__.py`: Contains `migrate_recursive` and allows for import from `aiida.cmdline.utils.migration`\n- `utils.py`: Contains `verify_metadata_version` and `update_metadata`\n- `v01_to_v02.py`: Contains function to migrate from export version v0.1 to v0.2\n- `v02_to_v03.py`: Contains function to migrate from export version v0.2 to v0.3\n- `v03_to_v04.py`: Contains functions to migrate from export version v0.3 to v0.4\n\nMoving and renaming new test-file `test_export_file_migrations.py` from `aiida.backends.tests`\nto `aiida.backends.tests.cmdline.utils.migrations`, renaming it to `test_v03_to_v04.py`.\n\nSee previous commits for actual authors of the code.",
      "New tests for export file migrations - now in separate repo \"aiida-export-migration-tests\".\n\nTests for the following files in `cmdline.utils.migration`:\n- `__init__.py` in `test_migration.py`\n- `v01_to_v02.py` in `test_v01_to_v02.py`\n- `v02_to_v03.py` in `test_v02_to_v03.py`\n- `v03_to_v04.py` in `test_v03_to_v04.py`\n\nFound and fixed bug for migration of TrajectoryData in Rev. 1.0.26 and 1.0.27 for v0.3 to v0.4:\n\"symbols\" was only updated for \"node_attributes\" not \"node_attributes_conversion\".\n\nAdded more representative export files for the different export versions.\nThey have been created using the same workflow (by @yakutovicha) in specialized QuantumMobile VMs.\nAiiDA version used for export versions:\nExport: 0.2, AiiDA: 0.9.1\nExport: 0.3, AiiDA: 0.12.3\nExport: 0.4, AiiDA: 1.0.0b2\n\nExport version 0.1 was set to AiiDA version 0.6.0,\nbut AiiDA was never released on PyPi until version 0.8.0,\nand the changes from export version 0.1 to 0.2 are very minor.\nSo a more representative export file for export version 0.1 is left out.\nThe export files are in the separate repo.\n\nAdded dependency for separate repo if \"testing\" is chosen when installing via pip.\n\nTo be pythonic and have more transparent code,\nthe migration steps were divided into separate functions.\nThis also results in the fact that a lot of migrations can now be skipped,\nif no Nodes are present in the export file.",
      "Update to EXPORT_VERSION 0.5\n\nAdd migration from v0.4 to v0.5.\nThis includes migration 0034 and 0036 (0035 has no effect).\n\nCreate new simple export file for v0.5 and update tests to use it.\nMigrate other export files.\n\nUp version dependancy of `aiida-export-migration-tests` to 0.5.0.",
      "Fixes according to @sphuber review"
    ]
  },
  {
    "pr_number": 4345,
    "commits_list": [
      "ac0d20987b275a1b780f6c16ea735dc092bca704",
      "30fcf6ab58fc09d9345543895ef501ac8183531d",
      "45e7ca25fe8d74be09520f113b6566692cb82407",
      "583d09f779dc8cdf9e2db2419132d9edc2cedd85",
      "ee7a65ec8cc528a95798eeb0ff85a35b17c1e8db",
      "b89411c0a141d9d0ce067a28096fd2dcedeb3467",
      "f882df80fa6569d168c13701b2353bf8d4bcff6a",
      "2d4ae66f3c762a828fea2fc0dad2831b4519e609",
      "a3930ffaddb5e94caac02ecd3993ce8c5b5d00f8",
      "f30f000f985d0acf63c00b7f0d63ea6660feb9f6",
      "03bb76857457810704adc5f65a1ade66d3571079",
      "36939cdecede72834160c146425835f81d0b29aa",
      "0e250e6aa59dc81242eea4b0437bb2df31a687ec"
    ],
    "message_list": [
      "Add the `repository_metadata` column to the `Node` database models\n\nThis new JSONB column will be used to store the metadata of the contents\nof the file repository. It essentially is a mapping of the file object\nhierarchy, where each object contains the relative filepath of it within\nthe repository, which is purely symbolic, and a hashkey, which is the\nactual identifier that uniquely identifies the object in the repository\nbackend.\n\nThe column is made nullable and there is no default set. This is to make\nsure to not add unnecessary bytes to the database for all the nodes that\ncontain no files whatsoever, which will be a decent chunk. The front-end\nORM class `Node` will return an empty dictionary if the database model\nfield is null, such that the clients don't have to deal with `None`.",
      "Add the base implementation for the new repository\n\nThe `Repository` class is the new interface to interact with the file\nrepository. It contains a file hierarchy in memory to now what is\ncontained within it. A new instance can be constructed from an existing\nhierarchy using the `from_serialized` class method.\n\nThe actual storing of the objects in a repository is delegated to a\nbackend repository instance, represented by `AbstractRepositoryBackend`.\nThis backend class is only tasked with consuming byte streams and\nstoring them as objects in some repository under a unique key that\nallows the object to be retrieved at a later time. The backend\nrepository is not expected to keep a mapping of what it contains. It\nshould merely provide the interface to store byte streams and return\ntheir content, given their unique corresponding key.\n\nThis also means that the internal virtual hierarchy of a ``Repository``\ninstance does not necessarily represent all the files that are stored by\nrepository backend. The repository exposes a mere subset of all the file\nobjects stored in the backend. This is why object deletion is also\nimplemented as a soft delete, by default, where the files are just\nremoved from the internal virtual hierarchy, but not in the actual\nbackend. This is because those objects can be referenced by other\ninstances.\n\nThe interface is implemented for the `disk-objectstore` which is an\nefficient key-value store on the local file system. This commit also\nprovides a sandbox implementation which represents a temporary folder on\nthe local file system.",
      "Improve the storage effiency of the `repository_metadata` format\n\nSince now all nodes will have a full representation of the virtual file\nhierarchy of their repository content, which is stored in the database\nin the `repository_metadata` column, it is crucial that the format of\nthat metadata is optimized to use as little data as possible, to prevent\nthe database from bloating unnecessarily.\n\nCurrently, we only define two file types:\n\n * Directories: which can contain an arbitrary number of sub objects\n * Files: which cannot contain objects, but instead have a unique hashkey\n\nThis means that we can serialize the metadata, without encoding the\nexplicit type. If it contains a key, it is necessarily a file. If it is\nnot a file, it has to be a directory.\n\nThe serialization format therefore can make do with simple nested\ndictionaries, where each entry either contains the key `k`, indicating\nit is a file, or `o`, which can be a dictionary itself, containing the\nobjects contained within that directory. Note that we can even safely\nomit the `o` key for an empty dictionary. Since there is no `k` we know\nit has to be a directory anyway.\n\nThis logic is implemented in `FileObject.from_serialized` that can fully\nreconstruct all instance properties by inferring them from the sparse\nserialized data.",
      "Remove the code to perform incremental backup of the file repository\n\nThis code was put in place because a naive backup of the file repository\nwas impossible already for reasonably size projects. The problem was the\nunderlying design where each node had an associated folder on disk, even\nif it contained no files, and all files were stored separately, creating\na huge numbers of files. This meant that just rsync'ing the folder could\ntake days even to just calculate the difference with the backed up\nfolder.\n\nThe ad-hoc solution was this script that would only transfer the folders\nof the nodes added since the last backup date, the list of which was\ndetermined by a simple query. However, now that the new file repository\nis implemented, which has been explicitly designed to be easily backed\nup by tools like `rsync`, this custom solution is no longer needed.",
      "Integrate the new `Repository` implementation\n\nThe new `Repository` implementation is integrated by wrapping it in a\nnew class `NodeRepository` that is mixed into the `Node` class. This\napproach serves multiple purposes.\n\n 1. The class serves as a backwards compatibility layer between the old\n    repository interface, which would allow clients to set the mode for\n    file handles for reading, as well as passing in text-like streams,\n    and the new interface, which exclusively operates with bytes. The\n    interface will decode the bytes when the caller is expecting normal\n    strings in order to not break the interface.\n\n 2. It allows to implement the concept of immutability which is a\n    concept of the `Node` and is completely unknown to the `Repository`\n    class.\n\n 3. It nicely separates the file repository functionality into a\n    separate file preventing the already huge `Node` implementation from\n    getting even larger.\n\nThe downside of the latter is that the `NodeRepository` class contains\nsome potentially confusing logic. It accesses `Node` properties, but\nfrom the code it is not clear where they come from. Since this class is\nonly meant to be used as a mixin for the `Node` class, I think that is\na sacrifice that is worth the benefits.",
      "Add the repository mutating methods to the `Sealable` mixin.\n\nThis ensures that the repository is still mutable for sealable nodes as\nlong as they are not yet sealed. After sealing, the repository of even\nthese nodes becomes fully immutable.\n\nNote that the implementation completely overrides the methods of the\n`NodeRepositoryMixin`, because it needs to override the check in those\nthat will raise if the node is stored. An alternative would have been to\ncall the `super` from the `Sealable`, which due to the correct MRO,\nwould in fact the underlying method on the `NodeRepositoryMixin`, but it\nwould need to accept an argument, like for example `force`, to skip the\nmutability check. This is not desirable, however, since those repository\nmethods are public methods of the `Node` interface and so any user will\nbe able to disable the mutability check for any node.\n\nThis solution does not suffer from that vulnerability but the downside\nis that the implementation of the method from the `NodeRepositoryMixin`\nneeds to be copied and it needs to be kept in sync.",
      "Docs: add \"Internal architecture - File repository\" section\n\nThis section describes in detail the new file repository design,\nincluding the design guidelines that were kept in mind and what\nrequirements it was designed to respect. It also gives a detailed\noverview over how the implementation is integrated in the existing\ncode base.",
      "`Repository`: allow `File` class to be changed",
      "Add migrations for existing file repositories\n\nThe migration of an existing legacy file repository consists of:\n\n 1. Make inventory of all files in the current file repository\n 2. Write these files to the new backend (disk object store)\n 3. Store the metadata of a node's repository contents, containing the\n    virtual hierarchy to the corresponding `repository_metadata` column\n    in the database.\n\nThe repository metadata will contain a hashkey for each file that was\nwritten to the disk object store, which was generated by the latter and\nthat can be used to retrieve the content of the file.\n\nThe migration is performed in a single database migration, meaning that\neverything is executed in a single transaction. Either the entire\nmigration succeeds or in case of an error, it is a no-op. This why the\nmigration will not delete the legacy file repository in the same\nmigration.\n\nThe advantage of this approach is that now there is no risk of data loss\nand/or corruption. If the migration fails, all original data will be in\ntact. The downside, however, is that the content of the file repository\nis more or less duplicated. This means that the migration will require a\npotentially significant amount of disk space that is at worst equal to\nthe size of the existing file repository. This should be the upper limit\nsince the disk object store will both deduplicate as well as compress\nthe content that is written.",
      "First implementation of export/import using new disk object store\n\nUse the `Container.export` method in export/import functionality",
      "Database: add cross-reference of repository UUID to settings table\n\nThe new repository implementation, using the `disk-objectstore`\nunderneath, now provides a UUID for each repository. Currently, each\ndatabase can only be configured to work with a single repository. By\nwriting the UUID of the repository into the database, it will become\npossible to enable a consistency check of the repository and database\nthat are configured for a particular profile. This will prevent\naccidental misconfigurations where the wrong repository is coupled to a\nparticular database.\n\nThe repository UUID is generated automatically by the `disk-objectstore`\nlibrary when the container is created and it provides an interface to\nretrieve it. This value is written to the database in the `verdi setup`\ncommand when a new profile is created. If the database already has the\nrepository UUID setting defined, it will be cross-referenced with the\none of the repository to make sure it is compatible. This case is to\nfacilitate the creation of a new profile for an existing repository and\ndatabase. However, if the UUIDs do not match, the setup command fails.\n\nFor databases that were created before this commit, the database\nmigration that performed the migration of the legacy file repository\nincludes a statement that will insert the UUID of the new object store\ncontainer once is has been created.",
      "Profile: check database and repository compatibility on load\n\nSince the migration to the new repository implementation, each file\nrepository has its own UUID. This UUID is now written to the settings\ntable of the database that is associated to it. This allows to check\nthat the repository and database that are configured for a profile are\ncompatible.\n\nThe check is added to the `Manager._load_backend` method, as this is the\ncentral point where the database is loaded for the currently loaded\nprofile. We need to place the check here since in order to retrieve the\nrepository UUID from the database, the corresponding backend needs to be\nloaded first.\n\nIf the UUID of the repository and the one stored in the database are\nfound to be different, a warning is emitted instructing the user to make\nsure the repository and database are correctly configured. Since this is\nnew functionality and its stability is not known, a warning is chosen\ninstead of an exception in order to prevent AiiDA becoming unusable in\nthe case of an unintentional bug in the compatibility checking. In the\nfuture, when the check has been proven to be stable and reliable, the\nwarning can be changed into an exception.",
      "CLI: fix loading time of `verdi`\n\nThe `from disk_objectstore import Container` import has a significant\nloading time. The `aiida.manage.configuration.profile` module imported\nit at the top level, and since this module is loaded when `verdi` is\ncalled, the load time of `verdi` was significantly increased. This would\nhave a detrimental effect on the tab completion speed.\n\nThe work around is to only import the module within the methods that use\nit. Ideally, the loading of this library would not be so costly."
    ]
  },
  {
    "pr_number": 4344,
    "commits_list": [
      "056d4d7d78312fb795b7afb006fd1a29ea0bb0a3"
    ],
    "message_list": [
      "Prepare the code for the new repository implementation\n\nIn `v2.0.0`, the new repository implementation will be shipped, that\ndespite our best efforts, requires some slight backwards-incompatible\nchanges to the interface. The envisioned changes are translated as\ndeprecation warnings:\n\n * `FileType`: `aiida.orm.utils.repository` ->`aiida.repository.common`\n * `File`: `aiida.orm.utils.repository` ->`aiida.repository.common`\n * `File`: changed from namedtuple to class\n * `File`: iteration is deprecated\n * `File`: `type` attribute -> `file_type`\n * `Node.put_object_from_tree`: `path` -> `filepath`\n * `Node.put_object_from_file`: `path` -> `filepath`\n * `Node.put_object_from_tree`: `key` -> `path`\n * `Node.put_object_from_file`: `key` -> `path`\n * `Node.put_object_from_filelike`: `key` -> `path`\n * `Node.get_object`: `key` -> `path`\n * `Node.get_object_content`: `key` -> `path`\n * `Node.open`: `key` -> `path`\n * `Node.list_objects`: `key` -> `path`\n * `Node.list_object_names`: `key` -> `path`\n * `SinglefileData.open`: `key` -> `path`\n * Deprecated use of `Node.open` without context manager\n * Deprecated any other mode than `r` and `rb` in the methods:\n    o `Node.open`\n    o `Node.get_object_content`\n * Deprecated `contents_only` in `put_object_from_tree`\n * Deprecated `force` argument in\n    o `Node.put_object_from_tree`\n    o `Node.put_object_from_file`\n    o `Node.put_object_from_filelike`\n    o `Node.delete_object`\n\nThe special case is the `Repository` class of the internal module\n`aiida.orm.utils.repository`. Even though it is not part of the public\nAPI, plugins may have been using it. To allow deprecation warnings to be\nprinted when the module or class is used, we move the content to a\nmirror module `aiida.orm.utils._repository`, that is then used\ninternally, and the original module has the deprecation warning. This\nway clients will see the warning if they use it, but use in `aiida-core`\nwill not trigger them. Since there won't be a replacement for this class\nin the new implementation, it can also not be replaced or forwarded."
    ]
  },
  {
    "pr_number": 4317,
    "commits_list": [
      "1d86f04e38f5a5f372eb7690629cab50adb111c1",
      "4bb7dbefd7b5dd40d513632f6820d3108c8c708b",
      "92e3bc465613efad81224094885f8d95c9fbbe96",
      "177298e42aa190181eb764705b3a5b9a55782ddc",
      "ba1e43f98bac0efa85dc981af1e356f45aa2721b",
      "040b7e43dd54b90d08f263ef50e24123775bd962",
      "016aee56ff789630e9ad5196186319dcbf79f9ce",
      "80af135d771639fa5189d74f999c794e49e74436"
    ],
    "message_list": [
      "Engine: replace `tornado` with `asyncio`\n\nThe `plumpy` and `kiwipy` dependencies have already been migrated from\nusing `tornado` to the Python built-in module `asyncio` in the versions\n`0.16.0` and `0.6.0`, respectively. This allows us to also rid AiiDA of\nthe `tornado` dependency, which has been giving requirement clashes with\nother tools, specifically from the Jupyter and iPython world. The final\nlimitation was the `circus` library that is used to daemonize the daemon\nworkers, which as of `v0.17.1` also supports `tornado~=5`.\n\nA summary of the changes:\n\n * Replace `tornado.ioloop` with `asyncio` event loop.\n * Coroutines are marked with `async` instead of decorated with the\n   `tornado.gen.coroutine` decorator.\n * Replace `yield` with `await` when calling a coroutine.\n * Replace `raise tornado.gen.Return` with `return` when returning from\n   a coroutine.\n * Replace `add_callback` call on event loop with `call_soon` when\n   scheduling a callback.\n * Replace `add_callback` call on event loop with `create_task` when\n   scheduling `process.step_until_terminated()`.\n * Replace `run_sync` call on event loop with `run_until_complete`.\n * Replace `pika` uses with `aio-pika` which is now used by the `plumpy`\n   and `kiwipy` libraries.\n * Replace `concurrent.Future` with `asyncio.Future`.\n * Replace `yield tornado.gen.sleep` with `await asyncio.sleep`.\n\nAdditional changes:\n\n * Remove the `tornado` logger from the logging configuration.\n * Remove the `logging.tornado_loglevel` configuration option.\n * Turn the `TransportQueue.loop` attribute from method into property.\n * Call `Communicator.close()` instead of `Communicator.stop()` in the\n   `Manager.close()` method. The `stop` method has been deprecated in\n   `kiwipy==0.6.0`.",
      "`Process.kill`: properly resolve the killing futures\n\nThe result returned by `ProcessController.kill_process` that is called\nin `Process.kill` for each of its children, if it has any, can itself be\na future, since the killing cannot always be performed directly, but\ninstead will be scheduled in the event loop. To resolve the future of\nthe main process, it will have to wait for the futures of all its\nchildren to be resolved as well. Therefore an intermediate future needs\nto be added that will be done once all child futures are resolved.",
      "Unwrap the futures returned by `ProcessController` in `verdi process`\n\nThe commands of `verdi process` that perform an RPC on a live process\nwill do so through the `ProcessController`, which returns a future.\nCurrently, the process controller uses the `LoopCommunicator` as its\ncommunicator which adds an additional layer of wrapping. Ideally, the\nreturn type of the communicator should not change depending on the\nspecific implementation that is used, however, for now that is the case\nand so the future needs to be unwrapped explicitly one additional time.\nOnce the `LoopCommunicator` is fixed to return the same future type as\nthe base `Communicator` class, this workaround can and should be\nremoved.",
      "`Runner`: use global event loop and global runner for process functions\n\nWith the migration to `asyncio`, there is now only a single event loop\nthat is made reentrant through the `nest-asyncio` library, that monkey\npatches `asyncio`'s built-in mechanism to prevent this. This means that\nin the `Runner` constructor, we should simply get the global event loop\ninstead of creating a new one, if no explicit loop is passed into the\nconstructor. This also implies that the runner should never take charge\nin closing the loop, because it no longer owns the global loop.\n\nIn addition, process functions now simply use the global runner instead\nof creating a new runner. This used to be necessary because running in\nthe same runner, would mean running in the same loop and so the child\nprocess would block the parent. However, with the new design on\n`asyncio`, everything runs in a single reentrant loop and so child\nprocesses no longer need to spawn their own independent nested runner.",
      "Engine: cancel active tasks when a daemon runner is shutdown\n\nWhen a daemon runner is started, the `SIGINT` and `SIGTERM` signals are\ncaptured to shutdown the runner before exiting the interpreter. However,\nthe async tasks associated with the interpreter should be properly\ncanceled first.",
      "Engine: enable `plumpy`'s reentrant event loop policy\n\nThe event loop implementation of `asyncio` does not allow to make the\nevent loop to be reentrant, which essentially means that event loops\ncannot be nested. One event loop cannot be run within another event\nloop. However, this concept is crucial for `plumpy`'s design to work and\nwas perfectly allowed by the previous event loop provider `tornado`.\n\nTo work around this, `plumpy` uses the library `nest_asyncio` to patch\nthe `asyncio` event loop and make it reentrant. The trick is that this\nshould be applied at the correct time. Here we update the `Runner` to\nenable `plumpy`'s event loop policy, which will patch the default event\nloop policy. This location is chosen since any process in `aiida-core`\n*has* to be run by a `Runner` and only one runner instance will ever be\ncreated in a Python interpreter. When the runner shuts down, the event\npolicy is reset to undo the patch.",
      "Tests: do not create or destroy event loop in test setup/teardown",
      "Engine: explicitly enable compatibility for RabbitMQ 3.5\n\nRabbitMQ 3.6 changed the way integer values are interpreted for\nconnection parameters. This would cause certain integer values that used\nto be perfectly acceptable, to all of suddent cause the declaration of\nresources, such as channels and queues, to fail.\n\nThe library `pamqp`, that is used by `aiormq`, which in turn is used\nultimately by `kiwipy` to communicate with the RabbitMQ server, adapted\nto these changes, but this would break code with RabbitMQ 3.5 that used\nto work just fine. For example, the message TTL when declaring a queue\nwould now fail when `32767 < TTL < 655636` due to incorrect\ninterpretation of the integer type.\n\nThe library `pamqp` provides a way to enable compatibility with these\nolder versions. One should merely call the method:\n\n    pamqp.encode.support_deprecated_rabbitmq()\n\nThis will enable the legacy integer conversion table and will restore\nfunctionality for RabbitMQ 3.5."
    ]
  },
  {
    "pr_number": 2596,
    "commits_list": [
      "62b398bc3d16812cd587a1fa15f0e43eca303583",
      "4a11d5cb9d40d0f1f11fe11f3c58daf7901b7822",
      "1714c4dcab46ac3d54a7bae3298efdbc6fedb8ec",
      "5d3ca0b77fb7df61c6170cf40471dbc4c64ed767",
      "8ef11f2895fb86529a1665a73f657354f1a637ab",
      "f44c16d58c6791f8ba6b14553e0cb0c20d2907c8",
      "6e40e7d723ddfe397e825afca568b014316a9d12",
      "2ef18e660ac47e70328d96883c0e645fb29099ce",
      "1089287532f3dd67ee452561f6b9ca012e04cb40",
      "b236a657ea5973cede6855b4ad74b12f28123a1e",
      "104f4e4ff23d5d7c55db2b0977c1e22ddc582578",
      "d5371a5acba36acd9934f9497d4482e1b791b82a",
      "35685d31a1a926fbf434488ed934baab1bcdfda3",
      "4d02467e34637560020341a286da4f47c502ac6c",
      "2d85794f97348633f3202989890d600b913e1340",
      "4cdca70b655cd5f290069268c98fdb7ee6133cd3",
      "2ee6ca9b096d979eb602e3f7c7a3970cd0385025",
      "1f8a01c6166e4da7004cd20ecb66b0707835ce79",
      "5a111b5b8a51995292ec3a8a85b0eb140d4cf5a0",
      "07fe0f59ed9cc1ccc4e46a721ad0201accd8d75c",
      "28cc1b7f26cc558e247d1948c31074fae81b792b",
      "695de89fcce6dcf01be527d41e55e34227fd3ced",
      "661eba496718a3761255cb3ffb082f30f54c818c",
      "e3ce61663ca673a9aea351d22fd02d89fb7234cd",
      "ba9ea49452364a0574fff169907127c6dbcd2e8b",
      "ab8a4c3827bf71ee6d49307946515a7b43f889c5",
      "ae3f87385bb4736053dc132e1b74fc863a866a71",
      "aec83f390e3724eca47f5d3f86fba1178741ea95",
      "bc2d1cb37654062adbc4376a82c23df6e6038afe",
      "5bf1e2838f89dbd483bbcd7ed3e220df9d17efc0"
    ],
    "message_list": [
      "CJS fix",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "add main code",
      "added add_origins_to_targets method",
      "fix _cif_style",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into enhanced-graphviz",
      "updated to be compatible with v1.0.0b2",
      "test fixes",
      "convert docstrings to ReST style",
      "test fixes",
      "test fixes",
      "test fix",
      "docstring fixes",
      "Update cmd_graph.py",
      "default sub-label to node.get_description()",
      "move the process node styling to a separate function",
      "attempt to get default style from node.get_style_default()",
      "Merge remote-tracking branch 'upstream/develop' into enhanced-graphviz",
      "Merge remote-tracking branch 'upstream/develop' into enhanced-graphviz",
      "Merge remote-tracking branch 'upstream/develop' into enhanced-graphviz",
      "clear jenkins workspace",
      "change default link/node styles",
      "update Code node default style",
      "Update graph.py"
    ]
  },
  {
    "pr_number": 4532,
    "commits_list": [
      "d42aa88e693c19e75f8d7e0b090652157f6fbcd6",
      "e9ad3224fcb7d37b6f8a47500ac54f686a2c2ddb",
      "c2c9e6064b23a23afb3e06cd4a8d867236bec00a",
      "66021a4378814cec6b140c1cb64e17413342c516",
      "70c1fb0c24a5fe5939bfebc452ed401561710f61",
      "298ee420efa0b6f76586822c74d0c17ce162a90a",
      "cbcaec6e3a4e0f90922f3502f12798098011d915",
      "b44e3501df57c63f14cae1752d261a880e069086",
      "cf676551a089659d25e2abdc62e52921aebcfaa4",
      "ffb36990ec9069b8f078be2eb20dd6aece2193b7",
      "a80c5d9eefe978bd51ed96c6f8e536fb3025df2a",
      "7e04ff18cf496ccbcfbd9ae22b1ddb841d5e5f41",
      "072af2612df96b2ac2e6d22a3661ae6ce03cf53e",
      "d24481e35853dc79e8a91d95bbd2e51d31024932",
      "4356e0c8def5c53fdbdf5fd1f761cd36f39511cf",
      "f690b180ed6e20f8e206a2e0522b4635bf8a89e7",
      "641b7045ad16e5d630addf6037bd9e512df9dc8c",
      "4238cb2ec3a391e62b54efa596335025ee5b5f2d",
      "baf4eb1cba5e0558292e5c4b0892394fc4ce3339",
      "12960b45ecbb22623d1a6b0fd6891c03d680c4a5",
      "be1bbb461a9dc2aa580409d077979e278b5079a8",
      "6a8df4eecfb13423509061d2945be6a4b949c4f2",
      "cd7a27c646fe202b65b585c48b9ae091583ecace",
      "3b8b07f31a5b242c4ba6d3e0f5917b7b6be3d560",
      "f95b6aeed66bc1cb5c2652d92d8fbdedc309f783",
      "5d62743cd6c073a09db34369811e1d5bb2402b6f",
      "06fcd92919866c4594c9032a63013b90e158f350",
      "4dd7fa807c0b1652558be9f1454bfbe63c9eec00",
      "026b5cb5d755e249407ab2a463529e369523e3c4",
      "348242f0892e63c109a48a1db2e681396f1eaaba"
    ],
    "message_list": [
      "refactor (tests not yet fixed)",
      "update tests",
      "fix docs",
      "update verdi import with new migration process",
      "Apply suggestions from code review\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "fix tests",
      "Add CacheFolder",
      "Improve caching mechanism for json\n\nallow for only a single copy of a dict to exist in memory at any time",
      "minor fix",
      "simplify `verdi import` logic\n\n- fail on first error (do not continue loop of additional archives on click.confirm)\n- never ask user whether to migrate (just use --migration/--no-migration)",
      "fix pre-commit",
      "remove some pylint disables",
      "typo fix",
      "Update aiida/cmdline/commands/cmd_import.py\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "fix pre-commit",
      "update docs",
      "Apply suggestions from code review\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "apply review suggestions",
      "add extra debug statements",
      "fix tar extraction",
      "add more progress feedback",
      "move compression to common",
      "fix over-count",
      "Allow for no compression before import",
      "fix pre-commit",
      "fix pre-commit",
      "Update aiida/tools/importexport/archive/common.py\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "improve memory usage",
      "refresh migration description early",
      "Add tar migration test"
    ]
  },
  {
    "pr_number": 5058,
    "commits_list": [
      "2517a7a480a316845af242d577ff8c43e8a9bd87",
      "f943ee72a577db542d7da606e955ac0e6e70dbc8",
      "3f68491c16877c9e07dd44eacf9e0a0cbe6b6201",
      "74a9ebd6ad3537f8d503695da8a8cf5cca978802",
      "f709b5e2c9ae4f78aa6a39ea5cac7a4928aea803",
      "bf2b78912a18251b1c5ab1e5d685b272703855f8",
      "ab20f5e4e24cb921787ef3e84b8a65dee2296c04",
      "3aa73bdb82a481b8081e238e0f6d22e48bbc56ec",
      "e3612a3b100f8ba76f9a12b714a72fa23010991f",
      "09d9fe38c679c52b83b013da0c02ea9639dd0956",
      "27ef0ad37a0ed2d367ff822614a194def3175b94",
      "6cde07fc2a62a5101f9b0d779adddeefa53918c2",
      "52f460adc67be6d6bd410f00f54a765dcc9787e0",
      "48f8a1e4ec2604abadf7c8bf99d7c4ff1f440dba",
      "5bf24f5c2d03dd5439d71be4bfee7b8ff3766a4b",
      "80d1fcb368c82ee431fcd2bd3f531bfc4b767bf9",
      "8acbec12e23c13674c1c79ec92de01ba8f582e03",
      "dac9630699b111c27881d072ec8c9ded637966d1",
      "3437aef5550ab4cb8558a3befe362ccdc5ecdabf",
      "c6c060d8720ae61f382c77609c191bd48d51065c",
      "039c0c3664f233c16f2eb047e51b0a7ac837375e",
      "42903929f63ec7a74739165cf4fb198686f788b6",
      "6689fd253f7bd9ef3ca54011da6d15827ef7077c",
      "72c99d32c926d7884c70e87410ece54776e8ce50",
      "4a0dec3749942e449aa64cc95d26488fce3b973b",
      "3fd5b4c58d9d3553e59e7e35aace6ab6d699714a",
      "8362ef1854fa6dcbd49191ab508d6918514ccb7d",
      "418c1a52f2543b1b0653d7125163e54085137695",
      "d540e99666bd7410c368be499f1f88655692cead"
    ],
    "message_list": [
      "\u267b\ufe0f REFACTOR: Remove reentry requirement",
      "fix docs",
      "update",
      "Update nitpick-exceptions",
      "Update calcjob.py",
      "Update entry_point.py",
      "Update entry_point.py",
      "Update entry_point.py",
      "run load time for all python versions",
      "pin to `importlib-metadata~=4.3`",
      "Update environment.yml",
      "re-add fastentrypoints",
      "Remove pyproject.toml validation",
      "remove try/except",
      "Merge branch 'develop' into remove-reentry",
      "Update .github/workflows/ci-code.yml",
      "Merge branch 'develop' into remove-reentry",
      "Update .pre-commit-config.yaml",
      "Add caching for additional functions",
      "Merge branch 'develop' into remove-reentry",
      "Merge branch 'develop' into remove-reentry",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "cache `eps()`",
      "Update requirements-py-3.7.txt",
      "Update verdi.sh",
      "Update .github/system_tests/test_verdi_load_time.sh",
      "Merge branch 'develop' into remove-reentry",
      "Merge branch 'develop' into remove-reentry",
      "Update ci-code.yml"
    ]
  },
  {
    "pr_number": 3599,
    "commits_list": [
      "4ac3efdfa73f8943232d32ac4625337e4055bfc9",
      "44bf4a53b855ffb189683b08df35cfe493b0e57d",
      "f3b663ddd42aa7c4175b72c743dbf6b72a7b4ce5",
      "2f9a17fab940a849166c04c8372b81cbf56a3ee4",
      "db6a4600910352acd46c0b7b411227b28a647fc7",
      "219645f0417ac445716a599b3d8fdbb20298beee",
      "ff3230f17a74fc2c5fbde05b4ee708da2e4cd757",
      "2424946f8b276602bc72e978a9e2a018c99f68c9",
      "8f8ce553a9a52d3192fe96ad9ee02444b8b55c1b",
      "9136f27325f9ab44e13024d99d0b879dd36181f0",
      "e16d44a7f0fdb061e2f622e28610f06649d84547",
      "94dbc39bcbe9302f17d11f58b7b74ed78ee51cdb",
      "e712f780e1432e45df10c8fb48e8455ab241951b",
      "0ed38f7f39fd2695e3090d0c7f4f1088754c7476",
      "d21d7a31df3ff6f73132840d9d8b618126db3cbc",
      "a26d3345f2e478d6e73faf1f9d665e635e483499",
      "1ea98ea28dfda03c501b40836e6d3e5d003513af",
      "cb11a916bb39b0baec769f7d9641b7e100d9713a",
      "5cdeb70fba5f615a407872447d2fc4e2e08740b4",
      "3d3eabacdfa4d5be075b51ef3a9fb1adff70d621",
      "c68460cb9e82d536796666f53fdd7748910d4c25",
      "4aba3c2e10cc66ea0d5811891bd515a0408123df",
      "ce0276b5e534b991c15770b59c18fa884f42d38e",
      "ff70578ff150670ba511dc94dc6eba1aacefb901",
      "454a11db7c3e7d08fdace2badeb49f02236873c3",
      "d79021c0700f0968d43fdeb1a47562b8cd668f9f",
      "bf1a587a9cc9991214110d8b8ad839a02528d486",
      "3ddcb03c987310ff0832237780bf8bc1df75a597",
      "9b86b7b21f78004ee8aa53dcbf89bded042d5595",
      "79f3b34adf2c5114ab9fe2315c641f9189b38194"
    ],
    "message_list": [
      "Update folders doc-strings",
      "Introduce tqdm dependency",
      "Progress bar - archive and config",
      "Introduce debug param - import/export",
      "Progress bar - django",
      "Progress bar - SQLA",
      "Progress bar - export",
      "Print header - export",
      "Optimize Exception handling `verdi export create`",
      "More intelligent error handling `verdi import`",
      "Header - export",
      "Remove entity_sig from import sqla",
      "Optimize import of existing Node Extras",
      "Header - import",
      "Header re-write - ex/import - use tabulate",
      "Progress bar labels and refresh rates\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "Optimize export",
      "Minor cosmetic fixes - Export, Archive",
      "Explicitly set `refresh` for set_description_str",
      "Fix export inspect test\n\nSince `verdi export inspect` now prints the extraction information, the\ntest needs to take this into account when checking the output is\ncorrect.",
      "Update requirements files",
      "More explanatory debug messages\n\nUnify export header table headers.",
      "Minor tweaks to resetting progress bar\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "Remove unimportant .copy() statements",
      "Align django import with sqla import function\n\nSome initial logic and progress bar updates that were present in SQLA\nimport have been implemented the same way in Django import.",
      "Add --debug as common `params.options.DEBUG`\n\nUpdate it not just for the import/export click commands, but also for\nthe REST API command.\n\nKeep it hidden as default.",
      "Don't refresh new desc.s when extracting archives",
      "Being more descriptive\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "YAPF correction",
      "Pass and handle exception in _echo_error()"
    ]
  },
  {
    "pr_number": 1682,
    "commits_list": [
      "147d54c3022cb4b33a357fb14e900c790eab152e",
      "8f05c286e8a39e316a845fe3654afcf4a3451172",
      "0e9c10cbd7619105d1dc4958395671badfaededa",
      "4ac9aec6bf338c5f71caebce46542619fbde8aa9",
      "32665ac22e9f5c760fb2fcb9934055d748c7b8b7",
      "af4447ce15da2f1a38b5580d3c1f4ca5044304bc",
      "f4301144b06ddfbd25978b5213387eb23962c929",
      "ad5504fb2420ab28090b9854484caa91368ea068",
      "a1efa4eaabd4567c60d3e3f662c0316ba3700b98",
      "2c7fbe5e6790beb068482882f4124d86c32f5c95",
      "2dd73c3eb3e9d74b2811bfeb0fb8a803f889236e",
      "e7f1240da0663a3bcf4e4029240275e936455990",
      "611962c849d480aa43e2cbffdb28cfc3dd71fadb",
      "f8c6b8e0699acdb1e49eb3bf9646092cd8b1f6ea",
      "d38361bf9abb631ba4ed9c1d0ea93fa845618725",
      "b5384c05dfee2b122331ad331edc877cd336e1e1",
      "4047d14ea1d158e0690d8dffa441ba3e1852e81c",
      "b0acd2e6edb9cddd136b0e6dcda02d8171b34737",
      "b7db0dc70d3f373f69cc64b435d70f1d7cbec34c",
      "74e29d148aeebf3cffc29e749f9b9ea104bc3c7b",
      "187844265c2d00d9e13a497098ba64c5505fa4dd",
      "48848f316a815a786a933518d5f90950878ed093",
      "bf3ccef56b94ccba15aeea2f3779a44d13ca5b7d",
      "ba01a31bfc381727a698e811e925a2aba6e3eb0d",
      "c48c671bca38620b1bd8719cbfe04b60a54691bd",
      "0e9983e8f9b3b3f587fa92a179aaf7af7fad565b",
      "5f919e87e425a3411024a4f738441452e298ee81",
      "47b2bae6b922e32738ed34f9a55adb7f57949e20",
      "6af91531a696b22e48e7963d305c11aac53baf01",
      "2c4b061f64e911861fde1cded154e45b9faf46ca"
    ],
    "message_list": [
      "setup.py: make `setup.py` py3 compatible",
      "make `exec` calls Python 3 compatible",
      "travis: add python 3.6 for basic testing",
      "pre-commit-config: add python-modernize hook",
      "make imports absolute by default for Python 3 compatibility\n\nAt this point it is important to note that `.ci` and `.ci/polish` are no\nPython packages and adding the `__init__.py` will break 2to3/modernizes\nfixer. The directory of a Python script is always added to `sys.path`,\nalso in our uses of `exec()`.",
      "prospector: disable flake8/pep8\n\nmostly covered by pylint and pylint is properly configured already",
      "pylint: (re-)enable Python 3 relevant warnings\n\nthis is (partly) based on the list of language differences here:\n\n  http://python3porting.com/differences.html\n\nand PEP 3100 here:\n\n  https://www.python.org/dev/peps/pep-3100/",
      "python-modernize: enable all fixers without changes\n\nTo avoid any regressions.",
      "replace `print` keyword with `print()` function\n\nSome `print()` calls are consequently replaced by either `click.echo`\nor `echo.echo` (and variant) if either click or the echo utils package\nare already in use.",
      "replace old-style octals 0NNN with new style 0oNNN",
      "fix old-style except clauses",
      "replace old-style `a.has_key(b)` with `b in a`",
      "replace func_globals with __globals__",
      "pre-commit-config: mark optional 2to3 fixers as such",
      "use reduce() from functools instead of built-in\n\n`functools.reduce()` is the same as the `reduce()` built-in, see\n\n  https://docs.python.org/2/library/functools.html#functools.reduce",
      "replace/check current uses of `filter()`",
      "convert to new-style division where necessary\n\nthe changes by the 2to3 fixer needed to be carefully reviewed, since it\nconverted all `/` to `//`, which is definitely not what we want\n\nAfter adding `from __future__ import division`, the division operator `/`\nalways returns a float. To get the integer division, use now `//`.\n\nSince most of the time, the float division was intended, the commit\nconsists mostly of adding the import statement.",
      "Drop removed types.StringTypes",
      "replace all calls to `a.next()` with `next(a)`\n\nAlso replace some implementations where Python 3 style `__next__()` was\nimplemented based on Python 2 style `next()` with a copy of the Python 2\nimplementation instead.",
      "replace all calls to `map()` with list comprehensions\n\nmost of the time they are easier to comprehend, in some cases this is\neven an optimization since the intermediary list can be removed (for\nexample when inside `join` clauses which also takes generator\nexpressions).\n\nfor the `cod` and `icsd` there is also some refactoring of stringify\nfunctions to use `.format()` instead (increases readability further).",
      "make sure all uses of `zip()` are future-proof\n\nThe import `from six.moves import zip` resolves to `itertools.izip` on\nPython 2 and the built-in `zip` on Python 3.\n\nThis can actually give a speedup on Python 2 since a lot of `for` loops\nare now using iterators instead of full lists.\n\nThere is a nice idiom here. Instead of converting the iterator returned\nby new-style `zip` to a list and then get the first element by index\n`[0]`::\n\n  list(zip(a, b))[0]\n\none can use the `next()` function on the iterator instead::\n\n  next(zip(a,b))\n\nCare has been taken to ensure that every `for` loop over a `zip`\niterator does not modify the contents of the variables for the `zip`\n(which would have side-effects in Python 3, but worked before in Python\n2).",
      "enable the `filter` fixer after all\n\nThis makes sure that people are using the Python 3 style `filter` and\nsince we're using `six.moves` anyway we can also do it for this one.\n\nAlso fixes one usage of `filter` which was only discovered by running\n`python-modernize` using Python 3.\n\nCould be squashed with 'replace/check current uses of `filter()`'",
      "enable fixer for old-school `raise` clause to prevent regressions",
      "replace all tuples `(int,long)` with `six.integer_types`",
      "put tests involving `long` behind a `if six.PY2:`\n\nthis means we can't run the `long` fixer automatically since it doesn't\nunderstand this valid case (and the conditional)",
      "replace all calls into the reorganized urllib with six.moves\n\nThe fixer would originally replace calls like::\n\n  import urllib2\n  urllib2.urlopen(...)\n\nwith::\n\n  import six.moves.urllib.request\n  six.moves.urllib.request.urlopen(...)\n\nThis has been manually changed such that only the import line contains a\nreference to six::\n\n  from six.moves import urllib\n  urllib.request.urlopen(...)\n\nto be closer to the Python 3 variation of it::\n\n  import urllib\n  urllib.request.urlopen(...)\n\nInstead of importing the whole `urllib`, one could have imported only a\nsubmodule instead::\n\n  from six.moves.urllib import request\n  request.urlopen(...)",
      "replace references to itertools.ifilter with filter+six.moves",
      "replace all calls to `raw_input` with `six.moves.input`",
      "replace some more imports of renamed modules with six.moves\n\nthis involves:\n\n* `HTMLParser`\n* `cPickle`\n* `urlparse`",
      "make Meta Class spec Python 3 compatible via six\n\nThis uses the `@six.add_metaclass` decorator instead of\n`six.with_metaclass()` factory function. This should result in a cleaner\nMethod Resolution Order and avoid an intermediary base class.\n\n**Important**: the fixer rewrites the code using the factory function\ninstead."
    ]
  },
  {
    "pr_number": 3974,
    "commits_list": [
      "177c35349cca55c0b9fe866e51225082bef3da01",
      "11130a4d9a85978a4c15f89590f90236461da091"
    ],
    "message_list": [
      "Add possibility to pass keyword arguments to `create_engine`\n\nThe function `aiida.backends.utils.create_sqlalchemy_engine` now takes\nkeyword arguments that are passed straight to the SQLAlchemy function\n`create_engine` which creates the engine that connects to the database.\nThis utility function is used by the SQLAlchemy and by both backends\nfor the `QueryBuilder`. By allowing these keyword arguments to be passed\nand plumbing them all the way in to the `Backend` class, the parameters\nof the SQLAlchemy engine and the associated queue pool can be\ncustomized. This is not meant to be used by end users, but can be\nimportant for important use, for testing and performance reasons.",
      "Close SQLA session after every REST API request\n\nThis is needed due to the server running in threaded mode, i.e.,\ncreating a new thread for each incoming request. This concept is great\nfor handling many requests, but crashes when used together with AiiDA's\nglobal singleton SQLA session used, no matter the backend of the profile\nby the `QueryBuilder`.\n\nSpecifically, this leads to issues with the SQLA QueuePool, since the\nconnections are not properly released when a thread is closed. This\nleads to unintended QueuePool overflow.\n\nThis fix wraps all HTTP method requests and makes sure to close the\ncurrent thread's SQLA session after the request as been completely\nhandled.\n\nUse Flask-RESTful's integrated `Resource` attribute `method_decorators`\nto apply `close_session` wrapper to all and any HTTP request that may\nbe requested of AiiDA's `BaseResource` (and its sub-classes).\n\nAdditionally, remove the `__init__` function overwritten in\n`Node(BaseResource)`, since it is redundant, and the attributes `tclass`\nis not relevant with v4 (AiiDA v1.0.0 and above), but was never removed.\nIt should have been removed when moving to v4 in 4ff2829.\n\nConcerning the added tests: the timeout needs to be set for Python 3.5\nin order to stop the http socket and properly raise (and escape out of\nan infinite loop). The `capfd` fixture must be used, otherwise the\nexception cannot be properly captured.\n\nThe tests were simplified into the pytest scheme with ideas from\n@sphuber and @greschd."
    ]
  },
  {
    "pr_number": 4730,
    "commits_list": [
      "a6019007d7daa12e9f11588ba8181f2512b6daa7",
      "60297b38c3d6830fbaee95ae585c05760f735ed0",
      "9b050bdcba8ed93172b10d02de94e10bf61860ba",
      "b52ff0021d625364bd6caafe6c13dfd9373ada4e",
      "be54520c5010f1874e35a8dd387acb4e10884849",
      "c6ceb78717b8fbc1c1a04362e0f8d33cb79f180f"
    ],
    "message_list": [
      "Base `AiiDALoader` on `UnsafeLoader`, strip checkpoints on import.\n\nTo allow e.g. numpy arrays to be serialized to a process checkpoint,\nthe `AiiDALoader` is based on `yaml.UnsafeLoader` instead of\n`yaml.FullLoader`. Since this could pose a security risk when sharing\ndatabases with maliciously crafted checkpoints, the checkpoints are\nremoved upon importing an archive.\n\nFixes #3709.",
      "Rename the `deserialize` function to `deserialize_unsafe`.",
      "Fix copy behavior in `_strip_checkpoints`.",
      "Relax requirement on `pyyaml` version.",
      "Remove 'silent' keyword in call to 'export'.",
      "Merge branch 'develop' into fix_yaml_load"
    ]
  },
  {
    "pr_number": 4337,
    "commits_list": [
      "9e584dd8451e16f40ffbe98a643908c5c1ef67f1",
      "a4ec5122b9d2e79b183cd6fd73aa21a34c3f00a3",
      "4c9d44af4d8c2550444d9d528dce1b890c7772f6"
    ],
    "message_list": [
      "REST API fixes\n\n- Use node_type in construct_full_type().\n- Don't use try/except for determining full_type.\n- Remove unnecessary try/except in App for catch_internal_server.\n- Use proper API_CONFIG for configure_api.",
      "New /querybuilder-endpoint with POST for REST API\n\nThe POST endpoint returns what the QueryBuilder would return, when\nproviding it with a proper queryhelp dictionary.\nFurthermore, it returns the entities/results in the \"standard\" REST API\nformat - with the exception of `link_type` and `link_label` keys for\nlinks. However, these particular keys are still present as `type` and\n`label`, respectively.\n\nThe special Node property `full_type` will be removed from any entity,\nif its value is `None`. There are two cases where this will be True:\n- If the entity is not a `Node`; and\n- If neither `node_type` or `process_type` are among the projected\nproperties for any given `Node`.\n\nConcerning security:\nThe /querybuilder-endpoint can be toggled on/off with the configuration\nparameter `CLI_DEFAULTS['POSTING']`.\nAdded this to `verdi restapi` as `--posting/--no-posting` option.\nThe option is hidden by default, as the naming may be changed in the\nfuture.\n\nReviewed by @ltalirz.",
      "Use importlib in .ci folder"
    ]
  },
  {
    "pr_number": 4111,
    "commits_list": [
      "abd7004e81f20869f77a0e5091362006ac407647"
    ],
    "message_list": [
      "add improved search using rtd-sphinx-search\n\nincludes \"search as you type\"\n\nhttps://github.com/readthedocs/readthedocs-sphinx-search\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>"
    ]
  },
  {
    "pr_number": 4951,
    "commits_list": [
      "c3dbf09bddb56630c7e775ee75a1eda6b7ee9119",
      "eab722fbba3e7e05c0cba44d38d1371ee7c07e61",
      "78c98260efc27747ef8f3219b5b53e410dbba56d",
      "71c1ee38707df33c34b751b1f9cd0604ab012ec8",
      "cca94a5cf41b459ee3a82af5bc4184b50e543130",
      "b871855c1438a25154b133641ae67c42723e5004",
      "61a1b872c1de02dcf1fb05f152efde53717bd6ee",
      "c734bf2e5fd43016313d9029910a5aa37b421d5e",
      "054e300e1f41068911b29a38279d348fdf3b39e8",
      "04520e46e6b11c43f28de2c08366a7ce64400975",
      "9295bcfa8c503bf1dca441c807d0bf1e1100f664"
    ],
    "message_list": [
      "docs/ssh: add proxy_jump instructions",
      "transports/ssh: support proxy_jump",
      "Update docs/source/howto/ssh.rst\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update docs/source/howto/ssh.rst\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "docs/ssh: add proxy_jump instructions",
      "Merge branch 'feature/proxyjump' of github.com:dev-zero/aiida_core into feature/proxyjump",
      "docs/ssh: redo the proxy part",
      "Apply suggestions from code review\n\nCo-authored-by: Marnik Bercx <mbercx@gmail.com>",
      "Merge branch 'develop' into feature/proxyjump",
      "Merge branch 'develop' into feature/proxyjump",
      "Merge branch 'develop' into feature/proxyjump"
    ]
  },
  {
    "pr_number": 4753,
    "commits_list": [
      "8fc48ed0e41bf03b7b3f37b71cd4e9ed07a975b9",
      "e54acebe440eec805bc6604256a0a7aefb549123",
      "48393b9bcde72b38f5481a99b9011acf64a60cd0",
      "a07145066b66dc49033e190dc4367d0e7c693c2d",
      "0a03f65ad73d7dd77902dcd0076c62572cef5ca2",
      "295c92f1ba50ba077e327796fb5a56d0b2c4bf9f"
    ],
    "message_list": [
      "Add fallback equality relationship based on uuid",
      "Fix port test",
      "Fixed case delegation to superclass.",
      "Fix incompatible hash",
      "Apply PR corrections.",
      "Merge branch 'develop' into fallback_uuideq"
    ]
  },
  {
    "pr_number": 3771,
    "commits_list": [
      "b1c7270062da7c771bfa28f9aabf4e7f0b73d0bc",
      "070a78dd49c7115930948133a8f5d6d8d26f9127",
      "c7601ed68fc74a86c0baf35f9cf89411ef0e75c4",
      "5d552ea6458544f01d78cc7a43789fdd9facc2d7",
      "f6604bcd7c2ed0754287a06a7874c2b42d8f7999",
      "13af1d3e39c04383fe85bc97e31adc621f680aa7",
      "586adf7dbe6576f531d92f31ca6cb24b2c13de63",
      "c27d2bb0c09236b9d3268196ccf66348a2f551da",
      "a65a0c833ae08679e18d1c339c73282ec2f4ae4e",
      "78a34569d939ed15a8f5a18c1dae107f301a81d1",
      "f3b57024ad6cfeedec053b89b6bba51be8234ce0",
      "d9beaceb6b3e17674be6123048c66ed176e2723b",
      "0f5a60efb88ca568ab97b20830652ce3a9386e65",
      "59c64a3859500fc8b037cadcaeb8c24d4959950f",
      "ed8eaf0599a2d1d70d8c181410f726e716ff4cba",
      "364b34ce581872cad1090d0dc97cbfc7e2dd8dc9",
      "e502f4250e2f1133ca0f72881584c281651db7c3",
      "e98fd81b8972c3a30ddc3ef5b10cfdd286eed5e2",
      "46d4f2473a613689d11dec123937d5a7706bebe8",
      "420983f993361b1f945e0929a9b25c47fdb95f69",
      "9b86152d2adc2ac9ebfc1100832d5b12e58198ba",
      "21ed385094c375da679c44783909aaa5eb83b74d",
      "9b635bd2f56b8be6ebecd74167daa59bd0e3e397",
      "e03b77f298a470a361e26895ee9b544b2fff6a01",
      "f8892362db598ac8bf189d67246ea96a89c9622d",
      "79c962bdc4601e135c27a3b1b877459b01f9ec07",
      "5063076b852f86a8af1032db0bc9462cfca9965b",
      "1f13c4cc29ee11c3256a631545f20731d5fce20a",
      "240fcd025abd43b3ffa835c03ca10d21ddaf6f38",
      "b6a3c1a9f4b39599c74513fd8bcc9c71654a5bc4"
    ],
    "message_list": [
      "Add .github/CODEOWNERS file.\n\nTo automatically trigger a review request from the DM for changes\nthat modify files that are related to dependency specification.",
      "Rename script 'update_dependencies' to 'dependency_management'.\n\nTo better reflect the broader intended scope.",
      "Remove the 'etetoolkit' channel from environment.yml.\n\nAll, but one of the dependencies (sqlalchemy-utils) are available via\nconda-forge and mixing channels is generally not advisable.",
      "Implement validate command for dependency management util script.",
      "Add explicit dependency validation step to CI.",
      "Fix yaml requirement.",
      "Implement 'generate-conda-environment-file' for dependency_management script.\n\n- And remove equivalent function from validate_consistency script.\n- Update pre-commit-hooks to reflect change.",
      "Implement validation of rtd_reqs in dependency_management script.",
      "Implement validate-pyproject-toml in dependency_management script.",
      "Use pathlib for paths in dependency_management script.",
      "Use consistent exception type for dependency specification errors.",
      "Place loading of 'setup.json' file in function.",
      "Fix bug in 'validate-environment-yml' command implementation.",
      "Make implemenation 'validate-environment-yml' more robust.",
      "Improve error message for 'validate-rtd-reqs' command.",
      "Implement 'generate-pyproject-toml' dependency management command.",
      "Regenerate 'environment.yml' and 'docs/requirements_for_rtd.txt' file.\n\nRewrites requirements in 'canonical' form so that future changes are\nbetter tractable.",
      "Fix bug in 'generate-rtd-reqs' implementation.",
      "Improve 'dependency_management.py' script API consistency.",
      "Remove the seperate 'dependency' github action.\n\nThe relevant checks are carried out as part of the pre-commit checks.",
      "Ignore pylint error related to json.decoder module (false positive).",
      "Remove dependency management script main help.",
      "Implement 'pip-install-extras' command for dependency management script.\n\nTo install extra dependencies without the core package dependencies.",
      "Vendor toml package in utils directory.\n\nTo simplify the execution of the dependency management script.",
      "Add test for pip installation.",
      "Extend test for installation with conda.",
      "Add CI step to show test environment.",
      "Use 'source activate' instead of 'conda activate'.",
      "Refactor pip and conda install tests.",
      "Only run tests if installation succeeds."
    ]
  },
  {
    "pr_number": 4424,
    "commits_list": [
      "5070c5848d28f41454cd5e25cb5a8a1bf2a1ae69",
      "a721d4b712dacd39ed7b8ec5ea3f083a89ec6c86",
      "f05e9ea1d3448ea7965b2f3480db1f4ad3b76114"
    ],
    "message_list": [
      "`CalcJob`: add the option to stash files after job completion\n\nA new namespace `stash` is added to the `metadata.options` input\nnamespace of the `CalcJob` process. This option namespace allows a user\nto specify certain files that are created by the calculation job to be\nstashed somewhere on the remote. This can be useful if those files need\nto be stored for a longer time than the scratch space where the job was\nrun is typically not cleaned for, but need to be kept on the remote\nmachine and not retrieved. Examples are files that are necessary to\nrestart a calculation but are too big to be retrieved and stored\npermanently in the local file repository.\n\nThe files that are to be stashed are specified through their relative\nfilepaths within the working directory in the `stash.source_list`\noption. For now, the only supported option is to have AiiDA's engine\ncopy the files to another location on the same filesystem as the working\ndirectory of the calculation job. The base path is defined through the\n`stash.target_base` option. In the future, other methods may be\nimplemented, such as placing all files in a (compressed) tarball or even\nstash files on tape.",
      "Apply comments from review\n\nNote the big change in `execmanager.py` is simply moving the\n`stash_calculation` before `retrieve_calculation`. I did this because\nall actions are in order and stashing comes before retrieving. There are\nno actual changes in the code.",
      "Merge branch 'develop' into fix/2150/add-archive-transport-task"
    ]
  },
  {
    "pr_number": 3896,
    "commits_list": [
      "665f9226e25edea75f152b5cedbcb98ebb4dbb33",
      "08ab9da8a017a03372f54d4e8b642a48941e5bad",
      "45e5292fc0e12cfc6007b6b3fe4f590eb7c3a189"
    ],
    "message_list": [
      "add logger for verdi cli\n\nSo far, the verdi cli has been using a number of variants of\nclick.secho in order to write information to the command line\n(e.g. echo_info(), echo_warning(), echo_critical(), ...).\n\nThis automatically adds colored indicators for info/warning/... levels\nbut offers no way to control which level of information is printed.\nFor example, it is often useful to print information for debugging\npurposes, and so it would be useful to be able to specify a verbosity\nlevel when running verdi commands.\n\nHere, we use python's logging module to build a custom handler that\noutputs log messages via click.\nWe also add a click VERBOSITY option that is applied automatically\nto all verdi commands. Its default verbosity level can be controlled via\nthe new logging.verdi_loglevel configuration option.\n\nNote: The code added here is inspired by the click_log package (but\nneeded to be modified in order to fit the needs of AiiDA).\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "address reviewer comments",
      "let CMDLINE_LOGGER inherit from AIIDA_LOGGER\n\nLet CMDLINE_LOGGER inherit from AIIDA_LOGGER in order to be able to set\nthe verbosity level globally."
    ]
  },
  {
    "pr_number": 4712,
    "commits_list": [
      "2d02af92c52270af60eca5b48729128243384468",
      "9ffb8efabe956b2734df4d805ed7d4e8ac5a8113",
      "9211a0852a1aa1b15bfb382349024731b9cac349",
      "c18d169ed841c1650de91962e652cc3c7f01c581",
      "36d88f054868723ea343777cffdc2d618c31f000",
      "64b6f4b0529c81808c47b7e5e5a0382e963d37b5",
      "7e74b504f1ff8d737edfb9f77bf16befa2e66fbe",
      "363797cc6f73b72b21620f3c153a3626ad892ea5",
      "912f773dc3aa9cbfef0277c9e13f4ed6ee20ad1e",
      "94e1294b44bb046fafdad3c956fa54e0dac36204",
      "e4b7f9cc6585627f08ebf2d6465d7a6a5501717b",
      "a41c62e6591384e97a7a89b4e06d1d1f8496cb4d",
      "3fc11b6f7738a2063bd27bf5c30703c29abbe9a5",
      "e53f4502847445ef12e4a4b5d953612c116e9f9f",
      "affb604b7acca9c11cab5cfb3a18f95af22c686b",
      "477ca5f289ece6d03c61648e30a0ce01f55610e4",
      "0d535dddc2a99a491ee42971e2802bb3c6187e26",
      "6d07bf142b7c3d2a0406d5d80a38f4ef1570985b",
      "67275b6e5813d181dac547c25607f9498d36a718",
      "7e4a3e61164044787d76cb33b279c12b383204dd",
      "547f30210e82b3af24360ed21a607dd01f38b594",
      "5f2cf269ceda1d21fd02ed45a69e139fdade96f4",
      "6b026d3a961c4ad76426376e64d28b0b473f0c8a",
      "093ac73111d7a4aa963e751c30b478be643fb479",
      "2f38b6259430f9ea1b5db02d333fde5005919056",
      "d3291674c064165e147765708ae9cad1d363a279",
      "1a3a9586bd92adc0bac3892bbb61138c7f2b4106",
      "2b6370fd466bb6dfa4c4363a8eabc2eeaad8fc66",
      "e1561154db9cfbcfa05fb7b01d0d0bce1865c276",
      "13f3da585043465d38207906679ee6258328b324"
    ],
    "message_list": [
      "Initial implementation",
      "Update options.py",
      "Update config-v5.schema.json",
      "Add more tests",
      "Merge branch 'develop' into verdi-config",
      "Update nitpick-exceptions",
      "Merge branch 'develop' into verdi-config",
      "validate Config on __init__",
      "improve `ConfigValidationError`",
      "Merge `cache_config.yml` into `config.json`",
      "Merge branch 'develop' into verdi-config",
      "add `verdi config set --append/--remove`",
      "Add `verdi config caching`",
      "Improve `verdi config caching`",
      "Rename `cache_config.yml` after merging into `config.json`",
      "Merge branch 'develop' into verdi-config",
      "Improve pytest fixture use",
      "Update test_caching_config.py",
      "Merge branch 'develop' into verdi-config",
      "Apply suggestions from code review\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>\nCo-authored-by: Aliaksandr Yakutovich <yakutovicha@gmail.com>",
      "Merge branch 'develop' into verdi-config",
      "Merge branch 'develop' into verdi-config",
      "remove duplication",
      "combine `_enable_all`/`_disable_all`",
      "add warning for skipped profiles",
      "reuse config",
      "Add `rmq.task_timeout` configuration variable",
      "add validation to `profile.set_option`",
      "Merge branch 'develop' into verdi-config",
      "Update documentation"
    ]
  },
  {
    "pr_number": 4534,
    "commits_list": [
      "681803b58b40616c6a0fcfa72c28261def36f37f",
      "187725f0a6236338d1d4eee37488f0a96179cfc9",
      "6d3311ae9615381d1b98a46d88068330b298f7e8",
      "f9e265b76fae3641e86175c642bfb6db21e9e001",
      "e4772800e87f35387842f5db42ec73fe182dd7e6",
      "f9feb27069cc6623b7fac2ebaf65feba12192c0b",
      "fca7620c8129496b279b418220191e321e99eef4",
      "95b55a0edf44e91cbabb5cd0f71fba1a640d10c1",
      "b3227acfa3afe70999cfc0c7bd45dca70a8ac39c",
      "4f1cef7391e2f906b2c4393527c2e6a179188639",
      "6097589e0162a93e227bfaeb687f357c2f7a8914",
      "b3fd6bbf65ba1f871d6677c3cbb23a2d0841112e",
      "cd7a0ef6c26cd6beb713e7af65fb85b469783965",
      "b3c0ea2a4a6af42460600fdee7670e87897d9345",
      "1294692b3d3b0aa9b693ef2aa42f5595fd63c74d",
      "d4e9f3e05c7181d96244b56839b6b142ce2aa8a7",
      "68f4944752a3e33aab7df60e9b54390742ce6376",
      "3d22a7bb1611ba7846452fa9b1637a82c189dca6",
      "73c99aa46e744a56a492ffe8321c6a09a6e16268",
      "5b4c30696117af01c0ec30112affdbde8ba4ba06",
      "0f1cb3c073e1b973361c2d386ec82ca8d9aacce3",
      "78eb16113f63cd84c2471c36664bc7032ddc46db",
      "ae32cabd088fe0b00930ff4af012f18f6d830cc8",
      "7681f7902ecf70688c74ef8f53c682ca36208d61",
      "4ca4bcb7892c0f7b82927ac6d6f069d5b89f9acc",
      "3d76c152a2a107243a326ffb0e399b71b34cf9ea",
      "88d75bb9888e47edbc3309c75a13399f7a0d878d",
      "f5096855feebdb040b83637e7ec255a5cb88b83f",
      "d7021c8780f52f584160c3a4080d3d8d44ebd768",
      "2b96a3e2cc87691734e26b7817d7f94364132615"
    ],
    "message_list": [
      "add intital implementation",
      "minor updates",
      "implement zippath",
      "Update writers.py",
      "full implmentation",
      "Merge branch 'develop' into archive/export-refactor2",
      "fix pre-commit",
      "fix pre-commit (2)",
      "pre-commit fix (3!)",
      "fix `__all__`",
      "update ZipPath",
      "improve `verdi import` stdout",
      "fix pre-commit",
      "Add null writer",
      "convert `test_simple` to pytest\n\nplus add extra parameterization for file_format, and replace export_tree by export with file_format='folder'",
      "Apply suggestions from code review\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "Merge branch 'develop' into archive/export-refactor2",
      "commented out batch size --batch-size option",
      "cache at set",
      "Improve zip/tar read efficiency",
      "move compression code to archive-path module",
      "minor logging improvement",
      "minor update",
      "fix error",
      "Remove `safe_extract_tar` and `safe_extract_zip`\n\nUse `archive-path` `extra_tree` methods instead",
      "Add `zip-lowmemory` archive format",
      "change `_zipinfo_cache` default to None",
      "Merge branch 'develop' into archive/export-refactor2",
      "some extra typing fixes",
      "Merge branch 'develop' into archive/export-refactor2"
    ]
  },
  {
    "pr_number": 3613,
    "commits_list": [
      "ec6d7ea6293ac1d9dd84cbb7f1fda5f58155769f"
    ],
    "message_list": [
      "`GroupPath`: a utility to work with virtual `Group` hierarchies\n\nGroups can be used to store nodes in AiiDA, but do not have any builtin\nhierarchy themselves. However, often it may be useful to think of groups\nas folders on a filesystem and the nodes within them as the files.\n\nBuilding this functionality directly on the database would require\nsignificant changes, but a virtual hierarchy based on the group labels\ncan be readily provided. This is what the new utility class `GroupPath`\nfacilitates. It allows group labels to be interpreted as the hierarchy\nof groups. Example: consider one has groups with the following labels\n\n    group/sub/a\n    group/sub/b\n    group/other/c\n\nOne could see this as the group `group` containing the sub groups `sub`\nand `other`, with `sub` containing `a` and `b` itself. The `GroupPath`\nclass allows one to exploit this hierarchical naming::\n\n    path = GroupPath('group')\n    path.sub.a.get_group()  # will return group with label `group/sub/a`\n\nIt can also be used to create groups that do not yet exist:\n\n    path = GroupPath()\n    path.some.group.get_or_create_group()\n\nThis will create a `Group` with the label `some/group`. The `GroupPath`\nclass implements many other useful methods to make the traversing and\nmanipulating of groups a lot easier."
    ]
  },
  {
    "pr_number": 5270,
    "commits_list": [
      "7e088a9c3ea7ef4212c083cb805246736d899c77",
      "27bd603f97ce5c3fea25f487305d524c9f7f574e",
      "3c73bec3a1a684cda53981fd2922cf129b23d3ab",
      "1f64f805d1e40fdd8478b7aa0bd164c3e58788cf",
      "5e281af8640c911eb93f8b4e2a987eef907d3255",
      "5d2808862011f8247b5ff52e383d349d5a9ad4a3",
      "d723cdf75f68152320989fbe8d268335359b9e0b",
      "440b323b4a59e5139807c38fe0a3b79a66950932",
      "9a7b589075db7acd9b9fbdc700ce5400a803641e",
      "70b1674dccf340751a3a36ae3137dc16874f15cc",
      "64b6c3d82794e3d3f4c1ed1aec7bc7c3292e43f3",
      "e2467085961ebfd90a75459f3fe2cb5652750003",
      "0259d6df39cd470a2dbd26fbace43fa7cf8861f5",
      "72152e4dc658e0b95133ab76165c29b48df5e78c",
      "7e2c43b06c073c9da67ca957045206032715dec0",
      "5756e6d074ff5c23f4dcda369e742274adfccbfb",
      "d51165d267e47adc9e830f6b2b3fd3adb5ec8f30"
    ],
    "message_list": [
      "First prototype for the profile locking mechanism\n\nFrom now on aiida will keep track of all processes that request access\nto the profile by saving their PIDs inside:\n\n$AIIDA_PATH/access/profile_name/tracked/<process_id>.pid\n\nBefore returning control to the client, it will also check that there is\nno files of the form:\n\n$AIIDA_PATH/access/profile_name/locking/<process_id>.pid\n\nAs this would indicate that the profile is being locked by a process that\nrequires exclusive access for safety of its operations.\n\nFor a process to request such access it will first have to check that there\nare no active processes using the profile, so it will look at all files in\nthe `tracked` folder and compare those to the currently running processes\nin the system to check that none is actually active (it will also delete\nthose outdated tracking files in the process).\n\nThe design is as follows:\n\n - A ProcessData class was defined to store the information relevant to the\n   processes.\n\n     - It can be initialized either with a PID (of a process to be looked\n       among those currently running in the system) or with a filepath\n       (where the data of a previous process was stored, typically either\n       in the `locking` or the `tracked` folders). If none of these are\n       provided, it will load the info of the currently running process\n       that is calling the execution.\n\n     - The class also has a method to write the information to a given\n       filepath (typically in the `locking` or the `tracked` folders).\n\n - An AccessManager class to control the access to a profile.\n\n     - It can be initialized with a profile to which the class will control\n       the access to (by default it loads the one currenly in use).\n\n     - It has a couple of internal methods to distribute and modularize the\n       different tasks, but the most important externally are:\n\n         - profile_locking_context: a context that can be called to work\n           with the profile locked. It will raise LockedProfileError if the\n           profile is already locked or LockingProfileError if the profile\n           is being accessed by other processes.\n\n         - record_process_access: a method to record the accessing to the\n           profile. It is now being called in `load_backend_environment`\n           to make sure every process that loads the backend gets recorded.\n           It will raise LockedProfileError if the profile is already\n           locked.",
      "New general structure for the profile locking mechanism.\n\nIt introduces the class `ProfileAccessManager`, used to keep track and\ncontrol what system processes access the aiida profiles. It has the\nfollowing public methods:\n\n - `request_access`: to be called every time the profile is loaded\n   (for example, inside of `load_backend_environment` in the module\n   `aiida.backends.manager:BackendManager`). A file will be created\n   with the process ID as its name so as to keep track of who is\n   accessing (or has accessed) the profile.\n\n - `lock`: this context manager makes sure the process calling it is\n   the only one accessing the profile. It does so by checking that\n   there are not tracking files that correspond to currently running\n   processes and by creating a locking file that will prevent other\n   processes from accessing the profile.\n\n - `is_active` / `is_locked`: for clients to easily check if a profile\n   is being used or locked by running processes (respectively). I would\n   not recommend to use these as guards before calling `request_access`\n   or `lock` since running conditions are still possible, it is better\n   to use `try ... except ...` instead.\n\n - `clear_locks`: this removes any current lock by force-deleting the\n   locking file. This will not stop any process that was locking the\n   profile if it is still running: that process will still \"think\" it\n   has exclusive access to that profile, which could result in data\n   corruption. This method should be used with extreme care.\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "Merge branch 'develop' into lock_profile",
      "Merge branch 'develop' into lock_profile",
      "Add init to see if it helps with path recognition",
      "Fix tests and docs",
      "Small display adjustment",
      "Add tests",
      "Apply requested change for point of entry",
      "Simplified and re-organized tests",
      "Merge branch 'develop' into lock_profile",
      "Improved TestProcess class for testing",
      "Fix sub-process default profile dependency",
      "Merge branch 'develop' into lock_profile",
      "Fix aux class name and docstrings.",
      "Moved files around.",
      "Merge branch 'develop' into lock_profile"
    ]
  },
  {
    "pr_number": 2471,
    "commits_list": [
      "14c135b4f1e60c091eec0fdeeaa27288e72baae4",
      "b14e5518884d484b4f6f39ffd37c1c3dac5de033"
    ],
    "message_list": [
      "Providing a non-ORM way to add nodes to a group. This speeds up a lot the group addition procedure and import method of SQLA should benefit from it",
      "Merge branch 'release_v0.12.3' into issue_1319_for_v0.12"
    ]
  },
  {
    "pr_number": 3787,
    "commits_list": [
      "62462ddeeb6a129be12a5ba3635d7714c455718d",
      "8b58288fc3f0fe0cc743ce9e1473d68264369e36",
      "10a245ab0e15bc825154fa0bc25c4fd0c240bcb7",
      "c8cbb6e5bf8a009aa805584641e9d460c5a45b19",
      "406a3bc6ec3eca49efcb55e95bbefd82c5ca5a4c"
    ],
    "message_list": [
      "Avoid deadlocks when retrieving stdout/stderr via SSH\n\nFixes #1525\n\nThe simple shift of the line `retval = channel.recv_exit_status()`\nbelow `stdout.read()` and `stderr.read()`, while partially improving\nthe situation, is not enough (see discussion in #1525).\nThis instead reads in chunks both stdout and stderr, alternating.\nTests show that this does not hang anymore for large files.",
      "Major refactoring of the exec_commmand_wait functionality\n\nNow, I add a exec_commmand_wait_bytes that actually does the job, and this\nneeds to be implemented by plugins. The two plugins implemented by AiiDA\nalready define instead the new function.\nAlso, I improved in both the API for the stdin, so that these commands\ncan accept also bytes ot BytesIO objects, that makes more sense in general.\n\nI tried to keep the API (when calling them) backward-compatible, so using e.g.\na str/StringIO as stdin still works, and similarly `exec_command_wait` works\nas before, but adds an optional `encoding` parameter (the default is `utf-8` to\ngive the same old behavior, but give more flexibility now in using different\nencodings).\nTests have been added for these usecases.\n\nHowever, note that the interface of plugins have changed (plugins now have to define\ninstead the `exec_command_wait_bytes` function instead). The change is minimal\n(change the function name and avoid decoding the bytes into strings) but plugins\nextending the transport functionality should take care of this (and, if they drop\nthe `exec_command_wait`, they should be depending on AiiDA >=1.6; otherwise they\nshould copy the implementation from the general Transport class if they want to remain\nbackward-compatible).\n\nAlso, I changed a couple of places to call directly the _bytes version, to avoid\npossible issues with strange encodings on the remote computer.\nOthers are left in to avoid too many changes in the code, since until now noone complained\nbecause they had a weird encoding on the supercomputer.\nWhen such issue will arise we'll fix it, and thanks to this PR now the code is all ready\nto move to treat bytes directly (or use a custom encoding, that e.g. might need to be\ndefined in the `verdi computer setup` or `verdi computer configure` steps, in the future).",
      "Optimizing the parameters to have a high throughput\n\nI could get ~100MB/s when not using compression, ~33MB/s when using it\n(the second case is capped by the compression speed; the time is only\nmarginally larger of the one I get if I run, on the command line,\n`time ssh -C localhost cat /path/to/remote/file | md5sum`;\nthe first one is not as good as running md5 (via SSH) on the command line\n(which can go ~3x faster of a 4GB file)\nbut it's still acceptable for most usecases.\n\n(Note that here we're speaking of differences measurable only when\nsending hundreds of MB sent over stdout, which should not be anyway\nthe default way of transferring a lot of data).",
      "Adding comments from code review\n\nCo-authored-by: Francisco Ramirez <francisco.ramirez@epfl.ch>",
      "Merge branch 'develop' into fix_1525_hanging_ssh"
    ]
  },
  {
    "pr_number": 5507,
    "commits_list": [
      "c6ee523a737527d3f2b809e7113267d5beca22c7",
      "b833510ce0a99f871b3d7154b5dde2cf9fdf26c4",
      "e47766597bb6e0c03a58796376d95c9b7ecf3dd6",
      "63e7a2b2145ac451500e3ac3c58a372265f15217",
      "e3fade6ef11c6e568e858776d448d9c0d039ec0f",
      "4e422a4f26920b21fce629b27d20e99910c811bf"
    ],
    "message_list": [
      "Running containerized code in aiida-core\n\nThe containerized code is allowed to setting through cmdline and used in calculation.\nThe containerized engines supported and tested are docker, Sarus and Singularity.\nIt is shown below how to configure the code and running calculation.\nI will then move the example below into documentation.\n\nTest command line options for containerized code\n\ndocstring added\n\n[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nreview\n\n[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "CI test for containerized code\n\ntest docker run",
      "DOC and TEST: containerized code how to set and how to use\n\npytest fix\n\nAdd more docs in data_types\n\nUse localhost to test add-docker container",
      "Attempt of inner mpi call",
      "r",
      "Merge branch 'main' into design/containerized-code"
    ]
  },
  {
    "pr_number": 4208,
    "commits_list": [
      "05020d32e370834aedc1f37aa94326d3e9054910",
      "5782cad6c92a97ebd1d5678968972d4c09ed8ac8",
      "b378e06142ecc2ade6601512e306e8529b992e47",
      "1b0fb395274410d877c58a2b42861a28b6541e9a",
      "8cbf259c0f35488b13397f5ba42fe0fc86d04f93",
      "c06dfe40025aee9d4c4f3a712dbedbcbae9eff0e",
      "fdc8537be80419675a3d78233aa1654425048d5d",
      "c526e1715bec3afde1cb56c9579770def605b0f2",
      "d41c0b5564249a74f047b6bef80120e6801c4197",
      "771552e93bc1a9da4aea60a16ba9ec2df39864c4",
      "56ac60bd34060a653f1770c93c3967b1a8534b06"
    ],
    "message_list": [
      "docs: revise \"how to run external codes\"\n\nGive a pass to this relatively new section.",
      "Apply suggestions from code review\n\nthanks @sphuber!\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "address comments by @sphuber",
      "add back sentence on outputs",
      "Apply suggestions from code review\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "apply suggestions by @csadorf",
      "finish \"writing a plugin for an external code\"\n\n * add SimpleArithmeticAddParser to codebase (+ add one test)\n * add complete example of how to run the created calculation and parser\nplugins\n * add complete example of how to register entry points for these",
      "Apply suggestions from code review\r\n\r\nthanks a lot for the thorough read @sphuber !\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "Apply suggestions from code review",
      "remove section on running without entry points\n\nWorking without entry points just complicates things without providing\nmuch benefit to the user.",
      "Merge branch 'develop' into tutorial-running-external-codes"
    ]
  },
  {
    "pr_number": 3319,
    "commits_list": [
      "a45e62c5e787b0f5a28ebad215ea666219c9cfb5",
      "c1b3f5a52c6d7a38e60fe95cab55b55cdb8d427f",
      "5b1967c29692d6ef415d0997a6840a83a1d5740a",
      "84977bf3a1170bcbaabcb331bb5dd34bbde54188",
      "ca8f1c60c452917280cdebf1e9356812e679f41b",
      "ebd3f68a67e6d5898b775f5b30969953041452cd",
      "57d452ad77b9d2a905197e2997049fbb26bc198a",
      "8c0b53794eae7ef5dc0c622a87de8eebdfb7f6a8",
      "883ccd265512c4d3b833292b80d9240bd027addb",
      "d37cfbec7860b979ca0a20a5f88b59ca0e17e9bb",
      "4f80c13f1fb91c0c017fb525d6eea2a2d2123a39",
      "70da425f6b7606fe472b1188200ba738ab2fe875",
      "68113c26545056593b92565546c0995512dd3599",
      "597e4b4a6e508e73343b34565faa7a9a9b2cded2",
      "f593d9afaa37cf835e4be1cf8fc0fccbd62ba475",
      "8df9b90f4e56a6a240d8116e20d1e6c7c4058b60",
      "735dc2d5d2fca62c93510b9791926fb40087e477",
      "686b9c0b7f24b568cd0ea0b1fa6818c75fd980ab",
      "abcc353b0b35a8437fbd9fc3bc89c3328fe86277",
      "3b5e9ea24bca6c769331ac58ba6a2e850826b23a",
      "a6f558ccb03d36a696d8e0e0be76b8883a523b8c",
      "a9568f37c92e03a5ca9756668a78b507030e7786",
      "edc96004da02035be16ce8300eff70d7b59199d3",
      "43fea6c772930945c35621c24ec4b31e49b1b35b",
      "403fd63c019ee7fe423126f8ea90a90aa5816efb",
      "1a636d0f49059c9b3fda98d3a19ed5b47a6ccf1a",
      "61d696e9a2263f1a8d8dd1e032a900ee52821cdb",
      "2999853dddcf90f5d4be521599aabbd012421f51",
      "70c234a042e3b8401e216ba451cdb2ed32a22b47",
      "912c3c2faf11c02b00ed57fee9566fb162b0b942"
    ],
    "message_list": [
      "reorganize plugin testing code\n\nplugin testing code is split up from a single file fixtures.py (that\nactually did not contain a single fixture) over multiple files\n\n  * one for the manager, which is now called TestManager\n    (it has nothing to do with fixtures)\n  * one for test classes & runners for use with unittest\n  * one for (newly added) fixtures for use with pytest",
      "add simple test for pytest fixtures",
      "update docs",
      "add automatic backend handling\n\nboth unittest test classes and pytest fixtures will now use the backend\ndefined in the `TEST_AIIDA_BACKEND` environment variable.",
      "remove double underscore class variables\n\nfor consistency (used nowhere else in the code). + see\nhttps://stackoverflow.com/a/6930223",
      "make temporary test profile optional\n\nby specifying the environment variable TEST_AIIDA_PROFILE=profilename\none can use an existing profile instead of setting up a test profile\nautomatically.",
      "fixes for test_test_manager",
      "Merge branch 'develop' into add-pytest-fixtures2",
      "fix aiida_local_code_factory\n\n * fix name according to suggestion\n * fix bug in database query\n\nsince the code label no longer referred to the full label (including hte\ncomputer), `Code.objects.get(label='network@computer')` would always\nturn up empty-handed, causing the utility to create a new code.\n\nNow, we're searching all available codes with the `network` label and\nsimply take the first one (if it exists).",
      "fix linter and docs",
      "fix ResourceWarning issue\n\nResourceWarning introduced in python 3.2",
      "ignore undefined variable",
      "fix logic bug in TemporaryProfileManager\n\nProbably edited code at a wrong position at some point...",
      "add pytest-cov dependency",
      "fix running on existing profile\n\n * add reset function to reset default user cache in User.objects\n * simplify logic for initializing DB in ProfileManager /\n   TemporaryProfileManager\n * restrict Capturing to setup of AiiDa profile (instead of the full\ntest run using the PluginTestCase)",
      "Merge branch 'develop' into add-pytest-fixtures2",
      "fix issue when code not found\n\nThe current implementation would not throw an error if the code was not\nfound but rather an \"IndexError\" because the returned list was empty.",
      "Merge branch 'add-pytest-fixtures2' of github.com:ltalirz/aiida_core into add-pytest-fixtures2",
      "update plugin testing documentation\n\n * remove section of documentation on avoiding imports at the module\n   level - this is no longer an issue\n * add information on `TEST_AIIDA_PROFILE` environment variable\n   for controlling testing environment\n * reorder some parts of the plugin testing docs",
      "remove autouse=True from clear_database fixture\n\nWith `autouse=True`, there is no straightforward way for plugin authors\nto disable use of a fixture.\nWhile this makes sense for the `aiida_profile` fixture, we've\nencountered cases where resetting the database in between tests may lead\nto issues.\n\nThis problem is solved by keeping the fixture available but requiring\nusers to depend on it explicitly.",
      "Apply suggestions from code review\n\nCo-Authored-By: Sebastiaan Huber <mail@sphuber.net>",
      "implement suggestions by @sphuber",
      "rename TEST_AIIDA_PROFILE => AIIDA_TEST_PROFILE\n\n * rename TEST_AIIDA_PROFILE => AIIDA_TEST_PROFILE\n * rename TEST_AIIDA_BACKEND => AIIDA_TEST_BACKEND\n * add warning when setting AIIDA_TEST_PROFILE while running\n   `verdi devel tests`\n * make test profile configurable at the test_manager level,\n   so that the context manager can be used without having to mess\n   with environment variables.",
      "Merge branch 'develop' into add-pytest-fixtures2",
      "Merge branch 'develop' into add-pytest-fixtures2",
      "Merge branch 'add-pytest-fixtures2' of github.com:ltalirz/aiida_core into add-pytest-fixtures2",
      "change cache reset to work on User.obects only\n\nimplement specific cache reset method for User.objects",
      "Merge branch 'develop' into add-pytest-fixtures2",
      "rename TEST_AIIDA_BACKEND on travis",
      "add test for aiida_local_code_factory"
    ]
  },
  {
    "pr_number": 2421,
    "commits_list": [
      "b948b2aa8c2f7756edc877c85ae95219eaefee01",
      "8c063ba2b725527dd3c78f7aef51549d895436fc",
      "541bb8428cdf851df0aad54f96d978eb6aa58a31",
      "7892800ef822b4bb81d73fbd1337932ce25346c6",
      "fbc5ce2acf67e07f2f496888ee7112dda06b5115",
      "8dccfce510af89052cba5e8501373812e83a8962",
      "b6ef0e5f09a57342873eda0ee125de2ad8d49d24",
      "15e500bc8ad8e5358de0a8b7be6c27ccdaa56c7e",
      "77769dd33ba01eb9e05f90c481e7e509f53d4830",
      "c73fb4de236f2cb23dc03ab3a83737361a1f1e5f",
      "9971bc9de5acd564bb93fc7efe174af87a6c243b",
      "a949e36d036ce1a7e78f0357e0afbfaf39949cd3",
      "07f57bcda97d21c5a28040f8310ea113be0b566a",
      "df825ddcdc3b9a90c24345e1fb8648741077d014",
      "268270ae926c79d44f3b32a030f9bc47ce5e24b4",
      "51af14d695f08f1fab839e677363e55d10fe65d9",
      "aca20c01e37c577334edad10db49752169666868",
      "897a7a1dbe6a667b6dfe02eda2596e242f6cb58b",
      "3285a194344a65783665210e8c225b792618b294",
      "6411e941a4da3b0ac98bb84b036bea9b4dcd3b61",
      "ef6c839e77e9a119a81089189da724fe80b1ef3c",
      "b99f9a5cb893b3fcc75eeb625750519199498e7c",
      "d00eafe972ca546485cbf4b1085d939e29575bd4",
      "7485abe65264161e0c65ee4bf37d9e153c533d6e",
      "a0c8c52707d9272eec2e3f427f619116fa01c097",
      "c11d59d9af8e48203b04f95a4e898fbb61a22854",
      "c84a70ffac4b6b3de7765833ec0d798b8adfbc2c",
      "fe8b7e89334610af546bd88110e7d2eb7d73b6ea",
      "fc1c4915adc2910e2caef6ed3e6fe441509d78ed"
    ],
    "message_list": [
      "Implement support for process classes to `QueryBuilder.append`\n\nAny `Process` class will have an associated `ProcessNode` sub class\nassociated with it that it will use to store its provenance when ran. In\naddition with the `process_type`, determined by its specific process sub\nclass entry point, defines a set of nodes that correspond to that\nprocess class.\n\nFor example: the `ArithmeticAddCalculation` will use a `CalcJobNode` to\nstore its executions and has a process type according to its entry point:\n\n    `aiida.calculations:arithmetic.add`\n\nThis commit implements the functionality necessary that allows a user to\nappend the `ArithmeticAddCalculation` process class to a query builder\ninstance directly.",
      "Merge branch 'provenance_redesign' into issue_2400_process_in_querybuilder",
      "add subclassing for process_type\n\n + start adding tests\n\ndirty commit - first need to get a working test database",
      "minor refactoring\n\n * set_process_type => build_process_type\n * move call to build_process_type outside _set_process_type",
      "fix tests\n\n * check that warnings are raised when querying for process classes\n   that don't have an entry point (and are not built-in)",
      "Merge branch 'issue_2400_process_in_querybuilder' of github.com:ltalirz/aiida_core into issue_2400_process_in_querybuilder",
      "split qb documentation across multiple files",
      "add note on process class",
      "move teardown to testimplbase\n\nremove unnecessary code duplication by moving the teardownclass_method\nfrom the backend into the base class",
      "reset manager in teardownclass_method",
      "move file repository cleaning further up the chain\n\n + fix wrong reference in docs",
      "fix pre-commit",
      "Merge branch 'provenance_redesign' into issue_2400_process_in_querybuilder",
      "Merge branch 'provenance_redesign' into issue_2400_process_in_querybuilder",
      "Merge branch 'provenance_redesign' into issue_2400_process_in_querybuilder",
      "fix most query tests\n\none query test still failing - querying for workfunction nodes\ndoesn't seem to work anymore...",
      "Merge branch 'provenance_redesign' into issue_2400_process_in_querybuilder",
      "Merge branch 'issue_2400_process_in_querybuilder' of github.com:ltalirz/aiida_core into issue_2400_process_in_querybuilder",
      "get rid of _set_process_type\n\nuse setter from top-level Node class",
      "add test_query to pre-commit\n\n + use locally defined WorkChain to avoid problems with\n   references to ORM objects stored inside WorkChain class",
      "fixes for newer pylint version",
      "fix pylint versions\n\notherwise, pylint may update on travis but not locally\n(not even after pip install -e .[dev_precommit])",
      "hopefully final pre-commit fixes",
      "Merge branch 'provenance_redesign' into issue_2400_process_in_querybuilder",
      "Merge branch 'provenance_redesign' of https://github.com/aiidateam/aiida_core into issue_2400_process_in_querybuilder",
      "minor adaptations",
      "fix verdi daemon issue\n\nnot sure why this was not fixed automatically...",
      "fix string renames",
      "address comments by @sphuber"
    ]
  },
  {
    "pr_number": 4970,
    "commits_list": [
      "713656d4beba3bd5fe0c2a475af689464c57fc7e",
      "fc7926a340c97757480e1a09d03a907a94ace487",
      "4f9b9861f7f57aabb6c4b693e34cffa9c437bb9d",
      "0431dde1b2d952469dd62fa2f4732976442ba0fa",
      "297db0cbae3b21bc649d589a2ed0e42fdccf1bb9",
      "6530761e75b400c7c457ea5f5e572446a95ea4b9",
      "2b1fe53c42e45a1996992092c4ce71ac78aa0359",
      "91bdb979fc17465969e662f1864c3e5eb50e8706",
      "13084211d8e3b85dd40fb9f08f9d9b95cd19b2fb",
      "2903eac618500d44511ffaf66d14b885ebd501ff",
      "27005b5dc64407828697ae5ea47a00e6f0a5f83a",
      "54b9bda9dbd2f3cd6c3b18ea8f8857eda225be19",
      "b85abc61a363d30603107d26af79d0374ffd1c03",
      "ff48c17a04d1aa7a687568d4502f2385b0ecfc47",
      "9fddc580efddc97a01874b551e07cc48c1aa323f",
      "cd91b0ee9313fb755bd3cf6ab6775ae43c2a8f7d"
    ],
    "message_list": [
      "`ProcessBuilder`: Add HTML representation\n\nThe representation of the `ProcessBuilder` (i.e. `__repr__`) is\ndifficult to read since it just prints the heavily nested mapping on one\nline. Here we add a `_repr_html_` method to improve the readability\nwhen using the `ProcessBuilder` in e.g. Jupyter notebooks.",
      "Switch to using _repr_pretty_",
      "Fix pre-commit (now tested with tox!)",
      "Attempt to appease the CodeCov gods",
      "More tests",
      "Apply reviewer suggestions",
      "use Dict import; switch to using textwrap.indent",
      "Switch to using a `JSONEncoder`",
      "fix pre-commit",
      "Reorganise tests",
      "Fixed mypy (I hope)",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "more test coverage",
      "Apply reviewer suggestions",
      "Add `types-PyYAML` dependency for pre-commmit",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci"
    ]
  },
  {
    "pr_number": 4129,
    "commits_list": [
      "980d55a8ae8192c70d2ea4c7dae20f987830d1df",
      "3c5e1e5178e38e6eddd3045e9b1ad07edec616a1",
      "fa5c65355759a2bbba129caa923b21c0827ccf81",
      "b6c5b826ec8cba2800a821d4de3e28e66e15a6c3",
      "b4c4f1e1f2036025ac1fd147defebbdd89adf0eb"
    ],
    "message_list": [
      "add top level indexes",
      "undo unintentional change",
      "Move to pydata-sphinx-theme",
      "fixes",
      "Fix CSS of top bar link icons"
    ]
  },
  {
    "pr_number": 4362,
    "commits_list": [
      "27498d4506667f610fd754c4e28fe34460606d8e",
      "8ed6cd3f056d6cb688c2d671c8c6763500434ec3",
      "ae5e294899360ccf05cc32d28ab76c30aec8ad8c",
      "3146ba45ec3991ea5d8e949cd14be55b21da9b37",
      "8fc68ff6a7e853a9dd399a5c4b9aa1f83edcc248",
      "40d64ce39fdc9cdfbdccb7bacafa5ec82aba82f7",
      "21d419fb35c0bc99c544126e5d26f1360a2cafc2",
      "ce1cb761c21615168a9086e0159c7f32bc9776db",
      "92a4be5538ade572e34d7d9622e237b5ae52f8e4",
      "45b1114af9d1d9a2e3eef05c8ce3c89dd3313d85",
      "0b414aaffcca5feaf2cfc04e11361e6108d8aba4",
      "ef2b61d687a40047f36462c1cc7a7f1f99f63369",
      "9029b77eadfb7f9451d56d0386f4ad6d3b24caf3",
      "02bae99e1ea821f213361fa2640542f440dd17fb",
      "80cd7e00d5d9c1fe6cc27132f38573e184586239",
      "859210745e604cfc48afad5475e3170b5bf9f639",
      "8c5ac9c469ec447733fdc87d29b2021efe5f233f",
      "05bf66a25c2efa420aa5bd3a96119513e8e2f0ae",
      "ecf7da85874a49a4d3c136c04cf29b7007232963",
      "1572d7806548c3aa2aeeabc6bdbbf54ea6fa82da",
      "97558ec3114ddb678011452e6dcce6c5976df600",
      "b0750653ae327c1f7291c55c1a38f2dd035cbaa7",
      "957ae068c74712befb810d3cad1d815acbf8b9b5",
      "be94c5e2ac3e4c75181f03e2a7b711af340bc761",
      "c6120d5a55d50e60586a0cecb36ccd7db1efa190",
      "a883674a8379d12d72d1549af6d54e0b2c0a3d61",
      "2a11abe84756827a76e1709f1727985a6d5e470f",
      "bc2d43a9ed2320c9dbd4a3b89f2e26f494a86d63",
      "988b4e25cf168fb4e08fb3fcb9659fffa558dba3",
      "8328e07c352e3c84cb7f4a67fe1c4537eb983ce4"
    ],
    "message_list": [
      "\ud83d\udd27 Add tox configuration",
      "\u2b06\ufe0f UPGRADE: Move to tomlkit",
      "Merge branch 'develop' into add-tox",
      "\ud83e\uddea TESTS: Add initial benchmark code",
      "move to ` chrisjsewell/github-action-benchmark@v2`",
      "hotfix tests with rabbitmq",
      "temporarily disable CI",
      "group nodes [ci skip]",
      "update [ci skip]",
      "update [ci skip]",
      "update [ci skip]",
      "update [ci skip]",
      "update [ci skip]",
      "update [ci skip]",
      "update [ci skip]",
      "update [ci skip]",
      "Merge branch 'develop' into benchmark-test-cjs",
      "update [ci skip]",
      "Merge branch 'benchmark-test-cjs' of https://github.com/aiidateam/aiida-core into benchmark-test-cjs",
      "update [ci skip]",
      "update [ci skip]",
      "update [ci skip]",
      "Merge branch 'develop' into benchmark-test-cjs",
      "update [ci skip]",
      "fix rebase error [ci skip]",
      "update [ci skip]",
      "update [ci skip]",
      "update [skip ci]",
      "Merge branch 'develop' into benchmark-test-cjs",
      "update [skip ci]"
    ]
  },
  {
    "pr_number": 4093,
    "commits_list": [
      "fddb693332116c7fa5c2a6a6ff5e4e6ea94476a5",
      "022b576075caf979897290ab82a122b0e2749115",
      "a20429668a53260403e29f51fac6f616d933e50b",
      "19e6057d03e8ecaed3d0211915840bbf78924e07"
    ],
    "message_list": [
      "Add how-to interface codes section\n\nIt is now divided into how-to interface codes and how-to parse outputs.\nThe parse output section also contains a sub-section on how to handle\nerrors during parsing.",
      "Latest set of corrections.",
      "Quick-apply suggestions from code review\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "Final changes commited."
    ]
  },
  {
    "pr_number": 3944,
    "commits_list": [
      "04e01fa722a839907eb57283ed151ecb4daa8882"
    ],
    "message_list": [
      "update spglib==1.15.0"
    ]
  },
  {
    "pr_number": 3325,
    "commits_list": [
      "041ed69ea20e70b4df32462234c1c86cd6ac5c14",
      "99aa63b6b5c32c7f6e88b1e614115710a4b85360",
      "8db9ceb006e533c8b295c9d3d0c1f73c49e8216d",
      "bd00c484d115b1abae6615efddd1b0c83d1b75df",
      "9b5527de1582747b6348efd0559e5674f20c6d14",
      "2f4e20019ac528956fd66287ef9a683460185a5d",
      "2cc05fd47f5f0847d95ec9c51993df732bf4dab3",
      "57115756444e0c641586ae15590781e2f87a4053",
      "2917f8363810946686b7a9cc7f2d9427aef474ae",
      "2f5a26311655f8083cfa1c748257e20a0510d32e",
      "444153b677abd3ce9d5e66acdb677949c9b6ccf3"
    ],
    "message_list": [
      "Documentation added for delete and export features.",
      "Added a few paragraphs on how to use graph-easy.",
      "Corrections to the delete/export section applied: it was made more concise,\nwith less text and more images.",
      "Moved the import in some files because there where some problems\nwith them when making the documentation.",
      "There was a highlighted section (it was made this way as a reminder for review purposes) that I had to fix.",
      "Merge branch 'develop' into delete_docs",
      "Merge branch 'develop' of https://github.com/aiidateam/aiida_core into delete_docs",
      "Requested corrections applied:\n* The section explaining how delete/export work was renamed \"Consistency\" and moved as a subsection of \"Terms and Concepts/Provenance\".\n* A section named \"Deleting Nodes\" was included beneath \"Working with AiiDA\" with the command usage and a reference to the concept explanation.\n* A line was added in the \"Import and Export\" section to reference the concept explanation.",
      "Merge branch 'develop' of https://github.com/aiidateam/aiida_core into delete_docs",
      "Merge branch 'develop' into delete_docs",
      "Merge branch 'develop' into delete_docs"
    ]
  },
  {
    "pr_number": 4496,
    "commits_list": [
      "609876d205c98bb76733c0e7a3118a95940b64a5",
      "fc3309529701d66eb90b167c304b9af242a874ae",
      "3011bc73ca1b3778b01b794e243b7d1ef007baad",
      "0bed035aef324504babbda7662c6937321f30e8c",
      "b97c47968d65203d7d2b767d6d89f7a73e9a2ddc",
      "bd934139d7295e1dae4a0ecfa5914386abc7d914",
      "0867e09ee112d903d8d649ca4452abc65821164b",
      "b4113ca6e027d2bcac6115d6708a46b6cf7706a5",
      "f90328570f7adc49673573a090aa80ab66dba465",
      "778e31ae9dece1293969b13b566cf1dc9050e6b2",
      "a9c57c6195c843c7f77d179e3f28dbd1deb3aa7f",
      "9c6ff076678a4ed90ca1422504a6f37f522b2a68",
      "a7140eed4caa7186a8c6d5533ce9ee9f4aba81e5",
      "afa249a56f437f726d8baa04a499b2090c5dc594"
    ],
    "message_list": [
      "Revise \"Get started\" page.\n\n - Overhaul of page structure and installation flow.\n - Incorporate content from detailed installation page.",
      "Rename 'Detailed installation' page to 'Avanced configuration'.\n\nAnd remove all content covered by the get started page.",
      "Fix references.",
      "Add hint for differing commands for activation of virtual environments.",
      "Place individual setup routes on separate pages.",
      "Swap 'pip + venv' and 'Conda' tabs in system-wide install.\n\nThat makes it more likely for users to choose that method which is\noverall still better supported than Conda; the latter has no provision\nto install extras.",
      "Update WSL 1+2 installation instructions\n\n- Distinguishes between WSL 1 / 2 by how RabbitMQ is installed only\n- Adds instructions for setting up a task in Task Scheduler for\n  starting the services.\n- Removes the warning about the timezone problem, as it seems to\n  be fixed.",
      "Fix formatting of WSL install instructions.",
      "Tweak the WSL instructions.",
      "Extend docker instructions.\n\nTo retain more from the previous advanced installation instructions.",
      "Replace all 'hint' admonitions with 'tip'.",
      "Directly link to Quantum Mobile docs for VM route.",
      "Adjust the list of tested Ubuntu versions.",
      "Merge branch 'develop' into docs/issue-4254-installation-flow"
    ]
  },
  {
    "pr_number": 3618,
    "commits_list": [
      "a227bd30a1543de59cbdc970a9194a0cb805ac18",
      "588211275d8a9284a47a439a426a00dd68a3e407",
      "42d0cd56a58ce243384bcdd29dd2b982c8f052b6"
    ],
    "message_list": [
      "(re-)implement coverage reports via codecov.io\n\nfixes #3602",
      "Update README coverage badge\n\nOnly upload coverage for Py3.5",
      "Remove omissions from coveragerc and reorder tests"
    ]
  },
  {
    "pr_number": 2657,
    "commits_list": [
      "996dd01985181bff945ce39a1997aec03e92493b",
      "0b6e940b9095b21202b2508bf38ac81ddb6f1efb",
      "282b644669470b5ef5628a615c9983aa6ba56f78"
    ],
    "message_list": [
      "silence psycopg2 warning\n\nI think we have been warned enough:\n\nUserWarning: The psycopg2 wheel package will be renamed from release\n2.8; in order to keep installing from binary please use \"pip install\npsycopg2-binary\" instead. For details see:\n<http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.",
      "work around pylint bug",
      "Merge branch 'develop' into silence-psycopg2-warning"
    ]
  },
  {
    "pr_number": 4548,
    "commits_list": [
      "7f7d42075e7ca91a2c3654467e16b312f2218953",
      "cb19d49b90a260837c511455207bc96b337e0689",
      "c778a7d0fe4b3208225f5b75bcd7258ef9593a92"
    ],
    "message_list": [
      "docs: include ipython magics from source code\n\nIncluding the snippet to register the AiiDA ipython magics from the\naiida-core codebase instead of the (already outdated) copy-pasted\nversion.\n\nAlso revisit the corresponding section of the documentation, starting\nwith the setup, and removing some generic information about jupyter.",
      "add tests for IPython extensions\n\n * add test for AiiDA IPython extension\n * move some additional lines from the registration snippet to\naiida-core (where we can adapt it if the IPython API ever changes)\n * rename misnomer `load_ipython_extension` to\n   `register_ipython_extension` with deprecation far in the future",
      "apply suggestions from code review\n\n + try suggestion by @chrisjsewell\n + fix pre-commit failure"
    ]
  },
  {
    "pr_number": 3466,
    "commits_list": [
      "425d39d11470d920bd49a5974a89f742b344e122",
      "3f3dc0869c0587b5591bd06fb7c4bb8c68c41941",
      "69f40372bdb1cb82c0a7bae1fd3350073872a1e4"
    ],
    "message_list": [
      "Emit a warning when input port specifies a node instance as default\n\nUsing mutable objects as defaults for `InputPorts` can lead to\nunexpected result and should be discouraged. Implementing this for all\ntypes, including python builtins such as lists and dictionaries, is\ndifficult and not the most pressing problem. The biggest problem is\nusers specifying node instances as defaults. This will cause the backend\nto be loaded just when the process class is imported, which can cause\nproblems during the building of documentation and with unit testing\nwhere instances in memory to deleted nodes can be referenced, among\nother things.\n\nTo warn users for these complications, we override the `InputPort`\nconstructor to check the type of the default and emit a warning if it\nis a node instance.\n\nThe `CalcJob` implementation had to be adapted to conform with the new\nrequirements as the `mpirun_extra_params` and `environment_variables`\nmetadata options were using mutable types for their default. Just as\ncertain test process classes, the defaults are now prefixed with the\n`lambda` keyword.",
      "Merge branch 'develop' into fix_3143_warn_mutable_port_defaults",
      "Merge branch 'develop' into fix_3143_warn_mutable_port_defaults"
    ]
  },
  {
    "pr_number": 5184,
    "commits_list": [
      "37b79ad3f4dd990b21584a4c5ee67271596a5c7c"
    ],
    "message_list": [
      "`Code`: add `validate_remote_exec_path` method to check executable\n\nA common problem is that the filepath of the executable for remote codes\nis mistyped by accident. The user often doesn't realize until they\nlaunch a calculation and it mysteriously fails with a non-descript\nerror. They have to look into the output files to find that the\nexecutable could not be found.\n\nAt that point, it is not trivial to correct the mistake because the\n`Code` cannot be edited nor can it be deleted, without first deleting\nthe calculation that was just run first. Therefore, it would be nice to\nwarn the user at the time of the code creation or storing.\n\nHowever, the check requires opening a connection to the associated\ncomputer which carries both significant overhead, and it may not always\nbe available at time of the code creation. Setup scripts for automated\nenvironments may want to configure the computers and codes at a time\nwhen they cannot be necessarily reached. Therefore, preventing codes\nfrom being created in this case is not acceptable.\n\nThe compromise is to implement the check in `validate_remote_exec_path`\nwhich can then freely be called by a user to check if the executable of\nthe remote code is usable. The method is added to the CLI through the\naddition of the command `verdi code test`. Also here, we decide to not\nadd the check by default to `verdi code setup` as that should be able to\nfunction without internet connection and with minimal overhead. The docs\nare updated to encourage the user to run `verdi code test` before using\nit in any calculations if they want to make sure it is functioning. In\nthe future, additional checks can be added to this command."
    ]
  },
  {
    "pr_number": 5123,
    "commits_list": [
      "218af2f2796f13f3bc07332ad47b0a30626ffd53",
      "0c839e3d77616612de2f5aa0ff90f09764bbb579",
      "a7a27cae918adbd998826d79c7b061c0367c1404",
      "7e0cb192f5227f2075c1c9c646fcd8dbc01cb4b1"
    ],
    "message_list": [
      "\ud83d\udc1b FIX: Ensure Container DB always closed after access",
      "Add additional context managers",
      "Update disk_object_store.py",
      "Remove the `container` public property\n\nThis should not be used externally. The two tests that were using it\nshould also not have to use it directly."
    ]
  },
  {
    "pr_number": 3686,
    "commits_list": [
      "d6afc369f4c227efc69b4469b0cbb57d418b117b",
      "367dc0c8a741f20f6fffeccac2256851d2638092",
      "cadad4cf0fb930fba58a809f71e50caaa9202c7f",
      "19d9f5b3c5a09dc3f86748f57bfc0804f678da6a",
      "8ec079c1b17dbdc372ffe80ad53f0fffc444f427"
    ],
    "message_list": [
      "Add feature AGE\n\nThe AiiDA Graph Explorer (AGE) is a general purpose tool to perform\ngraph traversal of AiiDA graphs. It considers AiiDA nodes and groups\n(eventually even computers and users) as if they were both 'graph nodes'\nof an 'expanded graph', and generalizes the exploration of said graph.\nThe 'rules' that indicate how to traverse this graph are configured by\nusing generic querybuilder instances (i.e. with information about the\nconnections but without specific initial nodes/groups and without any\nprojections). The initial set of nodes/groups is provided directly to\nthe rule, which then will perform successive applications of the query,\neach on top of the results of the previous one. This cycle is repeated\nfor a specified number of time, which can be specified to be 'until no\nnew nodes are found'.\n\nThe current implementation works with the following (public) classes:\n\n* Basket: generic container class that can store sets of nodes, groups,\nnode-node edges (aiida links) and group-node edges. These are the\nobjects that the rule-objects receive and return.\n\n* UpdateRule: initialized with a querybuilder instance (and optionally a\nmax number of iterations and the option to track edges), it can then be\nrun with an initial set of nodes to obtain the result of the accumulated\ntraversal procedure described by the iterations of the query.\n\n* ReplaceRule: same as the update rule, except that at the end of the\nprocedure the returned basket contains not the accumulation of the\ntraversal steps but only the nodes obtained during the last step. This\nis rule is not compatible with the 'until no new nodes are found' end\niteration criteria.\n\n* RuleSequence: this can concatenate the application of different rules\n(it basically works like an UpdateRule that iterates over a chain of\nrules instead of a single querybuilder instance).\n\n* RuleSaveWalkers and RuleSetWalkers: rules that can be provided in a\nchain of rules given to a RuleSequence to save a given state of the\ncurrent basket (Save) that can later be used to overwrite the content\nof said working basket (Set). This is useful in the case where one might\nneed to do two operations 'in parallel' (i.e. on the same set of nodes)\ninstead of doing the second on the results of the first one.\n\nCo-Authored-By: ramirezfranciscof <ramirezfranciscof@users.noreply.github.com>",
      "Add feature traverse_graph and others\n\nThe function traverse_graph works as a simplified interface to interact\nwith the AGE that also removes the need to manually handle the basket\nand the querybuilder instance:\n\n * The price to pay for hiding the basket is that this function can only be\n   used with sets of nodes and links (so, no groups).\n\n * The price to pay for hiding the querybuilder is that complex traversal\n   procedures can no longer be specified, the user simply defines which\n   links can be traversed forwards and which backwards, and this criteria\n   is then applied in every iteration (so one could not, in a single call,\n   search only for all called calc nodes of the  called work nodes of an\n   initial workflow node, as one will also obtain the calc nodes directly\n   called by that initial workflow).\n\nBesides the starting nodes (pks) and links, the user can also provide the\nnumber of max iterations desired (which by default is None, which means\n'until no new nodes are found') and a boolean that indicates if the links\n(edges) should be returned.\n\nAdditionally, two other interfaces are included for ease of use when\ndeleting and exporting. These functions only take the starting set of\npks and the rules provided by the user (as 'rule_name_dir' = False/True)\nand can automatically check if the rule is toggable, set defaults (using\naiida.common.links.GraphTraversalRules), and also parse the ruleset into\ntwo lists with the links for forward and backward traversal. They will\nreturn a dictionary containing the 'nodes' list, the 'links' list (if\nthis was requested, else this will contain `None`) and a dict with the\nway in which all the rules were applied (using the following format:\n'rule_name' = True/False).\n\nCo-Authored-By: Leonid Kahle <leonid.kahle@epfl.ch>",
      "Add traverse_graph / AGE as engine for node delete\n\nThe node deletion function now uses the get_nodes_delete function (with\nthe traverse_graph underlying interface using AGE as main engine) to\ncollect the extra nodes that are needed to keep a consistent provenance.\nThe procedure is not very different than the one that was initially\nimplemented so no significant performance improvement is expected, but\nthis is an important first step to homogenize graph traversal throughout\nthe whole code.",
      "Add traverse_graph / AGE as engine for export\n\nThe export function now uses the get_nodes_delete function (with the\ntraverse_graph underlying interface using AGE as the main engine) to\ncollect the extra nodes that are needed to keep a consistent provenance.\nThis is performed, more specifically, by the 'retrieve_linked_nodes'\nfunction. Whereas previously a different query was performed for each\nnew node added in the previous query step, this new implementation\nshould do a single new query for all the nodes that were added in the\nprevious query step. So these changes are not only important as a first\nstep to homogenize graph traversal throughout the whole code: an\nimprovement in the export procedure is expected as well.",
      "Add traverse_graph / AGE engine for visualization\n\nThe graph visualization feature now uses the traverse_graph function\n(with AGE as the main engine) to collect the requested nodes to be\nvisualized. This was implemented in the methods of the graph class:\npreviously, `recurse_descendants` and `recurse_ancestors` used to\nwork by calling `add_incoming` and `add_outgoing` many times, which\nin turn have to load nodes during the procedure. Now these are all\nindependent and they all call the traverse_graph function, so the\ninformation is obtained directly from the query projections and no\nnodes are loaded. So these changes are not only important as a first\nstep to homogenize graph traversal throughout the whole code: an\nimprovement in the visualization procedure is expected as well."
    ]
  },
  {
    "pr_number": 4328,
    "commits_list": [
      "a26488f506099e0ddc6c9c5fc881b3e233184042"
    ],
    "message_list": [
      "`Group`: add support for setting extras\n\nAdd extras column to the database models for groups, with an empty\ndictionary as a default. Add the `ExtrasBackendEntity` mixin class\nto the `BackendGroup` to make sure the group backend classes for\ndjango and sqlalchemy have the extras methods. Add the\ncorresponding extras methods to the frontend `Group` class.\n\nFor SqlAlchemy: instead of using the constructor to set the default\nvalues of the attributes and extras for `DbNode` and `DbGroup`, use\nthe Column declaration to do so. Generate the required migration\nfiles and make adjustments where necessary.\n\nAlso fix some typos in the utils.py modules and add exception\nchaining for exceptions raised straight from the handling of a\nprevious exception."
    ]
  },
  {
    "pr_number": 3134,
    "commits_list": [
      "09c67a8059b95321150a573bf24ae7e1ed6ae53e",
      "52b944722ef8a238b52389acbfcd74f23aa2fd4a"
    ],
    "message_list": [
      "Add data migration for legacy process attributes\n\nAttribute keys that are renamed:\n\n  * `_sealed` -> `sealed`\n\nAttribute keys that are removed entirely:\n\n  * `_finished`\n  * `_failed`\n  * `_aborted`\n  * `_do_abort`\n\nThe `sealed` key is still used, the key has just changed, so existing\nattributes are migrated. The last four attributes are left over control\nattributes of one of the first process based engines, but are obsolete\nand are therefore deleted to clean up some space.\n\nFinally, any process nodes that still do not have a sealed attribute,\nget one, because these most likely correspond to legacy calculation\nnodes that were run before the sealed concept existed. However, any\nnodes that have a `process_state` that is active, are excluded as they\ncorrespond to actual active processes and are therefore not yet sealed.",
      "Up EXPORT_VERSION to 0.7\n\nAdded now illegal attributes (migration 0040) to process nodes in all\n\"migrate\" fixture export archives.\nMigration 0040 and the removal of 'Attribute' and 'Link' entities from\nmetadata.json and the import functions are the reason for the new\nEXPORT_VERSION.\n\nThe dependency on aiida-export-migration-tests has been upped to 0.7.0,\ntogether with the new release.\n\nDue to migration 0040, it is now mandatory for ProcessNodes to be sealed\nbefore they can be exported, hence this is checked in `export_tree`.\nBecause of this all tests including ProcessNodes has been updated."
    ]
  },
  {
    "pr_number": 4707,
    "commits_list": [
      "774968713bddb04038084b8dde32e7eb5d6327b0",
      "3ed7ca05fb133cfbb13812ad192cd12037a901c5",
      "42882b56f19bcb7eed75ffc3bf1dd1862e316157",
      "dd51a4d1fec7f1d50fdfa872b3e07e371ac14fbc",
      "b116c3420f3a0aeaee0bfba52ca18df4d5874b5a"
    ],
    "message_list": [
      "Allow setting `rerunnable` from `metadata['options']`.",
      "Add tests for `rerunnable` option in scheduler plugins.",
      "Add documentation for the `rerunnable` flag.",
      "Fix the direct scheduler test.",
      "Replace manual caplog handling with `aiida_caplog` fixture."
    ]
  },
  {
    "pr_number": 4277,
    "commits_list": [
      "b21b33f01be67723d9fa65fd60ba6a8ee264836f",
      "b29e9555130e99aa38304d331931d2e80f7ddea9"
    ],
    "message_list": [
      "REST API: Modify assumptions for `process_type`\n\nThe `process_type` attribute has changed over the years; currently it\nmust have some descriptor for processes and be None for data types.\nApparently this has not only been the case, and thus old databases may\nhave both data and process nodes with either empty strings ('') and/or\nNone entries in their `process_type` attributes.\n\nAdditionally, there were some problems with how the unregistered entry\npoints were considered that made it impossible to query for them.\n\nIn order to consider all of this when filtering and doing statistics,\nit has been decided to:\n\n1) Group all instances of a given node_type that have either '' or None\nas their process_type in the same `full_type` (`node_type|` ) and hence\nalways query for both when the `process_type` is missing.\n\n2) Remove the `aiida.descriptor:` and the `no-entry-point` from the\n`process_type` part of unregistered processes. This was interfeering\nwhen the `full_type` was given to return the filtering options to query\nfor these processes.\n\nTests were adapted to test this new compatibility aspects.",
      "REST API: Add full_types_count as new entry point\n\nThis feature returns a namespace tree of the available node types in the\ndatabase (data node_types + process process_types) with the addition of\na count at each leaf / branch. It also has the option of doing so for a\nsingle user, if the pk is provided as an option."
    ]
  },
  {
    "pr_number": 4767,
    "commits_list": [
      "ae6d7a89c398ebfa45a28cddf98ec90d2570d2c7",
      "1f702b0a09a1ea4f0a00396f62a67114091de0e3",
      "0e4a7f612a5c06e987505bd9e8c86e32e73c06b3"
    ],
    "message_list": [
      "\ud83d\udc4c IMPROVE: Garbage collect on process termination",
      "fix pre-commit",
      "increase async sleep to 1"
    ]
  },
  {
    "pr_number": 4869,
    "commits_list": [
      "40cb067505ca31010540fe9badabb210911891c7",
      "dc9ec6f91e0b23608c58ece6e3283a13780dcf47",
      "f0f72a8927f700fd7bee6f0a75ec0469db6e0ec2",
      "146e436965897efd9bfe5ecee38f3e8d495e5c08",
      "13f95caac724fbad1d268d0e7258e97a3bf0503b",
      "28ee0cea2224642b2e747f516204007d09de8096",
      "5ab86cca5fc2070937cfa90b1a479d06878e68dd",
      "2bb18e015c32cdb23e6aab2476e9b9a217194aa3",
      "f3c335965b301af97b2d91e50b67ae57d3c5cacc",
      "fa8b24379f5731cef43ca1648d18142d0c55de97",
      "5407799b602837252400f918118d328ac1d3fda0",
      "f1aab1fd5015e9b4d203845d57136fca2aed6281",
      "42455ac4319f8de399048e1f87acc0d99852130a",
      "c8ee9e605663fbabc9f7f2484fa0f88668a3b322",
      "7295bb64e8bf1f6c5a6dfa75dd3764c45b9a603a",
      "372f5cbf057dc22f26e4c200d64f1a3f2ffcd2cc",
      "fac7c75eeaec7f5f9a1ffd6bef6b200f117231af"
    ],
    "message_list": [
      "\ud83d\udd00 MERGE: `master` -> `develop` (#4844)",
      "\ud83d\udcda DOCS: Minor fix to Changelog (#4845)\n\n\ud83d\udcda DOCS: Minor fix to Changelog",
      "\ud83e\uddea TESTS: Bump `urllib3` 1.26.3 -> 1.26.4 (dependabot) (#4849)",
      "FIX: dependency conflict (#4851)\n\nThe following dependecies have been pinned:\r\n\r\n* Constrained jupyter-client to <6.1.13: to avoid conflict for nest-asyncio required by\r\nboth plumpy (dependency) and  jupyter-client (sub-dependency).\r\n\r\n* Constrained pytest-cov to <2.11: to avoid conflict for coverage required by us with\r\nconstrain <5 and pytest-cov.\r\n\r\nAdditionally, a utility function to analyze the package dependencies has been added\r\nas part of this fix for future use.",
      "DOCS: add help in intro for when quicksetup fails (#4838)\n\nSometimes AiiDA will have problems identifying the PostgreSQL configuration,\r\nand when this happens, `verdi quicksetup` will fail. In case the user runs into\r\nthis, the following additions have been made:\r\n\r\n* A section in the troubleshooting FAQ explaining the alternative procedure\r\n* A warning in the corresponding sections of the intro that points to the FAQ",
      "\ud83d\udc4c IMPROVE: Add `account` option to `LsfScheduler` (#4832)\n\n\r\nCo-authored-by: Chris Sewell <chrisj_sewell@hotmail.com>",
      "DOCS: Add \"How to extend workflows\" section (#4562)\n\nSplit the section on \"How to run multi-step workflows\" into one that focuses on running\r\nthe workflows and one on \"How to write and extend workflows\". Add a subsection on\r\nhow to extend workflows to the second.\r\n\r\nThis subsection continues with the `MultiplyAddWorkChain` example and covers:\r\n\r\n* How to submit the `MultiplyAddWorkChain` within a parent work chain.\r\n* How to expose the inputs using the `expose_inputs` method and a proper namespace.\r\n* How to use the exposed inputs with the `exposed_inputs` method.\r\n* How to expose outputs and pass them to the outputs of the parent work chain.",
      "CI: install default `postgresql` instead of specific version (#4860)\n\nCI: Use `postgres` since update to Ubuntu Focal 20.04\r\n\r\nThe CI configuration uses `ubuntu-latest` which recently became Ubuntu\r\nFocal Fossa (20.04) which no longer has `postgres-10` available, but\r\ninstead provides `postgres-12`.",
      "Revert \"CI: Notify slack on failure of the test-install workflow. (#4690)\" (#4862)\n\nThis reverts commit dcc80618368f405c02c9eaa6d122177e78d70a4b.\r\n\r\nThe slack integration didn't work as intended and I currently don't have the time to properly fix this.",
      "CLI: Use DB `proxy_command` for `verdi calcjob gotocomputer` (#4761)\n\nCurrently the `verdi calcjob gocomputer` command only works for\r\ncomputers accessed via a proxy in case:\r\n\r\n* the `Host` is set up in the `.ssh/config` file. \r\n* the `Host` is the same as the `HostName`, also configured as the host\r\n  name for the AiiDA computer.\r\n\r\nIf both conditions are met, the `verdi calcjob gocomputer` command\r\ngenerates an `ssh` command (via `SshTransport.gotocomputer_command()`)\r\nthat can rely on the configuration in the `.ssh/config` to use the proxy \r\ncorrectly, because the generated command is executed via `os.system()`\r\nand hence it parses the `.ssh/config` file. However, since typically users set\r\nan alias `Host` instead of using the full `HostName`, this will often not work.\r\n\r\nHere we add the `proxy_command` that has been configured for the\r\n`Computer` in the database to the ssh command string that is created for \r\n`verdi calcjob gotocomputer`. This fixes the `verdi calcjob gotocomputer`\r\ncommand for computers which are accessed by a proxy in case the above\r\nconditions are not met and the user has configured the `proxy_command`\r\nfor the `Computer`.",
      "DOCS: Update ssh proxycommand instructions (#4839)\n\nIt was not obvious how to handle cases where the SSH key for the proxy\r\nserver needs to be specified.\r\n\r\nAlso, remove the note regarding redirection.\r\nThis was relevant for the old approach of using netcat directly and is\r\nmore confusing than helpful without it.",
      "build(deps): bump django from 2.2.18 to 2.2.20 in /requirements (#4854)\n\nBumps [django](https://github.com/django/django) from 2.2.18 to 2.2.20.\r\n- [Release notes](https://github.com/django/django/releases)\r\n- [Commits](https://github.com/django/django/compare/2.2.18...2.2.20)\r\n\r\nSigned-off-by: dependabot[bot] <support@github.com>\r\n\r\nCo-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>",
      "FIX: respect nested output namespaces in `Process.exposed_outputs` (#4863)\n\nThe `exposed_outputs` method was detecting nested namespaces in the\r\ndictionary of outputs of a node by using the namespace separator\r\ncharacter of the port namespace class. However, this character, which is\r\nthe `.`, gets converted to `__` when the link is stored in the database.\r\nThis transformation is necessary since the `.` is a reserved character\r\nto enable attribute based dereferencing, e.g.\r\n\r\n    node.outputs.some_output\r\n\r\nSince link labels can contain underscores, we use a double underscore\r\nwhich, together with the guarantee that link labels don't have leading\r\nor terminating underscores, can uniquely determine the namespaces in any\r\ngiven link label.\r\n\r\nThe solution is to not manually reconstruct the namespaces in the output\r\ndictionary but simply use the `get_outgoing().nested()` method which\r\ndoes it for us.",
      "NodeLinkManager support dot separated namespace attributes retrieving (#4625)\n\nThe `NodeLinksManager` is used by the ``inputs`` and ``outputs``\r\nattributes of the ``ProcessNode`` class to allow users to quickly access\r\ninput and output nodes by their link label. Nested namespaces in link\r\nlabels are converted to double underscores before it is stored in the\r\ndatabase because it can only store a flat string. This is an\r\nimplementation detail, however, and the user should not have to know\r\nabout it and should be able to use the nested namespace. However, up\r\ntill now, one had to pass the link label as stored in the database, i.e.\r\n\r\n    node.inputs.nested__sub__namespace\r\n\r\nAfter this commit, it is now possible to use the more intuitive\r\n\r\n    node.inputs.nested.sub.namespace\r\n\r\nFor backwards compatibility, the old flat link is still supported but\r\nwill emit a deprecation warning.\r\n\r\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "Fix `aiida.cmdline.utils.decorators.with_dbenv` always loading the db (#4865)\n\nThe premise of the method `with_dbenv` is that it will load the profile\r\nand corresponding database environment if not already loaded. However,\r\ndue to a bug in `load_backend_if_not_loaded` it was actually *always*\r\nloading the environment for every call, even if the database env was\r\nalready loaded, leading to very expensive calls for nothing.",
      "Fix `IndexError` in `override_log_formatter_context` utility (#4873)\n\nIn the `aiida.common.log.override_log_formatter_context` utility\r\nfunction, the formatting of all current AiiDA handlers is temporarily\r\nchanged for the duration of the yield, and then reset. The problem is\r\nthat the function assumed that the number of handlers would not change,\r\nwhich is not necessarily the case. Additionally, it also relied on the\r\norder of the handlers, which isn't guaranteed either.\r\n\r\nThe solution is to simply cache the handlers' formatters as a dictionary\r\nand directly use those handler references to reset the cached formatter.\r\nNote that we copy the `handler.formatter` return value because it may be\r\nreturned by reference and we want to keep the original formatter if it\r\nis changed during the yield. Note also that it doesn't matter that the\r\nAiiDA logger may have gained additional handlers during the yield, after\r\nwe recorded the cached formatters, but that is ok, because we didn't\r\ntemporarily change their formatter either.",
      "Release `v1.6.2`"
    ]
  },
  {
    "pr_number": 5017,
    "commits_list": [
      "9537ef4df2f6155d7186f0b8adbc953e8d0d89e3"
    ],
    "message_list": [
      "Add the `JsonableData` data plugin\n\nThis data plugin is designed to make it easy to wrap instances of\narbitrary Python classes and store them as a node in the database. The\nonly requirement is that the class implements the `as_dict` method,\nwhich should return a dictionary that represents the instance which is\nJSON-serializable. The latter means that it can be serialized by the\n`JsonEncoder` of the `json` built-in module.\n\nIf the class also implements a `from_dict` method, that can consume the\ndictionary that is returned by `as_dict`, to reconstruct an instance,\nthe original instance that is wrapped by the node (loaded from the\ndatabase), can be recovered through the `obj` property."
    ]
  },
  {
    "pr_number": 4965,
    "commits_list": [
      "a0543a0974f715c3a9fc1e847c64a142d0c4715d",
      "3bfc85636314bb75cfcc9fef61be4188ae8485f9",
      "5ee2fa6bce869a095d6f21cf560fde1338964298",
      "2784998797531e4910fa86ce8852f55cbb7934d6",
      "766709db662d8bd04b5098a4d8202bf0d550efaf",
      "df3bfd41149eee1acc8225e6540c06ec4ee0cb5b",
      "5b5b38ed0404213bf13b61ba799a2e6b6613851d",
      "a4ec0d1e1f15ed46a8880f87172cfe82dc4195d9",
      "9fa51eaf212fd48c905fe62685bf33a962fd6bee",
      "fd071a996148cae6fe213f0c3bb8a802953403fd",
      "aa5908959a6ddbde52aaf09a7073045fdabb1de3",
      "2501cde0b25683491e0f4f126f8d510fbe5a606d",
      "fa7d5ffeca3a1341466680352fc9a22351d8cede",
      "8cb095109143cc708366f91e9359a2dfb567a6f8",
      "7a6a939ba3a81b4c10c3a5b45a108f1f0d936519",
      "2402bf7c217577335e7b97f698ba453388717ad4",
      "1432dcf28c44331f523d8efecd6b9bf3b7b20f00",
      "50376650efed4a6ba0aaca548cca6f8b43cd4feb",
      "f0aa923a0380dee1e45af5c04885300ac346a5a5"
    ],
    "message_list": [
      "Add proposal for repository maintain functionality\n\nIt would in principle consist of a single verdi command with one\noptional flag and one optional argument:\n\n  > verdi repository maintain [--live] [--pass-down <STR>]\n\nThe base command will perform all the available and necessary\nmaintainance operations but will require to lock down the profile\nin order to proceed (it will ask the user to confirm before).\n\nOptionally, the `--live` option can be passed so that only the\nmaintainance operations that are safe to perform while actively\nusing AiiDA will be applied.\n\nFinnally, the user can also `--pass-down` a backend specific string\nthat will be interpreted by the backend method for even further\ncustomization of the process.",
      "Apply PR corrections.",
      "Apply PR corrections - moving control from storage to backend",
      "Apply PR corrections after rebase",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "Add corrections from PR - method naming",
      "Apply PR corrections - Method defaults",
      "Fix test problems",
      "Modifications due to PR review\n\n - Added a logger for immediate info / feedback\n - Modified returned variables from the methods to be more\n   \"machine readable\" (dicts).",
      "Fixing auto generation",
      "Re-adapt cli tests",
      "Major re-structure of the features",
      "Merge branch 'develop' into repocli",
      "Fix control tests",
      "Apply suggestions from code review\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "Apply latest corrections from PR",
      "Last PR corrections",
      "Merge branch 'develop' into repocli",
      "Add mypy ignores"
    ]
  },
  {
    "pr_number": 3722,
    "commits_list": [
      "ed72892a20dbe1ccba55d583b1c3587d64442e01",
      "ecc19e0cdbe9a91ee5fb4b32889452d9e5515570",
      "7e599dc8eb564922882fd5b53bab34222c9d6f0d",
      "d0e076fc2812e57404bd785f0264a999fbf452fc",
      "c2178d1c5421a6eafadad271e57496bbfc6d2371",
      "062b7aa3acab3eac9e1474b4af236a6f629f0e17",
      "812c4cf57abbe5167633d5f561ac13ac0452a4b9",
      "a06e06b823ded577fd79818d381f8df9dbb3d21c",
      "5d8971b34d058f062a029c6a73ae24ed7a0a34b8",
      "6047d2a201c08fee12cc1be285cbe5fba2278efb"
    ],
    "message_list": [
      "Build docker image based on aiida-prerequisites",
      "Adress the comments by @ltalirz and @sphuber",
      "Replace verdi setup with verdi quicksetup",
      "Add docker action",
      "Fix typos and improve comments\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "trying to fix docker action",
      "Fix github actions\n\n* Use `wait-for-services` script to wait till all scripts in /etc/my_init.d executed.\n* Replace `verdi profile list` with `verdi provile show default`",
      "Update .github/workflows/ci.yml\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update .github/workflows/ci.yml\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "implement suggestions of @sphuber"
    ]
  },
  {
    "pr_number": 4301,
    "commits_list": [
      "e804d33682ba3b68c491896751ce0f3fb5d2e7fd",
      "6ed6dc7f02a738d6888df146da22ace2ce02fcd9",
      "726c7b4a66e91ff519460138f99788bc1e74f7b4",
      "0ff1ebfb95a51fce4a9e69b874c8978e55c0d2b5"
    ],
    "message_list": [
      "CI: Update test-install workflow to include Python 3.9.",
      "Add Python 3.9 to tox configuration.\n\nAnd add requirements file for Python 3.9.",
      "Add 'Python :: 3.9' classifier trove.",
      "Merge branch 'develop' into ci/python-39"
    ]
  },
  {
    "pr_number": 5142,
    "commits_list": [
      "ded9b56ab629a5c46a99e11dbf5e027fd2ed84e0",
      "2c8f7f33278dcad7f3033b50a7b08c9948be8eae"
    ],
    "message_list": [
      "`Dict`: implement the `__eq__` method to compare plain dictionary\n\nThe `__eq__` method is overridden to not only let a `Dict` node compare\ntrue to itself, but also to the plain dictionary that represents its\nvalue. That is to say, the following:\n\n    node = Dict(dict={'a': 1'})\n    assert node == node.value\n\nnow passes the assert. This brings the behavior of `Dict` on par with\nall the other base types, `Bool`, `Float`, `Int`, `Str` and `Bool`.",
      "`List`: register the class with the `to_aiida_type` serializer\n\nThis will allow users to pass a plain list as input for a port that\naccepts `List` nodes and specified the `to_aiida_type` dispatch as\nserializer. The value will be automatically converted to a `List` node.\nThis brings the functionality for `List` on par with the other base type\ndata node classes."
    ]
  },
  {
    "pr_number": 4832,
    "commits_list": [
      "73dac1b0ab0aedb20d73adc0c317e93d1edf0718",
      "2f86d6963fd13f79ef6f23fc6b89b346fa5dd104",
      "63d78eb63b8ebf04be74cf5bc3bf3141122c3cf8",
      "dce1de99d589ec66262b8ec20ab10556b465ef84",
      "d7bbafe830e64c74a33a6e6a6100213bcdb96632",
      "ba0c0b1a15d21f83d7b4802d39d0b57a70186882",
      "5047e4e444f3764c9066ee88572e8e494eece9de",
      "a6c542e4ddf5c90a8ec58fa7b8c1eaafe3802d19",
      "574639e9257075654cbed8302f0cd3b11734719a",
      "b213324f5be80a16c6e233c72283605d77ef9ad8",
      "dc219932f723281cf3b3b0ae5a1c55cfead4ba05"
    ],
    "message_list": [
      "`LsfScheduler`: add support for `account` option",
      "Add tests for lsf scheduler plugin",
      "Improve error messages.",
      "Add test + code comment for edge case",
      "Improve check on kwargs of resources",
      "Merge branch 'develop' into fix/lsf-scheduler-add-account-option",
      "Merge branch 'develop' into fix/lsf-scheduler-add-account-option",
      "try pinning pg8000",
      "revert change",
      "Merge branch 'develop' into fix/lsf-scheduler-add-account-option",
      "Merge branch 'develop' into fix/lsf-scheduler-add-account-option"
    ]
  },
  {
    "pr_number": 4922,
    "commits_list": [
      "e8370c94a89998b1bb874f199f4a8e797a022769",
      "7e0e7bc03142e9b62d7e9f0957eb15427a259745",
      "a91f1c875f3803b331dcea4d6dae81f22b109a46",
      "0826ed2a950a1c8908b4379d703ddabf5df4135b",
      "51e4dd55316f13edc614ea556be2f1d6bcc0e79f",
      "3da1e343ec34f52b9deb6a18ea9df75999f734ef",
      "2e65008ebede45df1a5b7b96134eecdd9de21db3",
      "30599c9855647252d20489a0982be0b40fdb91cb",
      "9bf95612a80e12ce0f2f1ead814bf2218cabd84a",
      "3aff8f4a395ab9ba8f4abc6fdce86e500848106f",
      "39cc793cc9e746b43f3b748f3b411efc47e759ca",
      "f28adb0549f2efdecd49f4170452fd9c5c7c01f8",
      "4da2a591ced6cb1adf140e57e04a2bd5ef02e96b",
      "9e6c07f1e874ab0b952393113288d302982ea401",
      "dcadcdbfb95f231216cc68efdf2ffc3657a31e9c",
      "2579d0431c73c6493c9605e7db98bf3b5dddc599"
    ],
    "message_list": [
      "\u2728 NEW: Add `Node.objects.iter_object_keys`",
      "fix pre-commit",
      "Update node.py",
      "Merge branch 'develop' into iter_object_keys",
      "Merge branch 'develop' into iter_object_keys",
      "Merge branch 'develop' into iter_object_keys",
      "Update aiida/orm/nodes/node.py",
      "update and add tests",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "add batch_size",
      "Merge branch 'develop' into iter_object_keys",
      "fix linting",
      "Merge branch 'develop' into iter_object_keys",
      "Apply suggestions from code review",
      "Apply suggestions from code review",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci"
    ]
  },
  {
    "pr_number": 3597,
    "commits_list": [
      "a67fc3e3301f0108dd1a6d8fca92b84220ef1c20",
      "6f320faa7d6836039ce29b6fe1fe7f13d0196bac",
      "69096b938cb2128575fe2634a3953daa3c0ab4c2",
      "ed11e58a640db9b1cff61f9854db07e4b5325a3a",
      "1a07adcee43f5eca1ac0642928a9b013b16b3380",
      "8885d215f920a124ce6eb1784f6ffa38c489fdc8"
    ],
    "message_list": [
      "Relax requirements of required and optional dependencies\n\nThe version requirements of all `install_requires` and `extras_require`\ndependencies are reevaluated. Where we used to require explicit version\nby pinning to exact versions using the `==` operator, we now use the more\nliberal operator `~=`. This is the compatible release operator:\n\n    https://www.python.org/dev/peps/pep-0440/#compatible-release\n\nFor a given release identifier `V.N` the compatible release clause is\napproximately equivalent to the pair of comparison clauses:\n\n    >= V.N, == V.*\n\nWith this rule we define our dependency requirements using the minor\nversion for packages with a major version of 1 or higher and using the\npatch version for packages whose major version is still at 0. The reason\nis that when packages have released `v1` they can be expected to respect\nsemantic versioning and not break backwards compatibility in minor\nversion whereas this is typically not the case for `v0` versions where\nbackward incompatible changes are introduced in minor versions as well.\nNote that this approach will not prevent from breaking our builds\nbecause packages break their API in minor versions, but at least we have\na consistent dependency requirement policy.\n\nList of specific restrictions and changes:\n\n * `pymatgen==2019.7.2` is the last to support python 3.5\n * `jinja2` moved to `install_requires` as it is used explicitly by `verdi`\n * `astroid==2.2.5` required by `prospector`, will cause crash if wrong\n\nExplicit lower patch requirements are set for a few packages because\nthey are known to contain critical and required bug fixes:\n\n * `psycopg-binary>=2.8.3`\n * `sqlalchemy>=1.3.10`\n * `pgtest>=1.3.1`\n * `seekpath>=1.9.3`\n\nFinally, an exception is made for packages of the `dev_precommit` extra\nwhich can affect pre-commit behavior such as linting warnings or code\nformatting. To prevent updates in these packages from unexpectedly\nfailing the pre-commit hooks, they are pinned to an explicit version.",
      "Remove unnecessary explicit dependency `mock`\n\nThis is a rolling backport of the module `unittest.mock` which has been\npart of the standard python library since v3.3.\n\nNote that one has to do `from unittest.mock import patch` and use it\ndirectly as for some reason `import unittest` followed by using\n`unittest.mock.patch` will result in an `AttributeError`.",
      "Remove unnecessary explicit dependency `uritools`\n\nIt was only used to parse the repository URI of a profile, which can be\naccomplished just as well with the built in `urllib.parse.urlparse`.",
      "Remove unused explicit dependency `passlib`",
      "Deprecate `verdi node tree` and related utilities\n\nThey will be removed in `aiida-core==2.0.0` which will allow to drop the\n`ete3` dependency. This CLI command was they only code using this lib.",
      "Update dependency requirement for Django to 2.2"
    ]
  },
  {
    "pr_number": 3906,
    "commits_list": [
      "e72a4a3f04970033871e721887cdfe8989946493",
      "8562f5e95a594b3f0f8edf27db4c9cbaf52a06bc"
    ],
    "message_list": [
      "Add infrastructure to parse scheduler output for `CalcJobs`\n\nAdd a new method `Scheduler.parse_output` that takes three arguments:\n`detailed_job_info`, `stdout` and `stderr`, which are the dictionary\nreturned by `Scheduler.get_detailed_job_info` and the content of\nscheduler stdout and stderr files from the repository, respectively.\n\nA scheduler plugin can implement this method to parse the content of\nthese data sources to detect standard scheduler problems such as node\nfailures and out of memory errors. If such an error is detected, the\nmethod can return an `ExitCode` that should be defined on the\ncalculation job class. The `CalcJob` base class already defines certain\nexit codes for common errors, such as an out of memory error.\n\nIf the detailed job info, stdout and stderr from the scheduler output\nare available after the job has been retrieved, and the scheduler plugin\nthat is used has implemented `parse_output`, it will be called by the\n`CalcJob.parse` method. If an exit code is returned, it is set on the\ncorresponding node and a warning is logged. Subsequently, the normal\noutput parser is called, if any was defined in the inputs, which can\nthen of course check the node for the presence of an exit code. It then\nhas the opportunity to parse the retrieved output files, if any, to try\nand determine a more specific error code, if applicable. Returning an\nexit code from the output parser will override the exit code set by the\nscheduler parser. This is why that exit code is also logged as a warning\nso that the information is not completely lost.\n\nThis choice does change the old behavior when an output parser would\nreturn `None` which would be interpreted as `ExitCode(0)`. However, now\nif the scheduler parser returned an exit code, it will not be overridden\nby the `None` of the output parser, which is then essentially ignored.\nThis is necessary, because otherwise, basic parsers that don't return\nanything even if an error might have occurred will always just override\nthe scheduler exit code, which is not desirable.",
      "Remove superfluous `ERROR_NO_RETRIEVED_FOLDER` from `CalcJob` subclasses\n\nThe `ERROR_NO_RETRIEVED_FOLDER` is now defined on the `CalcJob`\nbase class and the `CalcJob.parse` method already checks for the\npresence of the retrieved folder and return the exit code if it is\nmissing. This allows us to remove the similar exit codes that are\ncurrently defined on the calculation plugins shipped with `aiida-core`\n`ArithmeticAddCalculation` and `TemplateReplacerCalculation` as well as\nthe check for the presence of the `retrieved` output from the\ncorresponding parsers. The fact that is now checked in the `CalcJob`\nbase class means that `Parser` implementations can assume safely that\nthe retrieved output node exists."
    ]
  },
  {
    "pr_number": 5197,
    "commits_list": [
      "62bda3a61cccc1a6b3426cf0e6f6d82a1cb6c799",
      "9d9b556e53534e0765b2d7bad35864bdc5183a0b",
      "680c2d3d27d603b8477759649f6c27fdde868770"
    ],
    "message_list": [
      "caching: order and use latest node when cache more than one",
      "doc: add notes for order caching nodes",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci"
    ]
  },
  {
    "pr_number": 4194,
    "commits_list": [
      "b31d6dc5198706d4149dc02ca1b39e76f8239b7c",
      "cea90020a80f30c599bd6440b30204365b53b7d9",
      "d615f44b86b042f965af947e2d53466ecf2b8ad9",
      "f515dae072a4a21b95b59c5bb7762285fce7fef1",
      "5c8f97ba02ffca4a37bc8100a1e3d2614f90b7e5",
      "6524743b8aa5e0ce528ccf06dda98d6d5f7961dd",
      "65f0872a89047ae08773a8603c4fe370a0aa2123",
      "a71d929e03cf00f7ccb9d7c34c861c2bec0a3e1c",
      "01eee1671cf0e562933ee310e69386d64d0f2853",
      "00e53ed680bd0f9ca622880439c8f460191c3d5f",
      "caeecb82812b8a0e3a40678013e64adfd5fa91a5"
    ],
    "message_list": [
      "Add transfer calcjob\n\nThis calcjob allows the user to copy files between a remote machine and\nthe local machine running AiiDA. More specifically, it can do any of the\nfollowing:\n\n* Take any number of files from any number of RemoteData folders in\na remote machine and copy them in the local repository of a single\nnewly created FolderData node.\n\n* Take any number of files from any number of FolderData nodes in the\nlocal machine and copy them in a single newly created RemoteData folder\nin a given remote machine.\n\nThese are the main two use cases, but there are also other more complex\ncombinations allowed by the current implementation.\nPlease check the documentation for more details.",
      "Add tests for transfer calculation.",
      "Add skip_submit option to calc_info\n\nThis is used by the process handler to decide wether to do the normal\nsequence of steps (upload, submit, update, retrieve) or just skip the\nmiddle two and immediately retrieve after uploading.",
      "Add integration test to test skip of submit step\n\nI am using TransferData as a way to check that the whole process of\nsetting and executing the run steps works correctly when using the\nCalcInfo setup to skip the submit/update steps.",
      "Initial PR corrections.",
      "Remove code from TransferCalcjob",
      "Add documentation on TransferCalculation",
      "Fix exception in docs class reference",
      "Doc fix",
      "Apply suggestions from code review\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "Add test for validators + other pr corrections"
    ]
  },
  {
    "pr_number": 5320,
    "commits_list": [
      "fe28f48816128b96305223561ac7153861fd4c4a",
      "297da50a21a6626e62d76c0465fbc769dbcb7b1a",
      "05aa2d16b1c9af35778e923744e0c43b1e8efb2b"
    ],
    "message_list": [
      "\u267b\ufe0f REFACTOR: Profile storage backend configuration",
      "\u267b\ufe0f REFACTOR: Profile rabbitmq configuration",
      "\u267b\ufe0f REFACTOR: Profile configuration top-level keys"
    ]
  },
  {
    "pr_number": 2420,
    "commits_list": [
      "eb7a5b555dbd11fb7df7dbbbd8f5fc6dabf18012",
      "6b5c8ff34f0ef1d95bae5939b2cb7726151b9a3a",
      "06e97ca168dbecf5be27f9f04834b79b7fac3397",
      "c4d9724fcd0d57536e244a52aa33cca86603fa42",
      "65e3728bd749150e43328d06fc4c7104587cb2cd",
      "ca81f9c410e5270363461ecb87c608f1ee0dad9b",
      "636cd2596f05bc413e34353800640ad86120106c",
      "6a384193b73b3002cddb8783e0c2757d9cfbe7eb",
      "262e418f4c7bfe7a62d2c097e64f07536aa493a5",
      "5e8d411ecff3b0e5556caa44d66a5b0dbe7bcab6",
      "381e9d0c7f87d84cf011d599a06cd69419768361",
      "e7d6630a2f6c3b9b36a3ba6a4fc9976b72e6d530",
      "d7e29530acb40065a7951b21277672c655c1c8b7",
      "7a667b75d4df5f9de17e6cee7f1e928d5497372f",
      "5fa4594c4017af091dedd5ce42c1af2b33d3f76a",
      "7df750da93414a9ec7bc150008c5cc7ae25c2749",
      "3705e5e2e75215e95481efd9ccb361578210daa7",
      "37c83f1dacc78e51479b9579598efa485c0fe411",
      "88d9126610bea7d9f0692552f88b54e550f4c2a1",
      "bcf2b4c751ecf68cef8f2dd05c469a7eed262481",
      "5cbce6c750297d9f8cdb88e51dfc92dad8626783"
    ],
    "message_list": [
      "add instructions when verdi import fails\n\nwhen verdi import fails because of an old/new export file version,\nadd a hint on what to do",
      "add fastentrypoints to build requirements",
      "try fixing pip issue",
      "missing flag for regular install",
      "update pre-commit to fix ruby issue\n\ndiscussed in #2362",
      "try bumping numpy version",
      "downgrade numpy because of 1.16 issue\n\nsee https://github.com/numpy/numpy/issues/12749",
      "try upgrading pymatgen",
      "go back to 2018.12.12 for py2 support",
      "trying cython version\n\ntrying cython>=0.29 from here:\nhttps://github.com/numpy/numpy/issues/12785",
      "revert back to pip 18.1",
      "try reverting other packages as well",
      "try fixing scipy version",
      "try adding numpy to build requirements",
      "try uninstalling numpy",
      "add --yes",
      "travis: add pip freeze + verbose flag",
      "tone down verbosity for travis",
      "new numpy version",
      "cleanup",
      "add missing exception"
    ]
  },
  {
    "pr_number": 4699,
    "commits_list": [
      "a216b1959e53a820df5454c80b80a609ba200f38",
      "a27aa944742cdcadef585035a22cdeaae8769b71",
      "69aa6b9cbcb2f72319f9d24951cb89f673141ce1",
      "20d43c952221f11779a5f26bf4cafad4c7a897bb",
      "cda8e5cdaacfdad12417d01ee8c4ba5e30ae2963",
      "99bb97c004c0d83dfa2993c78c70248af15bfe9e",
      "9842fc5a7904a4ddec5b83a51e8a3152ed2d61b4",
      "e27e802dac59a98bbddc5138b476938043ce600e",
      "df7af0fa8973199f31e8ec289cb4c8b8e63a7316",
      "b9ce49653ec5c5df944b3c061594258b31f5c5ed",
      "05750be255ccf834803d5695c154e34ff19cb04f",
      "fdd6b24bfc2594732b7dddbb27cd3e3f875057e2",
      "a1cf65e139c5ed38d52056958a7f87646a554a5a",
      "4d90829e8a46443a952f2905e7d9250f0968c669",
      "3e51b5631a585323f122753bb69325251427cbb7",
      "04d19d2070bbe57f75cebcd14affa3b8c477ed78"
    ],
    "message_list": [
      "memory leaks: don't pass process stack via context",
      "add test against memory leaks in engine.run\n\nTest that no refernce to AiiDA processes remains in memory after all of\nthem have exited.",
      "add pympler dependency also on CI",
      "refactor test to get rid of local variables",
      "try freeing results dict",
      "try adding more empty contexts...",
      "add memory leak test in test suite",
      "Merge branch 'develop' into issue_4698_context_var",
      "fix memory leak in test suite",
      "try fixing daemon tests",
      "move test to .ci",
      "Merge branch 'develop' into issue_4698_context_var",
      "fix mypy",
      "Merge branch 'develop' into issue_4698_context_var",
      "Merge branch 'develop' into issue_4698_context_var",
      "Merge branch 'develop' into issue_4698_context_var"
    ]
  },
  {
    "pr_number": 4357,
    "commits_list": [
      "9b4eb1711109690c70a760c6213f09946f4b867d",
      "45ed3b27d951b9a079a527fb130f31a44e3b39e0",
      "1d061a638cb060339f8b744e9eeb412cb34f46b2",
      "2830206176d82265ebdb9471f9019b82abfb98f7"
    ],
    "message_list": [
      "docs: clarify `verdi node delete` docstring",
      "remove unnecessary Group.clear()",
      "deprecate --clear flag and remove any functionality",
      "Merge branch 'develop' into issue_4356_group_delete_docstring"
    ]
  },
  {
    "pr_number": 5331,
    "commits_list": [
      "4e76e515bc19ff819f824d6ebe4bc8c0f7f62aa5",
      "c98fb5c629fdd41ff5932cbf4154c22778803999",
      "98f6bc41671bc6c78f04f3a1fe290fe0a02485d1",
      "d98e3014eb96dbb2674bc30c6cfbf9b5fa9136d4",
      "1b0c41315ed8b74f7cbe8c5696be74becbd2d03d"
    ],
    "message_list": [
      "Improve storage maintain related methods\n\nMoved the general purpose functions into methods of the storage class\n\nUse the lock mechanism for the maintain operation",
      "Apply feedback from PR review",
      "Merge branch 'develop' into incorporate_lock",
      "Generalize the get_info method.",
      "Final details"
    ]
  },
  {
    "pr_number": 4060,
    "commits_list": [
      "34d7f13b87fd0dc48c9f5a10d193231b7fc34419",
      "ed13abc5c8496af8d8dcb8ec64441ea80c65af42",
      "e9062c5b6e826f5111190159fb21995e808162bb",
      "18d62266433cfa4dd3c3e5682f3d4c270476eab1",
      "ccc90918b8d9c1456005c9fa853825a79abfe095",
      "c22979c78a5433f850ce8013f56f38e26fad2ec3",
      "081a94a5593624aeb646572a201de39a34c853bb",
      "db673cb55922899b8d057b45ed25cb1c6c4d2ec6",
      "f88ce4aa5b477d5c7a9befe27db6da4c0f8cc874",
      "4bd06e4cf9fabe969dd9743915b56128a52627a4",
      "5903f9e755582e54d0da89feac74eda0d96734d0",
      "160e49539fd4641a6e1f6d2dcdf44dca0b022f75",
      "9befbc9b82d64f8ab3a9418543465c2f6cfb4c12",
      "8766481566e6a68bf52c376284f68517703bfe20",
      "a53f5b54174e7ce3bb6621dd9fbf965e2c514fe1",
      "ec576b4f036cf00af2b926f23e2e24527b4856c6",
      "1e0869f8c4b13304b88682a9afd70a29605caa86",
      "179beda373b41ba481f279221ebec23b16e9a93a",
      "191d122b98b2c57f954eb57c20bb85cf6f29a15d",
      "3ed97b92e8abd83891c50e18b755666beef8b53b",
      "bfd98e35cc188e2463cb1b1c91e9a76e5de66a08",
      "80e1dd1ab1d1ae8be7d68fc03fb1252e4ce70a1d",
      "b83385cdb1bb7b1e1bb86976a9ea28f95a68f994",
      "aa3e59f4f01dae4aa4f0f901e195824d2872e2e2",
      "7dad928be7ae71db9621a8e7613dd874b9a47af5",
      "43aebd7fd7dba777538f411b774a03e849e352d0",
      "bf42e53c8cd0fdb57eafc880ce55c65b3c3e9b96",
      "39b344504fc213ef14a82240233dd40a694722d3",
      "ec3120055c91be4ffaf49a1ac0f7c75ca7c8d6fb",
      "11176819d4a49a0b7ef21ec45b6089f43ebcc327"
    ],
    "message_list": [
      "Update dpendency sphinx-panels=0.2",
      "Update install instructions",
      "Update sphinx-panels dependency",
      "Major update",
      "circleci fix",
      "circleci fix",
      "Update config.yml",
      "Update config.yml",
      "Update config.yml",
      "Finish docker section",
      "Add updating AiiDA sections",
      "typos",
      "Move other sections",
      "Fix pre-commit",
      "fixes",
      "fix title",
      "code block fixes",
      "Add further reading links",
      "Add sphinx-copybutton",
      "Add `copybutton_prompt_text`",
      "Update get_started.rst",
      "Add front page links",
      "Update docs/source/index.rst\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Update docs/source/intro/get_started.rst\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Update docs/source/intro/configuration.rst\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Update docs/source/intro/get_started.rst\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Installation -> Detailed Installation",
      "Update config.yml",
      "Merge branch 'docs-revamp' into docs-revamp-intro-install2",
      "Address some review comments"
    ]
  },
  {
    "pr_number": 3650,
    "commits_list": [
      "374543fc3517520d2527719d49e21422687e4745",
      "43d852ebf0048b0a623061d4b22007ce9dd3a576",
      "ba4b2174d1a08348eb914492d4f7c8714dae478a",
      "31d0470543c2213bd0ca0dbd99cf92b49e09c100",
      "a63cc52b67b503e54a2283d9d699a7551750987e",
      "9a681009d2d3095fc54c0f25aa0d129e7ab97d7f"
    ],
    "message_list": [
      "Clean-up of the autogroup logic\n\nIn particular:\n\n- deprecated use of 'group-name' in favour of 'group-label'\n  both in the internal API of AutoGroups and in the CLI parameters\n  of `verdi run`\n- Improved the Autogroup implementation and API\n- add various tests of the functionality of autogroups\n- fixed the CLI parameters to include/exclude groups, that were badly\n  broken (and untested). Now it accepts any entrypoint string (before\n  it was accepting the first part of an node_type string). Also added\n  click validation of the parameters.\n- fixed the --group flag to activate/deactivate autogrouping (it was\n  wrongly defined and thus always True)\n- linter fixes",
      "Centralised the creation of the Autogroup\n\nThis also remove an overzelous isinstance check, and\nmoves additional checks in a cached function that is run only when\nstoring the very first node (that needs to be put in an autogroup),\nmaking storing of nodes faster (even if times oscillates so it's hard\nto estimate exactly by how much).\n\nAlso, added logic to allow for concurrent creation of multiple groups\n(and test). This fixes #997",
      "Addressed comments by Sebastiaan.\n\nIn particular, the most important changes include:\n- now the auto-group flag is called `--auto-group`, is\n  a flag and is False by default\n- only kept `--include` and `--exclude` options, checking\n  typestrings and allowing to end with `%`. One benefit\n  is that while reimplementing I replaced some `isinstance`\n  with string comparisons, with potential further benefits.\n- `--include` and `--exclude` are now mutually exclusive\n- improved documentation of the main point of the issue",
      "A few further comments addressed.\n\nMost notable: now the % sign can be anywhere in the string\nof included or excluded classes, and not only at the end.",
      "Merge branch 'develop' into fix_997_parallel_autogroups",
      "Minor pre-commit fixes after merge."
    ]
  },
  {
    "pr_number": 5804,
    "commits_list": [
      "19cf0a662342f8683106a4c89c50e1845554f0e5",
      "192bcc62441868655f8ad7fb37286ccab695b968",
      "a19d76c7b7c9a973d6fb8bdde79e61c4956d658c",
      "29a30670eefaf548d3a38ec034597bfe2f6aa97f",
      "24921527b1fe7ca23e90fda9d03a1f7bbcddd952"
    ],
    "message_list": [
      "`PsqlDosBackend`: Use transaction whenever mutating session state\n\nStoring a node while iterating over the result of `QueryBuilder.iterall`\nwould raise `sqlalchemy.exc.InvalidRequestError` with the message:\n\n    Can't operate on closed transaction inside context manager.\n\nThe problem was that the `Node` implementation for the `PsqlDosBackend`,\nthe `SqlaNode` class, would not consistently open a transaction, using\nthe `PsqlDosBackend.transaction` method, whenever it mutated the state\nof the session, and would then straight up commit to the current session.\nFor example, when storing a new node, the `store` method would simply\ncall save. Through the `ModelWrapper`, this would call commit on the\nsession, but that was the session being used for the iteration.\n\nThe same problem was present for the `SqlaGroup` implementation that had\na number of places where sessions state was mutated without opening a\ntransaction first.\n\nThe problem is fixed therefore by consistently opening a transaction\nbefore making changes to the session. The `transaction` implementation\nis slightly changed as any `SqlaIntegrityError` raised during the context\nis now converted into an `aiida.common.exceptions.IntegrityError` to make\nit backend independent.",
      "`SqliteTempBackend`: Fix the `transaction` method\n\nWhen trying to store nodes, for example by running a calcfunction,\nSqlAlchemy would throw an exception complaining that the transaction had\nalready been closed and the context manager had to be closed first. The\nimplementation is fixed by matching it with that of the `PsqlDosBackend`\nstorage backend.\n\nThe error was only discovered through the execution of the `tutorial.md`\nnotebook in the documentation. An explicit unit test is added to make it\nmore obvious and easier to debug in case a regression is introduced.",
      "Add test for `SqlaGroup.add_nodes` to test `disable_expire_on_commit`\n\nThe test passes, but it seems independent of the use of the utility\n`disable_expire_on_commit`. It looks like this is because now a\ntransaction is properly used to add or remove nodes, and so the\ninstances in the parent session are not expired. It seems that the\nproper use of transactions essentially makes `disable_expire_on_commit`\nobsolete. When the same test is run on `v2.1`, where transactions were\nnot properly used, removing the context manager fails the test.",
      "`PsqlDosBackend`: Remove `disable_expire_on_commit` utility\n\nThe implementation of the backend `Group` ORM class for the\n`PsqlDosBackend` used the `disable_expire_on_commit` context manager in\nthe `add_nodes` and `remove_nodes` methods. By default, the engine used\nsets `expire_on_commit=True`, which means that all database model\ninstances in the session are expired once the session is committed,\nwhich forces the instances to be refreshed from the database the next\ntime they are accessed.\n\nThis is a sensible and important setting, as it ensures that we are in\nsync with the database when multiple processes are committing to the\ndatabase. However, expiring instances comes at a cost because they will\nhave to be refreshed at some point. It is unknown exactly why his change\nwas originally applied, but it is probably because expiring all nodes in\nthe session upon addition/removal of nodes from a group affected\nperformance badly and the probability of inconsistencies by omitting it\nare less problematic.\n\nThe behavior was never tested though and with the recent refactor of the\ntransaction control in the `PsqlDosBackend` it seems the utility to\nexplicitly disable the expire on commit no longer seems necessary. In\nthe previous commit a test was added to test the utility correctly being\napplied, but it works whether the utility is used or not at all. This\nsuggests that with the current proper use of a nested transaction in\n`add_nodes` and `remove_nodes`, the nodes in the outer session are no\nlonger expired, making the utility obsolete and therefore it is removed.",
      "Docs: Fix the build on ReadTheDocs\n\nThere was a backtick missing in the `help` attribute of an input port\ndeclaration of the `CalcJob` class. This class is rendered automatically\nthrough our sphinx plugin in the `topics/calculations/usage.rst` file in\nthe \"Options\" section. No idea why this didn't fail before this because\nthis mistake was not introduced in this PR.\n\nAlso add config option for `myst-nb` to `conf.py` so that it shows the\nstacktrace if an exception is raised during notebook execution."
    ]
  },
  {
    "pr_number": 4516,
    "commits_list": [
      "559abbaab690bc7f94c84ece63ad4810500592bf",
      "01845181740c2768ce3c31165a3f80e18d241a9f",
      "5e1c6fd965bc8cdeea8bc0c37ee19a71de5986f3",
      "dac81560647b2ffaa170ee87f673bd9f89db2b41",
      "ff30ebdb8860dc69bcbfec5e7a19e8b6e15a4f42",
      "1310abaa7f765866f636c9af2d3332e3eaf74ced",
      "e2b5385044076f135e5b769aa8fd24f7950738f5",
      "65ad067b18cffeb639994efe9a372ec1475e1615",
      "91449241ff2e12dd836b29882e32201cc7841716",
      "af91a8b10f2fe68360483a951d5c578863d38b76",
      "4544bc49a50c9aa3abebd2837efb5626958ee2b4",
      "02248cf3686a0ab89faf1625e0da24d9e33d8cde",
      "29331b558b45ba74acf1ca633a2d8bfabc1bdd05",
      "1e1bdf2dee779970654ed9b22eb996b78e9c4149",
      "bd6903d88a4d88077150763574784a1bc375c644",
      "16bc30548f7f1c686d200935174533535e850fd5",
      "e840ab62615bf18dbfbd24bf9b821489f2adc37b",
      "0fb6f72333b0e553590326fc348acbee3ef0763b",
      "4791046a58a1dfd0948f02cea6cf4b13eb1be4a5",
      "9ceb7b3b6a133227f8b1b46bd34ce41f3eb1357c",
      "2174924e40c3c9f00263f2e892e63a07eb90b40f",
      "eed191785da51b3c8ae105ee2073e3330deb0790",
      "db659ddf8a36db77963e2955df5066876c0d8017",
      "6202fac632f2e9c5c440b34695effc94f78e9492",
      "cda695007feca41c01c93bc0936e5463b3aa73dd",
      "5e0de12cd21d18da1e57c87f8f5e0130edda508d",
      "ce65988b7a90f0245467dae626d9f695181b1dba",
      "e63fdebb9d13a203adc23920fb036234baed6942",
      "cb268d1dd90a2aaeb94e7fd0ddc4091a6bf9ebc9",
      "58eb4d3a395294589e18a2cceb3a99415256e757"
    ],
    "message_list": [
      "Drop support for Python 3.5 (#4386)\n\nPython 3.5 is EOL as of September 13 2020. CI testing will now only be\r\ndone against Python 3.6 and 3.8.",
      "`LinkManager`: fix inaccuracy in exception message for non-existent link  (#4388)\n\nThe link manager was always referring to an 'input link' while it should\r\ninstead refer on an 'input link label' or 'output link label' depending\r\non the value of the link direction, determined by the `self._incoming`\r\nattribute.",
      "Implement `next` and `iter` for the `Node.open` deprecation wrapper (#4399)\n\nThe return value of `Node.open` was wrapped in `WarnWhenNotEntered` in\r\n`aiida-core==1.4.0` in order to warn users that use the method without a\r\ncontext manager, which will start to raise in v2.0. Unfortunately, the\r\nraising came a little early as the wrapper does not implement the\r\n`__iter__` and `__next__` methods, which can be called by clients.\r\n\r\nAn example is `numpy.getfromtxt` which will notice the return value of\r\n`Node.open` is filelike and so will wrap it in `iter`. Without the\r\ncurrent fix, this raises a `TypeError`. The proper fix would be to\r\nforward all magic methods to the wrapped filelike object, but it is not\r\nclear how to do this.",
      "Dependencies: increase minimum version requirement `plumpy~=0.15.1` (#4398)\n\nThe patch release of `plumpy` comes with a simple fix that will prevent\r\nthe printing of many warnings when running processes. So although not\r\ncritical, it does improve user experience.",
      "`verdi setup`: forward broker defaults to interactive mode (#4405)\n\nThe options for the message broker configuration do define defaults,\r\nhowever, the interactive clones for `verdi setup`, which are defined in\r\n`aiida.cmdline.params.options.commands.setup` override the default with\r\nthe `contextual_default` which sets an empty default, unless it is taken\r\nfrom an existing profile. The result is that for new profiles, the\r\nbroker options do not specify a default, even though for most usecases\r\nthe defaults will be required. After the changes of this commit, the\r\nprompt of `verdi setup` will provide a default for all broker parameters\r\nso most users will simply have to press enter each time.",
      "`verdi setup`: improve validation and help string of broker virtual host (#4408)\n\nThe help string of the `--broker-virtual-host` option of `verdi setup`\r\nincorrectly said that forward slashes have to be escaped but this is not\r\ntrue. The code will escape any characters necessary when constructing\r\nthe URL to connect to RabbitMQ. On top of that, slashes would fail the\r\nvalidation outright, even though these are common in virtual hosts. For\r\nexample the virtual host always starts with a leading forward slash, but\r\nour validation would reject it. Also the leading slash will be added by\r\nthe code and so does not have to be used in the setup phase. The help\r\nstring and the documentation now reflect this.\r\n\r\nThe exacti naming rules for virtual hosts, imposed by RabbitMQ or other\r\nimplemenatations of the AMQP protocol, are not fully clear. But instead\r\nof putting an explicit validation on AiiDA's side and running the risk\r\nthat we incorrectly reject valid virtual host names, we simply accept\r\nall strings. In any case, any non-default virtual host will have to be\r\ncreated through RabbitMQ's control interface, which will perform the\r\nvalidation itself.",
      "Merge branch 'master' of github.com:aiidateam/aiida-core into develop\n\nMerge after release of `v1.4.0`.",
      "CI: move `pylint` configuration to `pyproject.toml` (#4411)\n\nThis is supported by `pylint` as of v2.5.",
      "`verdi process show`: order called by ctime and use process label (#4407)\n\nThe command was showing the called subprocesses in a random order and\r\nused the node type, which is often uninformative. For example, all\r\nworkchains are always shown as `WorkChainNode`. By using the process\r\nlabel instead, which is more specific, and ordering the called nodes by\r\ncreation time, the list gives a more natural overview of the order in\r\nwhich the subprocesses were called.",
      "Dependencies: update requirement `pytest~=6.0` and use `pyproject.toml` (#4410)\n\nStarting from v6.0, `pytest` supports using the `pyproject.toml` instead\r\nof a `pytest.ini` to define its configuration. Given that this is\r\nquickly becoming the Python packaging standard and allows us to reduce\r\nthe number of configuration files in the top level of the repository, we\r\nincrease the version requirement of `pytest`.\r\n\r\nNote that we also require `pytest-rerunfailures>=9.1.1` because lower\r\nversions are broken in combination with `pytest==6.1`. See the following:\r\n\r\n   https://github.com/pytest-dev/pytest-rerunfailures/issues/128\r\n\r\nfor details.",
      "CI: add coverage patch threshold to prevent false positives (#4413)\n\nThe project diff percentage is the change in coverage w.r.t. all lines\r\nin the project, whereas the patch diff percentage is the change in\r\ncoverage w.r.t. only lines touched by the PR. The patch threshold is\r\ncurrently defaulting to 0%, hence it is very easy to fail. By raising it\r\nto 0.1% it should now only fail when there is a significant reduction\r\nin coverage. Number may need to be further tweaked.",
      "Replace old format string interpolation with f-strings (#4400)\n\nSince Python 3.5 is no longer supported, format string interpolations\r\ncan now be replaced by f-strings, introduced in Python 3.6, which are\r\nmore readable, require less characters and are more efficient.\r\n\r\nNote that `pylint` issues a warning when using f-strings for log\r\nmessages, just as it does for format interpolated strings. The reasoning\r\nis that this is slightly inefficient as the strings are always\r\ninterpolated even if the log is discarded, but also by not passing the\r\nformatting parameters as arguments, the available metadata is reduced.\r\nI feel these inefficiencies are premature optimizations as they are\r\nreally minimal and don't weigh up against the improved readability and\r\nmaintainability of using f-strings. That is why the `pylint` config is\r\nupdate to ignore the warning `logging-fstring-interpolation` which\r\nreplaces `logging-format-interpolation` that was ignored before.\r\n\r\nThe majority of the conversions were done automatically with the linting\r\ntool `flynt` which is also added as a pre-commit hook. It is added\r\nbefore the `yapf` step because since `flynt` will touch formatting,\r\n`yapf` will then get a chance to check it.",
      "CI: use `-e` install for tox + add docker-compose for isolated RabbitMQ (#4375)\n\n* Using `pip install -e .` for tox runs improves startup time for tests\r\n   by preventing unnecessary copy of files.\r\n\r\n* The docker-compose yml file allows to set up an isolated RabbitMQ\r\n   instance for local CI testing.",
      "Merge remote-tracking branch 'origin/master' into develop",
      "`ProcessBuilder`: allow unsetting of inputs through attribute deletion (#4419)\n\nThe builder object was already able to delete set inputs through the\r\n`__delitem__` method, but `__delattr__` was not implemented causing\r\n`del builder.input_name` to raise. This is not consistent with how these\r\ninputs can be set or accessed as both `__getattr__` and `__setattr__`\r\nare implemented. Implementing `__delattr__` brings the implementation\r\nup to par for all attribute methods.",
      "`CalcJob`: support nested directories in target of `remote_copy/symlink_list` (#4416)\n\nThe `upload_calculation` transport task would fail if either the\r\n`remote_copy_list` or `remote_symlink_list` contained a target filepath\r\nthat had a nested directory that did not exist yet in the remote working\r\ndirectory. Instead of inspecting the file system or creating the folders\r\nremotely each time a nested target path is encountered, which would incur\r\na potentially expensive operation over the transport each time, the\r\ndirectory hierarchy is first created in the local sandbox folder before\r\nit is copied recursively to the remote in a single shot.",
      "Docs: link to packaging plugins howto (#4439)\n\nThe how to on supporting external codes in AiiDA did not link to the\r\npackaging guide. While the how to is great for learning how AiiDA works,\r\ndevelopers who want to get started as quickly as possible are better off\r\nusing the AiiDA plugin cutter.",
      "Docs: Add docs live build to tox configuration (#4460)\n\nAdd docs live build using sphinx-autobuild. This dramatically speeds up the process of checking the rendered documentation while editing.\r\n\r\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "Docs: Add redirects from old documenation (#4457)\n\nUses the `sphinxext-rediraffe` Sphinx extension to automatically create\r\nredirects when documentation pages are moved and therefore their URLs\r\nchange. New redirect rules should be added to `docs/source/redirects.txt`",
      "Docs: add \"How to install plugins\" section (#4468)\n\nThere was no centralized location yet that explains how a plugin package\r\nis installed, yet this is one of the most critical first steps for new\r\nAiiDA users. A new how-to section is created \"How to install plugins\"\r\nthat details this information. Since a file `plugins.rst` already\r\nexisted, which contains information on how to develop a plugin package,\r\nit is renamed to `plugins_develop.rst`.",
      "Docs: add the \"Transport plugins\" topic section (#4465)\n\nContent is mostly moved from older existing documentation.",
      "Docs: \u2b06\ufe0f Update sphinx + extensions versions (#4470)\n\nThis commit primarily upgrades the sphinx dependency from sphinx v2 to v3, allowing for other upgrades of sphinx version pinning.\r\n\r\nIt also moved the `aiida/sphinxext` testing to the official sphinx testing infrastructure, and fixes an issue with the automodule writer. However, the automodule functionality cannot yet be re-instated, due to issues with referencing of these objects.",
      "REST API: list endpoints at base URL (#4412)\n\nThe base URL of the REST API was returning a 404 invalid URL response\r\nwithout providing any guidance to new users as to how to use the API.\r\nWe change this to return the list of endpoints formerly available only\r\nunder /server/endpoints.\r\n\r\nDocumentation of where to find the list of endpoints -- which seems\r\nto have been entirely deleted -- is added.\r\n\r\nCo-authored-by: Giovanni Pizzi <gio.piz@gmail.com>",
      "Docs: move the \"Daemon as a service\" section (#4481)\n\nInstead of copying the template file inline, links are provided to the\r\nsame template on the MARVEL NCCR repository that is used for the Quantum\r\nMobile.",
      "Docs: move the cookbook to its own how-to page (#4487)\n\nThe scripts were updated to work with the AiiDA v1.0 interface. The\r\nexample for `AuthInfo` was also simplified as the `Computer` class now\r\nprovides the `get_authinfo` method.",
      "Docs: add info on archive format to internals section (#4467)",
      "Docs: move how-to write data plugin to topics (#4482)\n\nThis information is felt to be too detailed for the how-to section and\r\nit breaks the flow too much.",
      "Docs: add \"How to share data\"\n\nThe \"how to share data\" section includes instructions both for dealing\nwith AiiDA archives (e.g. for publishing AiiDA graphs alongside your\npublication) and for using the AiiDA REST API.\n\nCo-authored-by: Casper Welzel Andersen <43357585+CasperWA@users.noreply.github.com>",
      "replace all occurences of \"export file\"\n\nWe have agreed on the terms \"AiiDA archive (file)\" and \"AiiDA archive\nformat\".\n\nCo-authored-by: Casper Welzel Andersen <43357585+CasperWA@users.noreply.github.com>",
      "Docs: add \"How to interact with AiiDA\" section (#4475)\n\nThis section gives a high-level overview of the various methods with\r\nwhich can interact with AiiDA. For details it refers to relevant\r\nsections with more information."
    ]
  },
  {
    "pr_number": 5046,
    "commits_list": [
      "f7c0097d3613a4fb46167926c159773841a870e8",
      "00dcbf7c309b58d316fe32e345e63391e4661cd4",
      "42ed72748657f9fdd3b2af622fe13267d367fe2e",
      "d9bca0aec61bdfffe7dfb495f14fc5b0415da31a",
      "56d6a49e05b8f61e22282ee41082af9935860680"
    ],
    "message_list": [
      "Dependencies: Bump jinja2 to version 3.0",
      "Dependencies: Bump pydata-sphinx-theme to 0.6.3.",
      "Automated update of requirements/ files. (#5057)\n\nCo-authored-by: csadorf <csadorf@users.noreply.github.com>",
      "Merge branch 'develop' into dm/bump-jinja2-3.0",
      "Merge branch 'develop' into dm/bump-jinja2-3.0"
    ]
  },
  {
    "pr_number": 4219,
    "commits_list": [
      "6219600e9204cb9b02dd52f14dd531eb204ef663"
    ],
    "message_list": [
      "rename test fixtures => static\n\nThe name \"fixtures\" is currently used both for the pytest fixtures and\nfor test data in tests/fixtures, such as AiiDA export files.\nThis is confusing and makes searching in the codebase unnecessarily\ndifficult.\nHere, we rename the test data folder to \"static\", which indicates the\nstatic nature of the files residing there, while avoiding a clash of\ndefinition with the pytest fixures residing in aiida.manage.tests."
    ]
  },
  {
    "pr_number": 4539,
    "commits_list": [
      "a549d673d4a73f5e245f9880ad88a04c5b0cbae7",
      "61a92b024110b401b40c33a9393aa28e5d658e02"
    ],
    "message_list": [
      "make process function submittable\n\nFix #2965, let the process function can be submit to the runner, means shutdown the daemon will not fail the function process (`calcfunction` or the `workfunction`). So they can restart when the daemon is restart. Since the process function still can only running on the local machine, it blocking the daemon even we 'submit' it to the daemon.",
      "Merge branch 'develop' into feature/2965"
    ]
  },
  {
    "pr_number": 4554,
    "commits_list": [
      "ce1ad67b9e58cd8bede80de8a92fef373b6bccce"
    ],
    "message_list": [
      "Process functions: Add the `get_source_code_function` method\n\nThis method returns the source code of just the process function itself,\ni.e., the wrapped function plus the decorator itself.\n\nThe existing method `get_function_source_code` would return the source\ncode of the entire file in which the process function was defined, but\nthis can at times include a lot of other code that is not always useful\nto have. The `get_function_source_code` method is deprecated and replaced\nby the `get_source_code_file` method to have consistent naming.\n\nThe `get_source_code_function` implementation retrieves the source code\nof the function by calling `get_source_code_file` and extracting just\nthe lines of the function. This is accomplished by using the starting\nline of the function, which already used to be stored, combined with the\ntotal number of lines that make up the function, which is an attribute\nthat is stored from now on for process functions."
    ]
  },
  {
    "pr_number": 3912,
    "commits_list": [
      "08e531001e62d12db1e981c28e56a8e1ce55fe0b"
    ],
    "message_list": [
      "Add export archive migration for `Group` type strings\n\nThe `Group` entity class is now subclassable, for which the type string\nof existing entries in the database had to be changed through a\nmigration. Here we add the corresponding migration for existing export\narchives. The only required change is to map the type string of groups.\n\nTo test this migration we use the existing export archives in the module\n`tests/fixtures/export/migrate`. They did not contain any group entities\nso `export_v0.1_simple.aiida` was updated first and had four groups\nadded, one for each of the migrated type strings. This initial archive\nwas then migrated to each subsequent version, step by step, using the\ncommand `verdi export migrate --version`.\n\nAlso fixed and migrated `tests/fixtures/graphs/graph1.aiida` which was\ncorrupt and could not be migrated automatically, because it contained a\nprocess node with an active process state."
    ]
  },
  {
    "pr_number": 544,
    "commits_list": [
      "ec5806de7d5ab83f2d650dbcea6e131fc18fa8c3",
      "2a8d936282f354bb99ab9e15ee724adbfad51537",
      "c3ce662359db0c10ea3736d3d9c3f483f8db1248",
      "b45a0a631c60485d25e8d738161ed224e498b608",
      "e59a37654fa70d74437461fd75b1b7eef39cb55e",
      "cbe037813796050857feca97cc214e1e0d2d19e0"
    ],
    "message_list": [
      "Implement inheriting input parameters from other processes",
      "Add tests for processes which inherit inputs",
      "Merge branch 'develop' of github.com:aiidateam/aiida_core into workchain_inherit_inputs",
      "Merge branch 'develop' of github.com:aiidateam/aiida_core into workchain_inherit_inputs",
      "Add 'namespace' to spec.inherit_inputs",
      "Allow inheriting from a Process's inputs with multiple namespaces"
    ]
  },
  {
    "pr_number": 5718,
    "commits_list": [
      "da51c93e2dd9b9e44ab9d761bb382753872ec52a",
      "fecf04dec7637557ce21cccd8d9c3061dacbc3a5",
      "82d4f74111a1bfa9022709a14f39922f67645b85",
      "26f82a8a65e108f18b24e9a7311ec345f3cef353",
      "60342fef8fda59e507a24ac1673bf706f22e788b"
    ],
    "message_list": [
      "Refactor: Turn `aiida.manage.external.rmq` into a package\n\nThis is in anticipation of more code being added soon and the file was\nalready getting large and difficult to read. Turning it into a package\nwith individual modules helps readability greatly.",
      "RabbitMQ: Remove support for v3.5 and older\n\nRabbitMQ 3.5 has been EOL since 31 October 2016:\n\n    https://www.rabbitmq.com/versions.html\n\nThis version required to explicitly run `support_deprecated_rabbitmq` on\nthe `pamqp` library (which is used all the way down the stack by\n`aiormq` to handle interactions with RabbitMQ) to enable compatibility.\nIn removing this, the explicit dependency on `pamqp` is also removed.\n\nThe `aiida.manage.manager.is_rabbitmq_version_supported` function which\nis used to check compatibility when a profile is loaded is updated to\ninclude the lower bound.",
      "Add a client to connect to RabbitMQ Manamegement HTTP API\n\nRabbitMQ provides a plugin `rabbitmq_management` that provides an HTTP\nAPI to manage a RabbitMQ instance. For example, it can be used to\ninspect existing queues, create new ones, or delete queues.\n\nFor the client to work, the plugin should first be enabled with:\n\n    sudo rabbitmq-plugins enable rabbitmq_management\n\nIf the API cannot be connected to a `ManagementApiConnectionError`\nexception is raised.\n\nIn the Github Action workflows, the command above won't work, since\nRabbitMQ is installed as a service in a Docker container. However, it\ncan be enabled in the Docker container by adding the `-management`\nsuffix to the version specification of the RabbitMQ service. Since the\nAPI is exposed on port 15672, that is also added to the forward list.",
      "CLI: Add the `verdi devel rabbitmq` command group\n\nThe command group has three subcommands:\n\n * `server-properties`: Report properties of RabbitMQ server\n * `queues`: Interact with RabbitMQ queues\n * `tasks`: Deal with messages stored in tasks queues\n\nThe first command is a simple utility that can come in handy during\ndebugging as it reports various server settings, more so than the\nversion of RabbitMQ that is reported by `verdi status`.\n\nThe `queues` command is also mostly useful for debugging. It currently\nallows to list, create and delete queues.\n\nThe most important command is the `tasks` command. Currently, there is a\nbug where it is possible that tasks in the process queue (which are\nconsumed by daemon workers to continue submitted processes) can get\nlost, despite the persistence requirements on the queue and the\nmessages. When a tasks gets lost, the corresponding process will become\na \"zombie\" and seems stuck. Currently there was no easy way to confirm\nthat a process had become a zombie due to its task having been lost.\n\nThe `list` method fixes this problem as it will introspect all task\nmessages in the process queue and collect the process ids that they\ncorrespond to. This can be used to see which active processes no longer\nhave a corresponding task and so are a zombie.\n\nThe `analysis` command is a utility command that automates this process.\nIt will get all process ids of the existing tasks in the process queue\nand cross-reference those with the active processes as stored in the\ndatabase of AiiDA's storage backend. By default it reports if there are\ninconsistencies between the two sets. If there are, the user can run the\ncommand again with the `--fix` flag and the command will automatically\ndiscard tasks that are incorrect and recreate those that are missing.",
      "CLI: Move `verdi devel revive` to `verdi devel rabbitmq tasks revive`\n\nThe command was recently added before the `verdi devel rabbitmq`\nsubcommand existed. At this point it makes more sense to group it there\nbecause it is directly related to RabbitMQ and if this service is ever\nremoved, this command also no longer serves a purpose."
    ]
  },
  {
    "pr_number": 4070,
    "commits_list": [
      "f8cbb354e5db6c70262d25fc734487469a7b3487",
      "844da351c8d7b234014e33f49456c8012966b36d",
      "f229ea8e808743f3034af59b1c4489b4a4132016",
      "1bc5aaa0f558e8dd7ba89a1419f578dd2c6f3370",
      "9f9a74d9e020915eda131fae7000bacb814cc771",
      "298ece32ff300a1be5ddad735cc346a8a057aff5",
      "0aca8517c2e6427d6895954d7d5be3754b4d249c",
      "8cd38edd82f67883d48451168465ca89e4808710",
      "f77f9f0d533cce55341be53b0296f98a97e88386",
      "ebe46b87865ab21e62ecd53dda15457139dc569f"
    ],
    "message_list": [
      "docs: add first draft of aiida tutorial\n\nFirst draft of the AiiDA tutorial. Next to adding the tutorial itself, I've also:\n* Changed the ArithmeticAddCalculation to set defaults for the resources and use a basic bash execution on the input files.\n* Added the MultiplyAddWorkChain for demonstration purposes.\n* Set the background-color of bash code snippets to aliceblue.\n* Added the sphinx-copybutton extension. This still needs to be improved, as currently all code snippets are provided with a copy button, and e.g. those with a Python prompt also copy this prompt.",
      "Remove new processes and sphinx-copybutton\n\nRemove the MultiplyAddWorkChain class and the changes to the\nArithmeticAddCalculation class from this branch, so they can be merged\nseparately into the develop branch.\n\nAlso remove the sphinx-copybutton extension for now, as this addition\nis being dealt with in another issue (#4062).",
      "Replace MultiplyAddWorkChain include with codeblock\n\nReplace the literalinclude for the MultiplyWorkChain with a code-block\nas the module has been removed from this branch.\n\nAlso, clean up the trailing whitespace.",
      "Update docs/source/tutorial/basic.rst\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Apply suggestions from code review\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Docs: Fix code-block formatting\n\nUse proper code-block formatting for the different code-block types.\n\nAdd aliceblue background color for console code-blocks.\n\nAdd output for `verdi daemon status` command, as well as small fixes\nto the text.",
      "Apply suggestions from review",
      "Fix: change accordion by dropdown",
      "Apply suggestions from code review\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Docs: apply reviewer suggestions"
    ]
  },
  {
    "pr_number": 3205,
    "commits_list": [
      "1128c8cbcd8b2c8fbc0126e3c63e9dfb8ec695ef",
      "80f72007f2bfbfeb7f68db133487eb161885ead8",
      "af7ba8aa81212983d754dbe74d2fe996a9dc30fb"
    ],
    "message_list": [
      "Run through backup docs on Ubuntu 18.04\n\nUpdating some lines according to going through most of this page on an\nUbuntu 18.04.02 LTS setup using AiiDA v1.0.0b5.\n\nAdding a few helpful lines elsewhere in docs according to some more\nadvanced information in the backup documentation.",
      "Satisfy (new) pylint",
      "Merge branch 'develop' into fix_3028_docs_working_with_backups"
    ]
  },
  {
    "pr_number": 4861,
    "commits_list": [
      "bfba8e978327d2aa08e13cc8addece4c5bdd681d",
      "4fbaea2423c22f70563d4f8ae89ba59ff354c2e7",
      "46bb0116eeb4090b9d24a5e344f4117642e1800a"
    ],
    "message_list": [
      "Added a remotecat command for printing remote file\n\nThis can be usedful for monitoring the calcjobs while they are\nrunning.",
      "Various minor improvements\n\nAlso adds a test for the `--monitor` option.",
      "Remove the `--monitor` option"
    ]
  },
  {
    "pr_number": 3958,
    "commits_list": [
      "b2af3f3869d33d6837ce2c7f9e1a2d80d2ff45de",
      "341327e99cdf6d87ec31205cf24a460fa1b0d4e4"
    ],
    "message_list": [
      "add github action of transifex upload",
      "Merge branch 'develop' into transifex-action"
    ]
  },
  {
    "pr_number": 3738,
    "commits_list": [
      "1c369a327be2286e67739a705430c16f7e514273"
    ],
    "message_list": [
      "Cleanup the top-level directory of the repository\n\n * Include `bin/runaiida` through `console_scripts` of `setup.json`\n * Remove the outdated examples from `examples` directory\n * Remove obsolete `utils/plugin_tpl/calculation.tpl` superseded by\n   plugin cookie cutter package\n * Move `conftest` to the `tests` directory"
    ]
  },
  {
    "pr_number": 5250,
    "commits_list": [
      "ccc5329dbc7b9b9c7a231acf0f5de2eff8db7952",
      "581695693b9758ba21ee84e6aebe188a8193188d",
      "e61fd73abb5a4a962fc3887dffb182f6c6f4cbe1",
      "56fd860373781395a7a754feba48f04c3d86ff63",
      "034e77b382f759e22d5f4f08d3dc324d02e143f4",
      "54ad5688825b63f8fabfcc46996b98bc1bee6fca",
      "77e996dbcb101d86c565afccb1d7f649b1ee6c61",
      "2e968b43adc39e3e269da36f7b5f7efba7978c30",
      "a8c5ce1985dcdd107815c2da7c909de2a2259493",
      "2f5b53f48f6cf70a62ee933077ac76e58188defa",
      "02067c33f4a4586401d7a80e6e9b5948f0e06eda",
      "549f43f73bbe8f11f2cdba81a5b516e9011a63f6",
      "e1e758d256f5604ae00c4aadff40182f10eddfaf",
      "9e23623321e5f0ecc1913c84ef74f9c3d5c88e1b",
      "ba40f5aec77eb0eed8f31d5bee0bf09607735e00",
      "afa69fc84cee587a889482b2c1e1f2dd566de15f",
      "26191fb322baf04c4b9674e1bd6d300045cd8144",
      "60049cea35e21f029046c46ecfdfbfa8d4a2bf39",
      "7d99acee7c48fb76764766942264f5e8de5d168e",
      "cba54e51d04c949099001eadc842a4a0c8f0376e",
      "1ea9e5e6093a52f8fc39c91a4516ba9ab8072a95",
      "00aa846668a40efbbdc5dfb4c4ab29be27e1835b",
      "235b67be3e3d29ba1c43233afc667584100c15bc",
      "aa1bd86317bb9f5b48c59f1648387a5f3b41f273",
      "b54e6e27b355c74e34388e7f29407698f96984bd",
      "c915cb25d1766b8b98e01885cbcba0ea34865644",
      "34812cc26d6ffe6d2ce2858abd82594b5ca74e65",
      "32845807748495b71b0ae829057fc0ed25975944",
      "ee8b20bd927f70941006ece863d1686492fb9198",
      "97d994a4d971275cca0671a0cd0d2a7fc7509e1f"
    ],
    "message_list": [
      "escape bash with double quotes as an optional",
      "address escape issue for ENV variable",
      "review",
      "double quotes set from computer and to exec command line",
      "update test",
      "seperate std input output",
      "computer_cmdline_params",
      "add use_double_escape to Code and CodeInfo",
      "unittest",
      "add computer_cmdline_params to code_info",
      "Add custom string for code info to maximum flexibility",
      "rename to prepend_cmdline_params",
      "code_info use_double_quotes as a tuple",
      "explicitly set use_double_quotes in every scheduler test",
      "code setup",
      "computer setup",
      "draft container code class from code",
      "test for container code by calcjob",
      "ugly implementaion of code setup CMD for container code",
      "add template for the sarus cmd parameters",
      "use tmpl to set sarus command",
      "do not escape '$' in bash script",
      "rename parameters of ContainerCode init",
      "cmdline for sarus",
      "clean ContainerCode class",
      "rollback to only set image in tmpl",
      "dont escape when there is $ cmdline param",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "rename to ContainerizedCode",
      "rename the variables\n\nuse `container-engine-command`, `is_containerized`, `container_command`,\n`ContainerizedCode`."
    ]
  },
  {
    "pr_number": 3052,
    "commits_list": [
      "9dbb85f65d7659ffe67e4007cb79cb4fe26bf0d6",
      "8b36757f585f20c6c5ca06cebcc284b303fd295f",
      "f8d79c367ce9351096fb940387fcb7fce6fe01b3",
      "707bf6dd91967aaa66555f5f74c388438d241d34",
      "3c1d52e0220ce3a880cfe0157af27765dd71a4df"
    ],
    "message_list": [
      "Refactoring importexport module.\n\nSplit up and then remove `aiida.orm.importexport`.\n\nMoved to `aiida.tools.importexport`.\nSplit up into structure (relative to `aiida.tools.importexport`:\n./dbexport/__init__.py\n./dbexport/utils.py\n./dbexport/zip.py\n./dbimport/__init__.py\n./dbimport/backends/utils.py\n./dbimport/backends/django/__init__.py\n./dbimport/backends/sqla/__init__.py\n./dbimport/backends/sqla/utils.py\n./config.py\n./utils.py\n\nImport/export is now considered a tool, and has nothing to do with the\nspecifics of AiiDA.\nIt should be in a state, where it would be easy to remove it and save\nit in a separate repo.\n\nImport with a django or sqlalchemy backend has been split up.\nFor export, this is not needed, since it is already backend-independent.\nTo further reflect the split of backends, the actual import functions\nhave been placed\nunder `aiida.tools.importexport.dbimport.backends.[django/sqla].`.\nIn the future, `dbimport` should reflect the situation from `dbexport`\nand be backend independent, creating the imported nodes through the\nnew backend-independent ORM entities.\n\nExport to a zip-folder/file has been split up in a separate file.\nThis should later be implemented in another place, since a ZipFolder\nclass exists here.\nFor example `aiida.common.folder`. See also issue #3100.\n\nNote: Be careful with wildcard imports in `__init__` files for backends\nspecific functionality.",
      "Split up and move importexport tests\n\nOriginal file: `aiida.backends.tests.test_export_and_import.py`\n\nSet up tests in `aiida.backends.tests.tools.importexport.`\nmatching the newly moved import/export module, which has been moved to\n`aiida.tools.importexport.`.\n\nTests are split up into separate files according to the test classes in\nthe original file.\n\nAll tests related to ORM entities have been put under\n`aiida.backends.tests.tools.importexport.orm.`.\nWhile `aiida.tools.importexport.orm.` does not exist, this simplifies\nthe overview of the tests.\n\nThe tests were moved under `aiida.backends.tests.` and not\n`aiida.tools.importexport.tests.` because we are currently looking into\nchanging to a pytest API, and it is therefore easier to do this later\nchange having all tests in a single place.\nFurthermore, other tests, which are not backend dependent, are also\nfound here.\nThis is more-or-less for the same reason.",
      "Move export archive migration\n\nMoving module at `aiida.cmdline.utils.migration` to\n`aiida.tools.importexport.migration`.\nUpdate `verdi export migrate` to point at new location.",
      "Add import/export test of `Attributes`",
      "Remove LINK_ENTITY_NAME and ATTRIBUTE_ENTITY_NAME\n\nThese are removed from the importexport module,\nsince they were/are never used."
    ]
  },
  {
    "pr_number": 2846,
    "commits_list": [
      "ffe1664fa0551aca2f00cf218277850c57bc707e",
      "ab02daa770a2797bf97809bb35045009964a68e6"
    ],
    "message_list": [
      "Separate core of `aiida.manage.external.postgres.Postgres` utility\n\nRemoving the core of the `aiida.manage.external.postgres.Postgres` into\na separate class (PGSU for postgres superuser), which only has the\npuprose to connect to a given database as the postgres superuser and\nexecute SQL commands.\n\n * Make sure psycopg2 will connect to 'localhost' and not try to\n   go via sockets\n * Minor changes in the `Postgres` API\n * Add new options to `verdi quicksetup`:\n   As mentioned in #2836, `verdi quicksetup` actually deals with two\n   users and databases: the database & user to be created for the AiiDA\n   database, and the superuser (& template database) to be used for\n   creating those.\n * Fix construction of `Postgres` instance from aiida profile\n * Switch to providing sensible defaults in `DEFAULT_DBINFO`\n   and dealing with special cases (psql vs psycopg) in the respective\n   functions",
      "Merge branch 'develop' into issue_2836_quicksetup"
    ]
  },
  {
    "pr_number": 2487,
    "commits_list": [
      "6f7039e31a9210bd4e1eab12d31226f81fe5182e",
      "99abc2b7ac78dc7fa62e97a3d900ad19f7616b52",
      "bead9e5340d747cf97cabc88c4d90100e47cb1c6",
      "e00a56526ea1e0ed3ea74c93a8b8aac432647606",
      "ac36a176712781cadd7485e44b9ac52c4980cfdf",
      "654cfe730b346b73445e56d085eb0926b92bdebc",
      "7c03278e9f013450876e230dc91c84e06e650da4",
      "07fcabf2dddc78fd0b2ec019dc7c83f159a02d87",
      "434b964457f967d98252096f267ecb70776d679b",
      "9327359a091f4328bce845c1d8f70281006e663a",
      "f5cf79521eb01306a4b4a3a1dde40d9afee36a95",
      "ccdfc3b06502b6f2335d8f3df643624deb526d88",
      "a791f2f630b876f2420d2a066f16c509c5a56c13",
      "d70a5c8ed1157c6dfb8191129da615d119151f8a",
      "8b21ca40176d68d93090adc633d8ed49599efbc1",
      "a339462f4aeb7e49c34b80b73b4ca3ece66fd0c5",
      "24e457c82779e491f51473b9875a4f22e0094c8a",
      "30c6e14cd746fcd98ed36f8e9cb821b2148c91d8",
      "31d37dc736f01e026a4b8dc6681fea3a85ffd494",
      "3aec2a7d285be775523dca281a1be7cbaf758e16",
      "ed5266c3e30a07ff222ed6958b141bb483ad0bc4",
      "e59121aca4e6d574f1b0905103d1c8640275ae92",
      "6202948c8616660c1d65d84d474fb69d801b0d85"
    ],
    "message_list": [
      "add verdi status\n\nShows\n * profile status\n * daemon status\n\nTo add:\n * rmq status\n * postgres status\n * file repo status",
      "add more status messages\n\n + status for postgres\n + status for rabbitmq\n + status for file repository\n\ntodo:\n\n + document this\n + get rid of load_dbenv (needed for rmq at the moment)\n + file repo always created automatically. do we want this?",
      "Merge branch 'provenance_redesign' into issue_2409_verdi_status",
      "move aiida.work.rmq => aiida.manage.external.rmq\n\n + remove mandatory dependency on aiida.common.serialize\n   since it requires the dbenv",
      "Merge branch 'issue_2409_verdi_status' of github.com:ltalirz/aiida_core into issue_2409_verdi_status",
      "remove export of rmq to aiida.work",
      "fix rmq import",
      "add documentation of verdi status",
      "Merge branch 'provenance_redesign' into issue_2409_verdi_status",
      "Merge branch 'issue_2409_verdi_status' of github.com:ltalirz/aiida_core into issue_2409_verdi_status",
      "add basic test",
      "Merge branch 'provenance_redesign' into issue_2409_verdi_status",
      "add different symbols for UP/DOWN\n\nso that you can paste them into forums/on github without losing\nthe meaning",
      "fix capturing + test error\n\n * capture stderr as well\n * stop communicator after creating it",
      "Merge branch 'provenance_redesign' into issue_2409_verdi_status",
      "switch to frozendict",
      "Merge branch 'issue_2409_verdi_status' of github.com:ltalirz/aiida_core into issue_2409_verdi_status",
      "fix layout",
      "move printing outside try/except",
      "rework postgres connection determination\n\n * use parameters specified in profile to check connectivity\n * fix #2519",
      "move away from frozendict\n\n * move back to regular dict (I thought frozendict was used elsewhere\n  but apparently it isn't)\n * make Postgres.from_profile more robust to be able to handle\n   test configurations that are missing data",
      "Merge branch 'provenance_redesign' into issue_2409_verdi_status",
      "add fixes by @sphuber"
    ]
  },
  {
    "pr_number": 5691,
    "commits_list": [
      "36118c8a741784ccbfd85d60ef1d517f6aa9c7f6"
    ],
    "message_list": [
      "`ProcessFunction`: Add support for variadic arguments\n\nUp till now, variadic arguments, i.e., arguments defined as `*args` in a\nfunction signature which collects any remaining positional arguments,\nwere not supported for process functions. The main reason was that it\nwasn't immediately clear what the link label should be for these inputs.\n\nFor normal positional arguments we can take the name of the argument\ndeclaration in the function signature, and for keyword arguments we take\nthe keyword with which the argument is passed in the function invocation.\nBut for variadic arguments there is no specific argument name, not in\nthe function signature, nor in the function invocation. However, we can\nsimply create a link label. We just have to ensure that it doesn't clash\nwith the link labels that will be generated for the positional and\nkeyword arguments.\n\nHere the link label will be determined with the following format:\n\n    `{label_prefix}_{index}`\n\nThe `label_prefix` is determined the name of the variadic argument. If a\nfunction is declared as `function(a, *args, **kwargs)` the prefix will\nbe equal to `args` and if it is `function(*some_var_args)` it will be\n`some_var_args`. The index will simply be the zero-base index of the\nargument within the variadic arguments tuple. This would therefore give\nlink labels `args_0`, `args_1` etc. in the first example.\n\nIf there would be a clash of labels, for example with the function def:\n\n    def function(args_0, *args):\n\nwhich when invoked as:\n\n    function(1, *(2, 3))\n\nwould generate the labels `args_0` for the first positional argument,\nbut also `args_0` for the first variadic argument. This clash is\ndetected and a `RuntimeError` is raised instructing the user to fix it."
    ]
  },
  {
    "pr_number": 4792,
    "commits_list": [
      "09463c4542bb0439b6353a70dcf22d5fafa82565",
      "9d765ce50a225282df2c5e02d3809ab105b3d848",
      "43236cc01f1186796fb5a326c3ad3c04428bdd2d",
      "68974a635a947842f240db06dcf103e1c6f4ad82",
      "533c42ed7c8ae36a76a021707b5f1df44070bf8b",
      "4feccf65327499a6177e5918b3de761fff298386",
      "6f041a56f7a185beefe7dc2f95f18eff9da68fcc",
      "87be4170869f68b447171024e8c1265c6ae53261"
    ],
    "message_list": [
      "\ud83d\udc1b FIX: Task.cancel` should not set state as EXCEPTED",
      "Merge branch 'develop' into fix-task-cancel",
      "Apply suggestions from code review",
      "Merge branch 'develop' into fix-task-cancel",
      "Merge remote-tracking branch 'upstream/develop' into fix-task-cancel",
      "Update kiwipy/plumpy versions",
      "add test",
      "fix pre-commit"
    ]
  },
  {
    "pr_number": 5054,
    "commits_list": [
      "2f87d7431b5120486753aa6bd676684b1c70a6d3",
      "9e95849eb517c7fec23edd744ac1950a83bac36f"
    ],
    "message_list": [
      "REST API: make the profile configurable as request parameter\n\nTo make this possible, after parsing the query string but before\nperforming the request, the desired profile needs to be loaded. A new\nmethod `load_profile` is added to the `BaseResource` class. All methods\nthat access the storage, such as the `get` methods, need to invoke this\nmethod before handing the request.\n\nThe `load_profile` method will call `load_profile` with `allow_switch`\nset to True, in order to allow changing the profile if another had\nalready been loaded. The profile that is loaded is determined from the\n`profile` query parameter specified in the request. If not specified,\nthe profile will be taken that was specified in the `kwargs` of the\nresources constructor.\n\nNote that the parsing of the request path and query parameters had to be\nrefactored a bit to prevent the parsing having to be performed twice,\nwhich would result in a performance regression.\n\nWhen the REST API is invoked through the `verdi` CLI, the profile\nspecified by the `-p` option, or the default profile if not specified,\nis passed to the API, which will be passed to the resource constructors.\nThis guarantees that if `profile` is not specified in the query\nparameters the profile with which `verdi restapi` was invoked will be\nloaded.",
      "Add global config option `rest_api.profile_switching`\n\nThis global config option is set to `False` by default. When set to\n`True`, the REST API will allow requests to specify the profile and it\nwill switch the loaded profile when necessary. If a request specifies\nthe profile query parameter and profile switching is turned off, a 400\nBad Request response is returned."
    ]
  },
  {
    "pr_number": 5454,
    "commits_list": [
      "6244f60987c2c8d3d3913c753e61bbc3c7f93267"
    ],
    "message_list": [
      "Made singlefile initializable empty"
    ]
  },
  {
    "pr_number": 3957,
    "commits_list": [
      "9d4cfc4230d8424b43310bc5a69e5175f4988be1",
      "08cdff4b67f17351bab645b5ac2948132df3ba64",
      "3b39e23ac51c1c19b2081fb2d53273d7fc0f7626",
      "bfa8ccfe63ef2b49050c9d883a58f8c6b08d02c8",
      "0f650b0fdc9f58f09b1158ece93f643612252192",
      "4aae357d1d07472da25ce835b3ce61695f4447a7",
      "5c3e8f4a1162b0d7b0633a89006a1a7d8306121c",
      "2fef6856f86dee89704e6e6982bc60d1c4772b82",
      "ba632c8d3b790b72830e08f88a424a21356e5b17",
      "f2440b5f670556bb30bb83dc7eb1dd44bd667fb5",
      "59bb168f4951a11dba8cf70cba2664fd1ef1c504",
      "232703f4426242604a36df14bbb6cac933b1ffbd",
      "b91c51d295b5aeccdb58792f4b398bf3e0e5cc73",
      "412411b2137f5add7a0611da9c9dc5f3a750795c"
    ],
    "message_list": [
      "Update 'test-install' workflow to address requirements/ issues.\n\nIdentify and attempt to remedy inconsistencies of the requirements/\nfiles with setup.json.",
      "Update commit comment for inconsistent requirements/ files.",
      "Use Python 3.8 consistently for dependency checks.",
      "Require check-requirements job to succeed to execute 'tests' jobs.\n\nWith inconsistent requirements/ files those tests are misleading.",
      "Provide detailed error message for missing matches of dependencies.\n\nWith some refactorization of the utils/dependency_management.py script.",
      "Allow to turn off file annotation as part of GitHub actions workflow.\n\nTo avoid the duplication of the warnings as part of the 'test-install'\nworkflow.",
      "Add README.md for requirements/ directory.\n\nTo provide some context for the purpose of the contained files.",
      "Ignore unrelated files for check-requirements command.",
      "Only create commit comment for expected check-requirements errors.",
      "Simplify comments in test-install workflow to be more concise.\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Improve commit and PR comments.\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Improve wording of requirements/README.md\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Fix line number in commit warning message referring to setup.json.",
      "Remove obsolete update-requirements workflow."
    ]
  },
  {
    "pr_number": 4564,
    "commits_list": [
      "39cabfad20daf2cd41ae9d6f8ec5381885a48eff",
      "7fbc4ce10f2781cf2140fd658bbfe684e12c7a02",
      "60ebfaaf4033f8d1acde442286772644cba20017",
      "e2675087dc77ad1f55a07c53cbd91b0407bc6da5"
    ],
    "message_list": [
      "Add .dockerignore",
      "Update .dockerignore",
      "Merge branch 'develop' into add/dockerignore",
      "Merge branch 'develop' into add/dockerignore"
    ]
  },
  {
    "pr_number": 4470,
    "commits_list": [
      "f53b3e9c1a4ee6a69c64c31bf9563d01423639fe",
      "6c9578e213ca418fcb77cd04bbe31c663b423d24",
      "e6a76c42e3f6f0b5f29009b3073932f55fa819a2",
      "648f68dfc60741a1475a7094dc7ead61ca827a0b",
      "772b40f520d57ea2741e511d825252ba8f3e57a3",
      "cd37991111912f6299e71f6274ba441bdebe75bd",
      "0b38ff1da22675b38d47008abe6f3043ee7b323b",
      "1ab9af8287521399ab3ec3e5fda0e9b960f7d1e5",
      "29954d44dc35ecb09da35671b6fb9dc6dcdf9eb4",
      "1b3d522872c0c381c2e78d63b4dd2531e36525e6",
      "446d1a3f41529d8d74bf4d53f8a79b544e1c3140"
    ],
    "message_list": [
      "Docs: \u2b06\ufe0f Updata sphinx version + extensions\n\nAlso moved sphinxext testing to the official sphinx testing infrastructure",
      "Merge branch 'develop' into docs/update-versions",
      "update",
      "Fix tests",
      "Merge branch 'develop' into docs/update-versions",
      "Update setup.json",
      "Fix autodoc bug",
      "Merge branch 'develop' into docs/update-versions",
      "fix test",
      "roll-back autodoc",
      "fix List.pop"
    ]
  },
  {
    "pr_number": 2049,
    "commits_list": [
      "1f5b79ff4c241abfefb3615576c8f3c0586bbaac",
      "d8cea6c00d749ffb8ca12499cd809d643369acd8"
    ],
    "message_list": [
      "Fix several issues with the hashing method\n\n* dict items are sorted _after_ hashing, fixing the behavior for\n  keys which can not be sorted\n* unicode/str and str/bytes are explicitly handled, with a different\n  type salt\n* in general, the combination between two hashes is done via the\n  method of boost::hash_combine, not by concatenating the hexdigest",
      "Add docstrings, remove dead code."
    ]
  },
  {
    "pr_number": 3327,
    "commits_list": [
      "d98976e03d986e120c698b435757eef2f4f461c6"
    ],
    "message_list": [
      "Fix the identifier format of the caching configuration file\n\nThe old caching configuration file implementation expected the module\npath of node sub classes that should be either enabled or disabled for\ncaching, for example:\n\n    `aiida.orm.node.calculation.job.TemplatereplacerCalculation`\n\nThe old caching implementation in the `Node.store` method would\nthen compare these directives to the node type of the current node being\nstored to determine whether it should be considered for caching. This\nmethod broke after the separation of the \"process\" and \"node\" concepts.\nSince calculation job classes, for example, are now no longer a sub\nclass of `Node`, but of `Process`, the corresponding node that will be\ncreated will all be `CalcJobNode` instances, regardless of the exact\nprocess sub class. That particular bit of information is stored as its\nentry point string in the `process_type` column of the node. For example:\n\n    `aiida.calculations:templatereplacer`\n\nThis entry point string is now also the identifier that determines the\ncaching behavior and should be used in the configuration file."
    ]
  },
  {
    "pr_number": 5319,
    "commits_list": [
      "c5e9d192466a1e50a044918a420e28c6fa6b73b9",
      "5f64ec17b46b78d481a436ec60ddda0c53523bd6",
      "e483a74b37a326e6b3900d6bac243a5efcc601f9",
      "fc142ab110b516216c8ddd9d36ce17f2c80ebc08",
      "16a2703384bf481cee88153cd342e3ec64163bca",
      "e3c059a8f0ef43fb65f7708072bbe81f6146ea6e",
      "532f3152feee2c736a80f80d5ee505455cc233de",
      "bd3a6df7cb879c7fdf9db1f0edab861d24d34423",
      "baea9cec3080d138407b5b5262b3bbaa727d6a56",
      "4b797eb519f6066167dfd27eb0603621f23f05ef",
      "2f9fa47f977c6e315beb5ff82b2f5094c7b83def",
      "0cfae86eba6a77e701d57313b8ddad20ab6e1dd4",
      "8e4b70a40d49ad2117d07f56e0678ee650fccf83",
      "e34d0410b95cf70d68a7975cff902c0628b225ff"
    ],
    "message_list": [
      "\ud83d\udc4c IMPROVE: Configuration migration\n\nAllow for both upgrades and downgrades, and expose this in the CLI.\n\nThis will allow a route for user to downgrade their configuration,\nif they try a new version of aiida, but then want to return to an old version.",
      "Merge branch 'develop' into storage-config",
      "address review comments",
      "Merge branch 'develop' into storage-config",
      "add new line between class attributes",
      "Update verdi_config_downgrade and add test",
      "Merge branch 'develop' into storage-config",
      "fix docstrings",
      "Add failure tests",
      "add typing",
      "Update migrations.py",
      "remove options",
      "add choice",
      "Merge branch 'develop' into storage-config"
    ]
  },
  {
    "pr_number": 4229,
    "commits_list": [
      "82faf15391b54cd1649e9110608acdf37b6736c2"
    ],
    "message_list": [
      "ci: run all on python 3.8\n\nFor unknown reasons, the python3.7 environment used for the \"verdi\" and\n\"pre-commit\" jobs was broken."
    ]
  },
  {
    "pr_number": 4457,
    "commits_list": [
      "10b44d9100a162c631672e415c39d7a1ac3b5437",
      "b7494f0349cf57bcda56fb42252215302f648ae6"
    ],
    "message_list": [
      "Docs: Add redirects from old documenation",
      "Merge branch 'develop' into docs/redirects"
    ]
  },
  {
    "pr_number": 2015,
    "commits_list": [
      "cb29f50aaa447e87af1bf76d1aa934f1522f5fae",
      "91b7ca8f00444b8ca2f3096189cbeb99f586b26c",
      "7751c2a3e766c94c65139b21034aeb3675ed993d"
    ],
    "message_list": [
      "Adding representer and constructor for AttributeDict.",
      "Add a docstring to 'add_mapping_representer'.",
      "Use single '!' for tag."
    ]
  },
  {
    "pr_number": 652,
    "commits_list": [
      "ce539d8b97a5059e2a9306b9cfc99737c9f29da4",
      "7589afdbb535a814e2a10b3c8bb916ae2d85e091",
      "13caff274d9965613dd0b374c87bf3a6463a125e",
      "ca5f0477021a27fe98e7ec503fff05fd9abec222",
      "87e546fc8fbd3856464db3f02deb5c8453b3f1f3",
      "39d378e3bb1cf1487237079a448a01c1540cf7b5",
      "f7e6fe039ffa4e317615edc4f0cdf2663e590cb2",
      "28af8ed96a3ec8c47dbf0873ca4decf2b37f0915",
      "f12c3cb8bd87f38e82b89fac12ca81330fd6ed6a",
      "219d53ba0b826de0e6438492e908e3fe2466fe93",
      "76acbb6e17d8dce555cba1820502fe6f5693b648",
      "2de9bc82e2a99c70fb9e51cd74a0959dc0a66fc2",
      "a4ae949e5fcce8de3e6a798085bfa5667ffacb52",
      "516c44e89531d93eea2de55cd8f7f9cae5e48e58",
      "dcb4a38b7c432edfba957c9598fbda6cc7547c7f",
      "bbe4c33ad582e45885679df00e3fdb83612a1257",
      "fb182ccb3116173f1a7df46f179a35d003066359",
      "dc939620a5c0bd1ec90024134fc86c818f9419ce",
      "5fef7f2fe1a26c20509ff947278398ada372073f",
      "f04d3bb8c44310746ed73815879383b9addbd9d8",
      "1bc7c90b38c22fb7e9338f5423653c77846393a2",
      "c5df6bf6c54dade644304bd51b5df34dbc18a9e6",
      "e5c72a186600b6c8407427f879ea03794a269a99",
      "a896eb102d983742f3b27f050dea9fb389a02c81",
      "d13f8dd37ac7fb60eb5f31feacf3f1fb3aeeae7a",
      "533e9684851dd3954e6c4ce6d6bd4f891af2401b",
      "55d1ad0c41286786787e4f2f405a9eca4f0def82",
      "29f14bae175ab70c35b05dbdcb5abe0eaabcced6",
      "7a0d4efbf2da476cbd41f2765dc8442a26a60f0a",
      "148770df25dcbd2b66d14a2d276dac961f0b2c8a"
    ],
    "message_list": [
      "#119 added first small test to check for node hashing",
      "#119 Added small functions get_hash and get_same_node to Node",
      "#119 Added functionality to store method (only for Django backend) that replaces the _dbnode member if a similar node already exists",
      "#119 Try-except clause if hashing fails, and also check for None for hash, which obviously should not be checked in the DB",
      "#119 Fix in test that was failing because there is now an additional extra (hash)",
      "* Failing test for two small but unequal floats\n* Failing test for two ArrayData with unequal content\n* (Accidentally) passing test for two ArrayData of different size, with same str representation\n\nFor the ArrayData, we need to take the actual array into account when creating the hash,\nnot just the shape which is return in get_attrs()",
      "Merge pull request #591 from greschd/node-hashing\n\nAdd more node hashing tests",
      "Merge branch 'develop' of github.com:aiidateam/aiida_core into node-hashing",
      "Merge branch 'node-hashing' of github.com:aiidateam/aiida_core into node-hashing",
      "Give name to '_' in loop",
      "Update hashing algorithm",
      "Fix get_hash for special case of ArrayData",
      "Fix ArrayData issue by hashing folder content for 'pathlib.Path'",
      "Explicitly raise error when non-directory path is hashed",
      "Add pathlib2 requirement",
      "Print modulename to check failing Travis test",
      "Try moving ArrayData import inside the test",
      "Move numpy import to test",
      "Add comma after checksumdir requirement",
      "Add find_same to store_all, add to SQLAlchemy",
      "Simplify get_same_node",
      "Set hash extra on SQLAlchemy nodes",
      "Change how extra is stored on SQLAlchemy node",
      "Use Folder interface for hashing the repository folder",
      "Add logic to ignore attributes",
      "Add tests for FolderData with empty files and folders.\nClose files after reading them to create the hash.",
      "Add functionality to set the default of caching True / False",
      "Merge branch 'develop' of github.com:aiidateam/aiida_core into node-hashing",
      "Add contextmanager to enable / disable caching",
      "Change caching default in store methods to False.\n\nThe caching.defaults.use_cache parameter should be used by\nplugin developers to mark whether a specific .store() call\nCAN actually use caching. The user decides whether to use it\nin the end by setting the default to True / False."
    ]
  },
  {
    "pr_number": 4983,
    "commits_list": [
      "85e7331169f4dd21f27591a22f0fa6424eedc3ab",
      "ef310f19f2fd4002f8d25b0cbf9adebbb27cf41e"
    ],
    "message_list": [
      "`ProcessBuilder`: fix nested namespaces losing `ProcessBuilderNamespace` class\n\nWhen a `ProcessBuilder` is constructed from a `PortNamespace` each\n`PortNamespace` is converted into a `ProcessBuilderNamespace` including\nthe nested `PortNamespace`s. This guarantees that even nested port\nnamespaces have the same autocompletion and automatic validation when\nassigning values as the top level namespace.\n\nThere was a bug however that when assigning a dictionary to a namespace\nof the builder, that namespace would lose its `ProcessBuilderNamespace`\ntype and become a plain dictionary. Subsequent assignments to ports\nwithin this namespace would then no longer be validated.\n\nThe source of the bug was in the `__setattr__` of `ProcessBuilderNamespace`\nwhich would simply assign the value that is being passed to the internal\n`_data` container, losing the `ProcessBuilderNamespace` class if the\nvalue is a plain dictionary. The solution is to, in the case where an\nattribute that is being set corresponds to a `PortNamespace`, to first\nassign a new `ProcessBuilderNamespace` in the internal `_data` container\nfor that given attribute, and then iterate over the contents of the\nvalue dictionary being assigned and call `setattr` for that on the new\n`ProcessBuilderNamespace`. By calling `setattr` again, the process\ndescribed above is applied recursively, as it should.",
      "`ProcessBuilder`: add the `_merge` method\n\nThe class already had the `_update` method, but like the normal `update`\nmethod of a mapping, this will not recursively merge the contents of\nnested dictionaries with any existing values. Anything below the top\nlevel will simply get overwritten.\n\nHowever, often one wants to merge the contents of the dictionary with\nthe existing namespace structure only overriding leaf nodes. To this end\nthe `_merge` method is added which recursively merges the content of the\nnew dictionary with the existing content."
    ]
  },
  {
    "pr_number": 5341,
    "commits_list": [
      "b40cbaa2bf91b58495063624f2bb7b85c01ccbc1",
      "bdb27d6634857cce70bd2c12aaf9c498852db554",
      "026f7e16e7501694941b714aa40bb16c42728214",
      "46fb66bbaaeba64a4eda37d8ef0460eba4019d02",
      "b190ecc50705d488b7e5d960b480501984febbae",
      "1cbb0899073cfa5c59ef56787e596e0400007acb",
      "59a5cc5332b0104dc4fe072a3d87f2730565816c",
      "e1d500489899841fbf5dff0cffcd91db66d720cc",
      "df08987b4e2bbc71b2b46983729dfeedca5a8cd8",
      "72ab12533d3e77e7412252c6214f5b43ea07db23",
      "75c792d413c70cbe92d1c140fa24c1c65b9b382c",
      "0094a92fa931fbf8f172c778e4ab6993b2afcc32",
      "4de7a5f6686ef75b5671789ad575eaa091fd938c",
      "396322be1e3d64e57f461b7c4f061df573efcb76",
      "d8374e143367ae0c667bf23512660ffc4588bd44",
      "e26cafa931c3455c94a0eb953315c0f452c6ac28",
      "3660518110486fc01691ba153f5a5b6266ee5f5a",
      "13019f5904e7abd30c48edd5a035b586c74af818"
    ],
    "message_list": [
      "fix: allow StructureData without specified cell\n\nSince StructureData is supposed to support storing non-periodic\nstructures, it should also support not specifying a cell.\n\nThe default [1,1,1] was both not distinguishable from an actual cell\n[1,1,1], and dangerous because it could give the impression that a\nstructure was ready for a periodic calculation when in fact no cell had\nbeen specified.\n\nThe new default `None` clearly signals that no cell was specified.\nA check is added that will raise when specifying periodicity without\nspecifying a cell.",
      "add test",
      "fix get_dimensionality",
      "fix get_cell_volume",
      "fix pymatgen test",
      "incorporate sugestions",
      "reducing diff",
      "Merge branch 'develop' into issue-5248-ase-cell",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "further fixes to tests",
      "fix tests",
      "fix cmd",
      "fix validation",
      "Merge branch 'develop' into issue-5248-ase-cell",
      "fix tests?",
      "minor fixes for failing tests",
      "fix error raised",
      "Merge branch 'develop' into issue-5248-ase-cell"
    ]
  },
  {
    "pr_number": 4577,
    "commits_list": [
      "a5717e8c71f282dbe33a9dbd77a8f60ee1380f99"
    ],
    "message_list": [
      "Fix wrong raise from dict nodes\n\nWhen accessing an attribute as a dict (`dictnode['key']`), the error\nraised was an AttributeError instead of a KeyError, which is rather\nunusual. This has been fixed and a test that checks the correct error\nraise has been added.\n\nCherry-pick: 87d7a60fa3ba9077aefac954dca696385196923f"
    ]
  },
  {
    "pr_number": 2694,
    "commits_list": [
      "343825547e8a3ffffdc757429f2173a071cffeee",
      "5c998780d8be43b34e0047a2680b980c50b9ef76"
    ],
    "message_list": [
      "Formalize the mapping of port namespaces to link labels\n\nThe `Process` and `ProcessNode` duality forces us to define a mapping\nbetween the nested port namespaces of the process and the flat link\nlabel hierarchy of the corresponding node in the provenance graph.\n\nOriginally the nested namespace were imploded into a single string,\nusing a single underscore to concatenate the various port names. This\nmade it however impossible to distinguish a link label that came from a\nnested namespace or simply from a single port that contained an\nunderscore. Given that underscores are allowed in valid python\nidentifiers we don't want to forbid the use of those. Instead, we\nchange the nesting character to be a double underscore and forbid a port\nname if it satisfies any of the following rules:\n\n * Contains more than one consecutive underscore\n * Starts with an underscore\n * Ends with an underscore\n * Is otherwise not a valid python identifier\n\nWith these restrictions, mapping the flattened link labels back onto a\nnested namespaced dictionary becomes an injective function.",
      "Implement a generic `ProcessNode.get_builder_restart`\n\nThis new method will be able to return an instance of the `ProcessBuilder`\nclass that is already configured to run the same process that created\nthe process node on which it is called. It also prepopulates the builder\nwith the inputs of the process node. This makes it trivial to relaunch a\nprocess with slightly different inputs starting from the original node.\n\nTo facilitate this functionality, the `ProcessBuilderNamespace` has been\nchanged from a normal `Mapping` to a `MutableMapping`. This does block\ncertain mapping methods, e.g. `values()` and `items()` if those also\nhappen to be the names of ports in the process specification but if the\nport value setters had to be namespaced, e.g. `inputs`, the use of\nnamespaced ports would become very ugly:\n\n  `builder.inputs.name.inputs.space.inputs.value = 1\n\ninstead of\n\n  `builder.name.space.value = 1`\n\nInstead, now one can simply cast a builder to a dict and all the mapping\nmethods will be once again available:\n\n  `dict(builder).values()`\n\nThe `update` method is very important for the `get_builder_restart` and\nso it has been implemented as `_update` in order to not block a process\nfrom having a port with the name `update`.\n\nFinally, the ad-hoc implementation of `CalcJobNode.get_builder_restart`\nhas been simplified by calling the parent and then simply setting the\noptions in the metadata namespace."
    ]
  },
  {
    "pr_number": 4216,
    "commits_list": [
      "5eaa2aa19457a4b0c2353254e3cf2d80074054df",
      "b341c2716d575ada1b09ecfdc7bfc7bf6337fe2e",
      "3eb705588095723d1ce0960ee171c76bd0030069",
      "57c4e8b0b6f972e878ac7977343f9c8fc72b1c82",
      "fe4e5ffd4d5e4a7bd41f3015ca468a30cb1c7398",
      "9c6122a1e39abb766479de47d7f213275069894c",
      "409bbd47d6dc6d6bcf0f8698bfbfa530b5c72b95",
      "9a1dbaaf8e61db505301d93bed1090cc0e643c1e",
      "0957650e640a443efb04d6f12e5c7f652bd95ba7",
      "dfd3622946be748dfb9684e8ceca8979d7e09db8",
      "62ee7b05208f7fe98d922794bce87f7af7537145",
      "1827a99672bb0ec1576df74c11b967a69ca5d72e",
      "e18a9e9c83003dbc48f6a5649ed90ea140d8a9f3",
      "eec8bf9aba22863130af38e33bea21425ec15a58",
      "9f45705f5ecb331128fc7bf275bcdb782c98e86c",
      "a69bfa3adb66a51810c8c9981172d711f5ebc354",
      "5b1d185220fd542b165c5e933cdb558bd936055a",
      "6696cf31f61d72421ca118960bd0af797a1e4647",
      "cce7226a554e361cc31184ada23d756b5b22b4c9",
      "c798b139351b7647f8e0e03d3f68d416564763f8",
      "c5b5b1bed7263b921acd53819a5054c41dc4105f",
      "460f01f75411df9c3af101f246248e11fd6a5b4c",
      "cd5bda7cc2dda0ffbc0eef8f4dbad20c18325d2f",
      "af46bc8444cb645347cfc719499d89b139441be8",
      "e3fcadfec6c467f7ae1d320e02b3824ca4b36e87",
      "cc7f351e4f63648a5303a503ba60ef8fecc809d2",
      "b1a38e5515d3845463396fdb1adc68c82e446a44",
      "59fae04f5d766dc303c5921b2fe990693f306665",
      "fe2418cf55b6a54e5af39e9ae072929867228505",
      "e6234e154b65a3a0054e5a70453faf9a8eeddc4d"
    ],
    "message_list": [
      "Add computer and code setup how-tos",
      "fix typo",
      "pre-commit fixes",
      "change heading levels",
      "fix list indentation",
      "improve heading",
      "Use definition lists for CLI options",
      "Add first of @ltalirz review suggestion",
      "fix docs",
      "Add 2nd @ltalirz review suggestion",
      "Add ssh @ltalirz review suggestion",
      "Add name change @ltalirz review suggestion",
      "fix docs",
      "merge computer/code setup",
      "Add installation @ltalirz review suggestion",
      "fix doc references",
      "fix toc reference",
      "remove old apidoc location",
      "Apply suggestions from @sphuber code review\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "Add @litarlirz suggestion",
      "Apply suggestions from @sphuber code review",
      "Apply suggestions from @sphuber code review",
      "merge lists",
      "update scheduler plugin help",
      "Update __init__.py",
      "fix tests",
      "remove default",
      "Update aiida/transports/plugins/ssh.py\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/cmdline/params/options/commands/code.py\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "Apply suggestions from code review"
    ]
  },
  {
    "pr_number": 4831,
    "commits_list": [
      "b1c05b74cd134b6d188c4fea73b4d66210118f7d"
    ],
    "message_list": [
      "\ud83d\udc1b FIX: Add `broker_*` parameters to default test config.\n\nIn the `TestManager`, set the `broker_*` parameters by adding\nthem to the `_DEFAULT_PROFILE_INFO`.\n\nThe default values were copied from the config schema. They could\nbe extracted directly, but since some keys (e.g. `AIIDADB_*`)\nare named differently in the `profile_info` compared to the\nconfig schema, this may be unreliable.\n\nPreviously, these were set to `None` by default, which caused the\nconfig validation to fail because only string values are allowed."
    ]
  },
  {
    "pr_number": 3966,
    "commits_list": [
      "000e2a86a8018dfe597818c602dc189b9d43ad27",
      "2beddb4c9269727282fa4962b88134bc1938c21b"
    ],
    "message_list": [
      "Docs: add a Frequently Asked Questions page\n\nCurrently, the new FAQ page mostly focuses on common problems that\nusers encounter and direct either towards the mailing list or create\nissues for. This FAQ page should become the central place to redirect to\nin case of these common problems.\n\nBesides the FAQ page itself, there are some other minor changes:\n\n * Improve `update installation` instructions to include the necessity\n   of running `reentry` scan, and to be even more clear that the daemon\n   should absolutely be stopped before upgrading, after having finished\n   all running processes first.\n * Include link to FAQ page in the README\n * Remove outdated section from the Troublehooting section, since it was\n   about Postgres versions that are no longer supported.",
      "Address PR comments"
    ]
  },
  {
    "pr_number": 4400,
    "commits_list": [
      "0698e19a7e83e0e4d4a736d91da31fc03e60e406"
    ],
    "message_list": [
      "Replace old format string interpolation with f-strings\n\nSince Python 3.5 is no longer supported, format string interpolations\ncan now be replaced by f-strings, introduced in Python 3.6, which are\nmore readable, require less characters and are more efficient.\n\nNote that `pylint` issues a warning when using f-strings for log\nmessages, just as it does for format interpolated strings. The reasoning\nis that this is slightly inefficient as the strings are always\ninterpolated even if the log is discarded, but also by not passing the\nformatting parameters as arguments, the available metadata is reduced.\nI feel these inefficiencies are premature optimizations as they are\nreally minimal and don't weigh up against the improved readability and\nmaintainability of using f-strings. That is why the `pylint` config is\nupdate to ignore the warning `logging-fstring-interpolation` which\nreplaces `logging-format-interpolation` that was ignored before.\n\nThe majority of the conversions were done automatically with the linting\ntool `flynt` which is also added as a pre-commit hook. It is added\nbefore the `yapf` step because since `flynt` will touch formatting,\n`yapf` will then get a chance to check it."
    ]
  },
  {
    "pr_number": 4990,
    "commits_list": [
      "93092b206761d49dd9aa4733233816cd232cef13",
      "247d312d61afe6e05a974801d9b2aca0c83663e4"
    ],
    "message_list": [
      "Default options for codes info setting when multiple codes involved\n\nImplementing `CalcJob` plugin with more than one code got incorrect\n`codes_run_mode` option check and raise. And the `metadata.option.withmpi` was\nnot able to be read and set for the codes when more than one code\ninvolved in the `CalcJob` plugin.\n\nIn this commit, the `codes_run_mode` has the default value set to `SERIAL` mode.\nAs for `withmpi` option, we get the value that was set in the inputs of the entire\nprocess (by `metadata.option.withmpi`), which can then be overwritten by\nthe value from the `CodeInfo`. This allows plugins to force certain codes\nto run without MPI, even if the user wants to run all codes with MPI whenever\npossible.",
      "Merge branch 'develop' into code-run-mode"
    ]
  },
  {
    "pr_number": 5364,
    "commits_list": [
      "b10247f0ddd7d32289095d4dd998bae6b0521a63",
      "8dc3d8a883c2cdf427b7dd9faf070731f4cfa135",
      "e58f51f1f29b37f75f1b0c57c9d1ded7a9518752",
      "18e1515bba231cb8c05377dbb0ac028337c5faec"
    ],
    "message_list": [
      "\u267b\ufe0f REFACTOR: `orm/implementation/sqlalchemy` -> `backends/sqlalchemy/orm`\n\nPreviously, this was possible,\ndue to django loading the database connection on install.\nBut now, the full `psql_dos` storage backend implementation\ncan be consolidated in a single module.",
      "\u267b\ufe0f REFACTOR: rename `sqlalchemy` -> `psql_dos`\n\nThis brings it inline with the actual backend name",
      "\u267b\ufe0f REFACTOR: `aiida.backends` -> `aiida.storage`",
      "\u267b\ufe0f REFACTOR: `backends.Backend` -> `storage_backend.StorageBackend`"
    ]
  },
  {
    "pr_number": 3358,
    "commits_list": [
      "1c33ec734ec0b39d5196bced21b0d61c155d234a",
      "a791c4db1f78edc9cf3d5d87dd115e75e10fa482"
    ],
    "message_list": [
      "Change `verdi computer list` to show unconfigured machines by default\n\nUnconfigured machines used to be hidden by default, like disabled ones,\nand could be displayed by using the `--all` flag. However, this was\nshown to be unintuitive for new users who after creating a new machine\nwith `verdi computer setup` would not see it in the output of the\n`verdi computer list` command.",
      "Add `-u/--user` option to `verdi computer list`\n\nThis allows to show the computers configured for another user then the\ncurrent default defined in the configuration file."
    ]
  },
  {
    "pr_number": 2458,
    "commits_list": [
      "6705be18ff4ceef0f8b020198ceea1e295122104",
      "6900d65f417687efd874af705483660b9b70b2c1",
      "9e2726f1dbe52645a3b51e6af7b34c4ef1f84db0",
      "5b43a3f8f043412601b86951b7cbbe3263698545",
      "deb1ac4c002bae707e5f6758e027af077ea13185",
      "3d21a2ed71e491c1f67a3d73015fee36e9cbc920"
    ],
    "message_list": [
      "added limit filter in rest api tree endpoint",
      "updated tree_limit parameter with in_limit and out_limit in iotree rest endpoint",
      "tree endpoint: returned total no. of incoming and outgoings",
      "Merge branch 'release_v0.12.3' into iotree_limit",
      "minor changes",
      "iotree rest endpoint: removed default limit"
    ]
  },
  {
    "pr_number": 3623,
    "commits_list": [
      "f74311414d76355042b1fa28b51040baeb3819fe",
      "44413d8dbdf2c16cc98f2bf1bdcd8f059a08c9e4",
      "6bd8b2a1901aa7ee74e7b333fcb65e8a03f68aca",
      "2c18ba01a45351383c90d005ae04c7af7ef78397",
      "27ed4828aa67d90666e34bba437f0e749a3b2806",
      "b63281ec3887570082a0af6b77b477c7cf0e6fc3",
      "e616d7e988df512f86cab58c8f5d7e4f76dca84e",
      "29ece242256e8d4a7b466232eefc815edf12d137",
      "ddf30487afb85c086d810f3360de1133e1a6c500",
      "8829b46766e8366f612814c37594dd67c374e224",
      "d9ff8accbeedee25c42125894f9ed9fee189a1f5",
      "da7ab3fa1fcde2b5c0f645e9ebfd26130c536563",
      "445880a9d92f2c8e31812abefd00a5b5304d84fc",
      "023583bdfa13b8a4971a33633ad7e6c450a8ea61",
      "f32d03cd048e35f9706fb06cb20384cbeb5fd005",
      "290d5ac3f3438e0442ec4e642a33e393d12a46ec",
      "340fa3e2a1468e8cd90eb80263f70103942cfd6b",
      "39f0800fc42b88b489155647a0c2e2a3389ed6ed",
      "00e0d631e30a8d3b1d4c8d003236fd2673ef8420",
      "10a958313ca160ef947207b2d07ab5325d1f1388",
      "7e60feb99577343182a2fcf3fe7ecfb312e486ca",
      "ef4a659fc73b3f3b6393c449b7a3f81510722ffd"
    ],
    "message_list": [
      "Add 'verdi node repo cp' command.\n\nThe command takes a node pk / uuid, zero or more file names, and an\nouput directory as arguments. By default, it will create a sub-directory\nwith the node uuid in the output directory, and copy the files with\nthe given names from the repository to that directory. If no file names\nare given, all objects in the repository are copied.\nThe command has two optional flags:\n --no-uuid: copies the files directly into the output directory, without\n            uuid prefix\n --force: Copy files even if they already exist at the target destination",
      "Change name and options in 'file copying' command.\n\nThe command is renamed to 'verdi node repo dump'. The option\nof specifying specific file names to be copied is removed. The\noutput paths no longer prefixed with a UUID. When a file or directory\nexists already, the command prompts to either abort, skip the current\nfile / directory, overwrite the file / directory, or (in the case\nof directories only) merge the contents. A 'force' option can be\ngiven to always overwrite files / directories.\nTests are not yet updated to the changed interface, since we want\nto get feedback on that first.",
      "Apply suggestions for simplified prompt wording.\n\nApply suggestions for simplified wording of the prompts that\nask if a file / directory should be replaced / merged / skipped,\nor the operation should be aborted.\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Add 'verdi node repo cp' command.\n\nThe command takes a node pk / uuid, zero or more file names, and an\nouput directory as arguments. By default, it will create a sub-directory\nwith the node uuid in the output directory, and copy the files with\nthe given names from the repository to that directory. If no file names\nare given, all objects in the repository are copied.\nThe command has two optional flags:\n --no-uuid: copies the files directly into the output directory, without\n            uuid prefix\n --force: Copy files even if they already exist at the target destination",
      "Change name and options in 'file copying' command.\n\nThe command is renamed to 'verdi node repo dump'. The option\nof specifying specific file names to be copied is removed. The\noutput paths no longer prefixed with a UUID. When a file or directory\nexists already, the command prompts to either abort, skip the current\nfile / directory, overwrite the file / directory, or (in the case\nof directories only) merge the contents. A 'force' option can be\ngiven to always overwrite files / directories.\nTests are not yet updated to the changed interface, since we want\nto get feedback on that first.",
      "Apply suggestions for simplified prompt wording.\n\nApply suggestions for simplified wording of the prompts that\nask if a file / directory should be replaced / merged / skipped,\nor the operation should be aborted.\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Adapt test cases to modified interface of the dump command.",
      "Merge branch 'add_node_repo_cp_command' of github.com:greschd/aiida_core into add_node_repo_cp_command",
      "Merge branch 'develop' into add_node_repo_cp_command",
      "Add check that stderr is empty in 'node repo dump' tests.",
      "Fix and simplify '--force' handling, add test.\n\nThe code was currently broken because not all flags were changed\nfrom 'o' to 'r' (in the rename from 'overwrite' to 'replace').\nAdds a test which runs with '--force' where a single file is\nreplaced.",
      "Do not catch exceptions when running commands through click runner.",
      "Fix compatibility with Python 3.5\n\nWhen passing a pathlib.Path to 'shutil', it needs to be explicitly\ncast to 'str'. The compatibility of 'os' and 'shutil' with pathlib\nobjects was added only in Python 3.6.",
      "Clean up implementation of _copy_tree.",
      "Merge branch 'develop' into add_node_repo_cp_command",
      "Merge branch 'develop' into add_node_repo_cp_command",
      "Merge branch 'develop' into add_node_repo_cp_command",
      "Merge branch 'develop' of https://github.com/aiidateam/aiida-core into add_node_repo_cp_command",
      "Simplify 'verdi node repo dump' command.\n\nInstead of prompting when a file / directory exists, we now only\nallow specifying an OUTPUT_DIRECTORY that does not exist, and\nsimply abort the command if it does.",
      "Merge branch 'develop' into add_node_repo_cp_command",
      "Merge branch 'develop' into add_node_repo_cp_command",
      "Improve docstring and abort message of 'verdi node repo dump'\n\nAdd a note that the output directory should not exist to the doc-\nstring. When it does, abort with the 'echo_critical' provided by\nAiiDA instead of the click default."
    ]
  },
  {
    "pr_number": 4013,
    "commits_list": [
      "7aab5787ed9b60b22e91e823e9682dcfda95cf10",
      "1270c62b98b26d8b74e6a8d46b4d2df8a74c9eef"
    ],
    "message_list": [
      "Docs: add scaffolding for the \"Reference\" and \"Internal Architecture\" section\n\nCo-Authored-By: ramirezfranciscof <ramirezfranciscof@users.noreply.github.com>",
      "Docs: move auto-generated CLI docs and REST API reference\n\nThe REST API reference is not yet auto-generated, like the command line\nreference section, but is moved to the \"Reference\" section nonetheless.\n\nCo-Authored-By: ramirezfranciscof <ramirezfranciscof@users.noreply.github.com>"
    ]
  },
  {
    "pr_number": 4448,
    "commits_list": [
      "bb888253024e68c1ee53cfcc9ea9f5f2be591a93",
      "124c6f6b52489b6133444448d6d7fc5b3769c4eb",
      "8bc6116e0daf609a92b374cc025c4752f0bb6990",
      "2093194244b2f2b22ee28fe72007f01b5cc5ed3b",
      "797b1b9e3d0749c0a0f0608210ed03cae9f4a9e9",
      "df2bcb8398c602db2635a7e7c7996aa0236e1e54",
      "cb5d1c2fba4f4b59ed3feb3b094d298f3e8af604",
      "3653d82de8c985c8fd8b9e640693e653f48d2879",
      "0c8d122f51d2f82926c3d213261d0f3fae8e6c74",
      "b3f072d4b63a87e10157d76dc968a13f2543058c",
      "6f9ddd51246da12a5af93f29eb4e6cf0f8beb43a",
      "b689f5e6986c15ab18ad225e5659f1ef3bf8f9c0",
      "06bf80f854541bca18033999cbb405e1cba885a4",
      "0121c49d300c322677994e3614efdf4e2668d655",
      "7b7931ea0fe0683e27e868f685d2ba001ebbd589",
      "e564a59cc7abfea1f50881edfad6d44dcd352c06",
      "9b9c0c9752b30d3ada99b7e52378b6b6565b8a5b",
      "309ef94bc914b492ee2e3523276839773313ab3a",
      "6f6b3e042c83e5cbe3e27f802af29245f4467c8b",
      "9151ab345ebeabe792fbf0aa66886df4e6732cd8",
      "aa2144bb0b84f198c532363a31300e5ffdefaf7b",
      "0a8d177f819527a9d29a6fef5141164bd7e79d33",
      "7206fdbb56b6327ad7d0b2b0b99f10cf7be28000"
    ],
    "message_list": [
      "\ud83d\udc4c Improve archive export  code\n\nThis PR is part of a set that aims to decouple the generation of data to export, from the actual writing of the archive; as a prelude to introducing a new archive format.\n\nThis particular PR does not introduce any breaking changes, it:\n\n- splits the amazingly complex `export_tree` function into a number of separate, self-contained, functions\n- adds mypy compliant typing\n- fixes an bug whereby, in python 3.8, `logging.disable(level=` has changed to `logging.disable(lvl=`",
      "\u267b\ufe0f Refactor archive exporter\n\nThis refactor fully decouples the extraction of data from the database, from the writing of that data to an archive.\n\nIt provides an abstract writer class, then each concrete writer is identified by a string (this could in fact be made into an entry point)\n\nThis code is backwards incompatible, in terms of the Python API, in that it removes the `extract_zip` and `export_tar` functions, but should still work with the verdi CLI, which only calls the `export` fcuntion, whose signature remains the same.\n\nThe commit also adds a backward compatible install requirement, `dataclasses`, which is included in python core from python 3.7 onwards.\n\nThe tests have not yet been altered to accomodate for these changes.",
      "make back compatible (partial)",
      "reinstate export_zip and export_tar",
      "make keyword arguments explicit",
      "make internal function private",
      "minor improvements",
      "Update __init__.py",
      "make node data an iterable",
      "rename ExportData attributes",
      "Add deprecation warnings",
      "Apply @ltalirz suggestions",
      "reimplement export_tree",
      "Ensure logging level is reset",
      "annotate ArchiveData attributes",
      "make metadata a dataclass",
      "add ExportReport",
      "move export version control to the writer",
      "add docstring",
      "fix traversal summary and make export version property",
      "Update aiida/tools/importexport/dbexport/__init__.py\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "Merge branch 'develop' into export/refactor2",
      "fix doc warnings"
    ]
  },
  {
    "pr_number": 4466,
    "commits_list": [
      "910b29c664b42b125869f77c0bbabdcdcb107e5c",
      "657c3f27e04342d42ab5e190de6106b9a5a2842c",
      "a280e5b1c8f10a978b50419f9b5f63423c30d55d",
      "6f8aa98b898c3b97d6c6d4a8ce3db4a0f7091c25",
      "5408a3bf4a9822df8a552beba7bb2e1cd49f2b37"
    ],
    "message_list": [
      "tox: add env for updating the requirements file(s)",
      "fix pre-commit",
      "Add pre-commit validation of requirements",
      "Merge branch 'develop' into tox/update-reqs",
      "remove the automated PR update GA workflow"
    ]
  },
  {
    "pr_number": 4271,
    "commits_list": [
      "d88fc5e91e0be685311e556d53c35e66bc7a37d9",
      "dd5afaa7a69a8dcb31f3d266a44c0e5099980d3f",
      "d30fa6f09f9a18e463e8e9088b051100b8e81e4c"
    ],
    "message_list": [
      "Add setting option to switch off login shell\n\nFor transports plugins(now we have ssh and local), add `use_login_shell`\noption to allowed user to not use login shell when executing the\ncommand.",
      "Avoid repeat _bash_command and gotocomputer_command\n\n`bash_command` are used in three places and set by `use_login_shell`.\nEncapsulate it as attribute `_bash_command_str` of `Transport`.\n\ngotocompyter_command have same string for local and ssh transport\nplugin, Encapsulate the string as `_gotocomputer_string` in `Transport`\nclass.",
      "Merge branch 'develop' into fix/issue/2978"
    ]
  },
  {
    "pr_number": 3905,
    "commits_list": [
      "af53bf9322c3b7c3fc15d3b572c3884c29e2978a",
      "e9066d76f2785dba9e1cab5b04737c413fb1a345",
      "8c019338290682c931d1c63bb96042f8063fc886",
      "863f5634b220be3b995b74e143dabcfc0ee74a4d"
    ],
    "message_list": [
      "Add apt-spy2",
      "Add apt-spy2 to all CI workflows",
      "Use the more stable apt-get throughout CI",
      "Commit updated list for apt sources\n\nCo-authored-by: M*C*O <mcofficer@gmx.de>"
    ]
  },
  {
    "pr_number": 4873,
    "commits_list": [
      "c35f96348621aab925ba0d0810b7c1addbc77cf8"
    ],
    "message_list": [
      "Fix `IndexError` in `override_log_formatter_context` utility\n\nIn the `aiida.common.log.override_log_formatter_context` utility\nfunction, the formatting of all current AiiDA handlers is temporarily\nchanged for the duration of the yield, and then reset. The problem is\nthat the function assumed that the number of handlers would not change,\nwhich is not necessarily the case. Additionally, it also relied on the\norder of the handlers, which isn't guaranteed either.\n\nThe solution is to simply cache the handlers' formatters as a dictionary\nand directly use those handler references to reset the cached formatter.\nNote that we copy the `handler.formatter` return value because it may be\nreturned by reference and we want to keep the original formatter if it\nis changed during the yield. Note also that it doesn't matter that the\nAiiDA logger may have gained additional handlers during the yield, after\nwe recorded the cached formatters, but that is ok, because we didn't\ntemporarily change their formatter either."
    ]
  },
  {
    "pr_number": 4176,
    "commits_list": [
      "a896fe329fe1cff1249451bdfc91de5ed0d15e93",
      "23e1eb8cf0403780520d2f818e5f5edef904a867"
    ],
    "message_list": [
      "Add minimal 'mypy' run to the pre-commit hooks.\n\nCurrently, only two files are checked: aiida/tools/groups/paths.py\nand aiida/engine/processes/calcjobs/calcjob.py. The option\n'follow_imports' is set to 'skip', because there are a lot of\nissues in other files, mostly because these lack type hints.\n\nFrom here, we can continue expanding the type hints one file at\na time:\n- add the file to 'files' in the 'mypy' pre-commit hook\n- fix all issues that mypy reports\n\nCo-authored-by: Chris Sewell <chrisj_sewell@hotmail.com>",
      "Merge branch 'develop' into add_minimal_mypy_run"
    ]
  },
  {
    "pr_number": 5166,
    "commits_list": [
      "c8a45219265cdb66d24b1d9b63a5fbae72cf6975",
      "ca65a7bf24107d51fec33df796b87c93ee95fbd8",
      "208bd0b588b6f1e3301f4bce23c7e9e09830a629",
      "12c0090f266fd4c914885e3bae532c4d33b7e945",
      "d647fd291fd567415ccf871533f5ae2d247c94d8",
      "6224bcfd78901bd7ebb75455b86c72f636815840"
    ],
    "message_list": [
      "(WIP) Adding capability of downloading json files from repository",
      "Merge branch 'develop' of https://github.com/aiidateam/aiida-core into develop",
      "Added dosctrings for the _prepare_json function in array.py\nAddresses #2809",
      "Adding dummy instances for the subclasses of ArrayData for their tests.",
      "Fixing minor issues with the pre-commit\n\n[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nRemoving unneeded dependency",
      "Added ignore_nan to the json.dumps so that any NaN in an array is transformed to null as to be JSON compatible."
    ]
  },
  {
    "pr_number": 5350,
    "commits_list": [
      "a8348d6f082179a8313de038514e19155349ceae",
      "0d80c85db7add75716eae5c2f965a62292eef599",
      "121e947ab3dacec32970ad8930b3695acf2e0111"
    ],
    "message_list": [
      "Add extra TemplateCodeInfo class to hold codeinfo for JobTemplate\n\nIn the current design, the code_info of calc_job is read from the\ncode setup and plugin then pass to the job template to create\nthe bash script. However, job template needs more flexibility to\ncontrol the different part of script runline where currently all\nthe part\n  - exec_name from code uuid,\n  - code_info.cmdline_params,\n  - mpi parameters from computer setting\n\nare stacked together to the job template's code_info. In this PR, the class\n`TemplateCodeInfo` is created to handle the elements, where the `code_uuid`\nand `withmpi` fields are not used in job script generation.\nThe code_info of JobTemplate and of `CalcJob` are decoupled\nfrom each other and lead to more flexibility.",
      "Merge branch 'develop' into design/tmpl-code-info",
      "Merge branch 'develop' into design/tmpl-code-info"
    ]
  },
  {
    "pr_number": 4719,
    "commits_list": [
      "c3374635286c8455aa446fbd4b74cf8aedc698d9",
      "12ad449088d581b5afe11056d95ed0375e168c5e",
      "ae979e05106457180bb3fba51f023f708e62a5fc",
      "c0f9513994788bb54cea3e257c5be63dcbd25167",
      "50541b54f1bfba1b4e205bb55025fc96fcadbb36",
      "e88ba6a661b8876c7b299b3026700e91aa16f02d",
      "c7d792ac900fc2ba5893cf3c047fb65606514a45",
      "8183bca31b34015b3f7c725a9a9df78ee796df3c",
      "57b1777a67f262bd5242b5364f46ee09321301b6",
      "ddc74d03a0e35221a29300d40738c5fcc22a0c7b",
      "e5bf51b0528c7364950ed08e14a7d0513e0841be",
      "87f1e1c42c58e4999ca4fef616827b61e1498078"
    ],
    "message_list": [
      "np.int is deprecated alias of int",
      "put_object_from_filelike: force is deprecated",
      "archive: stop using deprecated `silent` keyword",
      "fix pytest ignore",
      "more fixes",
      "dbmodel: keep using \"name\" for computer",
      "more restoration computer label=>name",
      "Fixing the repository-writing deprecations\n\nA few tests were writing directly to a node's repository folder, after\nthis got stored.\nWhile this still works, this is now deprecated and will not work\nanymore in 2.0.\n\nHere I'm replacing all times we use `.open` with `'w'` or `'wb'` mode\nwith a correct call to `put_object_from_filelike`, calling it\n*before* the node is stored.\n\nIn one case, the data comes from a small archive file. In this case,\nI recreated the (zipped) .aiida file adding two additional (binary) files\nobtained by gzipping a short string.\nThis was used to ensure that `inputcat` and `outputcat` work also\nwhen binary data was requested. Actually, this is better than before,\nwhere the actual input or output of the calculation were overwritten\nand then replaced back.\n\nThis commit fixes #4721",
      "communicator: stop() => close()",
      "Removing additional deprecation warnings\n\nTypically AiiDA deprecation warnings for tests of currently\ndeprecated features.\nI also removed a few more warnings that should not be printed,\nas well as enabled the ResourceWarnings that mention if you are\ne.g. opening a file and not closing it.\nI fixed also the few occurrences I was aware where this was\nbeing done - there might be more.",
      "Fix a few bugs while removing some warnings\n\nA couple of files were opened and not closed\nMaybe this was partially related to #4307 that closed\nas a \"tentative\" fix of #2386, but maybe a few more\nopen files were remaining?\n\nAlso, I'm adding a return status to most of the verdi daemon\ncommands, so tests will catch possible errors (they weren't being\ncatched in my own setup).\n\nIn addition, I'm also silencing some additional warnings that should be\nsilenced (the main point of this PR).\nNote that while unmuting in general ResourceWarnings was good to spot\nsome issues (see bug described above), there are a couple more that now\nissue it and are not obvious to fix (typically related to the daemon\nstarting some process in the background - or being started itself -\nand not being closed before the test actually finished).\nI think this is an acceptable compromise - maybe we'll figure out\nhow to selectively silence those, and keeping warnings visible will\nhelp us figure out possible leaks in the future.",
      "Addressing review comment"
    ]
  },
  {
    "pr_number": 4996,
    "commits_list": [
      "c3424289dba0d140d896565a94af95722f182f10",
      "e05ef2d9c25bcc17ad812cbba23df88e13ea73fe"
    ],
    "message_list": [
      "Pre-commit: move `yapf` and `mypy` config to `pyproject.toml`\n\nThe libraries `yapf` and `mypy` support the `pyproject.toml` file for\ntheir configuration starting from v0.31 and v0.9, respectively.\n\nCo-authored-by: Jason Eu <morty.yu@yahoo.com>",
      "Merge branch 'develop' into fix/yapf-mypy-config"
    ]
  },
  {
    "pr_number": 4863,
    "commits_list": [
      "e8b11c9660c1b2927ac075b5fef0352118eecae3",
      "966871745f7468b2fe202cd3ee2d9abfc8e82d39"
    ],
    "message_list": [
      "FIX: respect nested output namespaces in `Process.exposed_outputs`\n\nThe `exposed_outputs` method was detecting nested namespaces in the\ndictionary of outputs of a node by using the namespace separator\ncharacter of the port namespace class. However, this character, which is\nthe `.`, gets converted to `__` when the link is stored in the database.\nThis transformation is necessary since the `.` is a reserved character\nto enable attribute based dereferencing, e.g.\n\n    node.outputs.some_output\n\nSince link labels can contain underscores, we use a double underscore\nwhich, together with the guarantee that link labels don't have leading\nor terminating underscores, can uniquely determine the namespaces in any\ngiven link label.\n\nThe solution is to not manually reconstruct the namespaces in the output\ndictionary but simply use the `get_outgoing().nested()` method which\ndoes it for us.",
      "Merge branch 'develop' into fix/3533/exposed-outputs-nested-namespaces"
    ]
  },
  {
    "pr_number": 4353,
    "commits_list": [
      "1215406190aaeef35e3c12fd3e7379009d01672f"
    ],
    "message_list": [
      "Bump base docker image version.\n\nThe new image is based on the latest stable Bionic Beaver (18.04)\nUbuntu distribution, which contains fixes for critical vulnerabilities.\n\nAdditionally, this image contains a fix for `ruamel.yaml` package\ninstallation."
    ]
  },
  {
    "pr_number": 4504,
    "commits_list": [
      "01ad67aeae001cbe6954c80f30c533319d6773b7",
      "a3356a041be77cfe94084ce55ee2711e9a6efff8",
      "3ec26d3496c43f98119bbe34713e5ca1bf0f771e",
      "9fca8ce3ea1510846435fd1c55d4c163dd9c77d9",
      "4d3d552a28f18392ee748b4ecbefa9365354fb5a",
      "a54c319a913fbe80808255873b29e25f28fcde35",
      "9e552b5730c6373c9c5ecdfe145e8864f5cdaaca",
      "b853e8c51d97fddda4200f0a07a5ffbc6994edbe",
      "19da2ad7ef974de40cd9fae914d00ca5ca69192d",
      "81f2118c7abd93602cdc18e3f6e252fe380a7cca",
      "16744d3ffd95097bc40b39d535863cd45ef50401",
      "286e8656e42ce18cc5c93f5b3cacf2b55edc0528",
      "78e9e7989952db4f8dc56da72793494901136cd6",
      "1ab807a9c5739231dd090f7a88d5571beb9e6ffc",
      "8d0afa16a8bd83bf80763278b4febaa9b0d088b7",
      "694fb4762e7e2b595502dea368a631d772f7b441"
    ],
    "message_list": [
      "Refactor export progress bar",
      "fix doc warnings",
      "move prograss_context injection to the writer init",
      "Allow the tar archive to be created independently of an aiida repo",
      "Merge branch 'develop' into export/progress-bar",
      "re-re-factor progress bar!",
      "update help text for verbose option",
      "small update",
      "Move control of logging verbosity to CLI",
      "Move log formatting control to CLI",
      "Move partial to set_progress_reporter",
      "Apply suggestions from code review",
      "fix description",
      "rewrite progress reporter as class",
      "minor fix",
      "fix accidental change (whilst testing)"
    ]
  },
  {
    "pr_number": 5192,
    "commits_list": [
      "6d759a59afb136fddd7e98509a092412aaef9ce8",
      "5a7467da2e699c8e3ff24ba431e92348d9a36958",
      "659673f9dce55e136bbfe8436d34738016938fa8",
      "e44f0f8964d6d70f536849669997d2a023d151b2",
      "83a4e6f891ab3771427f69d516dc84aebae713a1",
      "5c42be7f2d9a6cddbb4c2ed1f4ef2b4775a8727b",
      "d38271840a4e38384de42dd15c509b067b1c8998",
      "e3edbe455a86d6c8909bc9851eb03554f19f363d",
      "6483b088bbe607feebd496b048eed59c8b87007a",
      "962d4fbfe7df6335bb9907bc0e6d769e62991616",
      "aee33ae72743220e748d6d117a8fb5138da9fefb",
      "61e75627c5b1bde0f4f734135ff692d35624277a",
      "fc9d4da32052e1ed84026dd38305d197ddd4a93c",
      "bd326c44800cf7a88c8ea5f65ef21d5e6e44cdea",
      "ce48ab4d9bfb52cd781bb4a48fdcd6c24a7b973c",
      "f1efe48b7170cb455a6f19f1535a919a58609f4d",
      "acea111979f9d95fb4f8f7a9752186da971e4b64",
      "24a871e3bc95b4c47824a29429f43759e71a66d8",
      "2bd39eb8bd3a7b7e58f69092b8863490e67b3d21",
      "59d6d03cdb87a9a7bd24fd61706000d39202bbfa",
      "f2bc475abb56f67375c4e1aa4c4badc9ccf70824",
      "f725fbea5744aa42168ba0ac3a535f788e032dde",
      "2b988a8f80c777ab73a5c23ff379433d188374d7",
      "f1301aa2b017dd11de6c7d30a5c5af5857f78d09",
      "cbc337abf56f0ab9c8fdd0df5dd498d1cf36c504",
      "cf4b10de1118c5b5c95c862530c0738ce58a1a6c",
      "a327b79c154bd44c9f914a4b9b880b686a938b9d",
      "c463220f7c1983a5acfa049bf78bfc6bb930edd6",
      "7733d4543f42c9c4ade4909ca9bc9c018e8e205f",
      "cd534be21c51802c6d507942f45ff1a391d7cff2"
    ],
    "message_list": [
      "\ud83e\uddea TESTS: SQLA Migrations -> pytest",
      "Merge branch 'develop' into update-migrate-tests",
      "create_user -> init_db",
      "Merge remote-tracking branch 'upstream/develop' into update-migrate-tests",
      "Change names",
      "Merge branch 'develop' into update-migrate-tests",
      "Merge branch 'develop' into update-migrate-tests",
      "Update tests/backends/aiida_sqlalchemy/migrations/test_all_basic.py\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "more splitting of files",
      "Merge branch 'develop' into update-migrate-tests",
      "Update module docstrings",
      "rename",
      "test commenting out tests/backends/aiida_sqlalchemy/test_session.py",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "fix stall",
      "Merge branch 'develop' into update-migrate-tests",
      "fix deprecation",
      "add fixes",
      "check",
      "Create test_session.py",
      "check",
      "update",
      "Merge branch 'develop' into update-migrate-tests",
      "Update conftest.py",
      "Update conftest.py",
      "check",
      "Update test_10_group_update.py",
      "check",
      "Update conftest.py",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci"
    ]
  },
  {
    "pr_number": 2107,
    "commits_list": [
      "deb9fa57238eee5f7c1a6cfb65801b0ec35253be",
      "98bb77cba8149b7ceb89d04e65315f4847ba6496",
      "305dffeed552086235e7ee199320371eafc7c2ae",
      "8a376768a1d1e06df6d678a816995a0bec01a42e",
      "9f19530f83e8a26df93a9bf523c44bddf0eba2a8",
      "ddd1293b597be71f421f09160f1b5104ccb55f78",
      "416b71dfe890006ec6ed5eca2063c512f33bbee2",
      "3c8e7adf86e06fac69ce811eaca29cbb6d4340d6",
      "87998bf51ff03b077480b728c121a2adaffcaf60",
      "cd0188eaf1e79e75c9c3508e3542fc3a858e905a",
      "bb3707d8ef1931a7c1a61e7611cc22b926ef18bc",
      "3c8dcaacd4b47cb95db0182100b399ac4ed770cd",
      "1b14477a7fdeace19b5f2e82a5478889fa508de3",
      "c7b86f8ea1bbe1efd55a6573248352cd07b58d0e",
      "1cebc9be3b61c6047f1f202ec4e6445790b928e1",
      "61cadf9edefe7a624ee69f43c20a29200fb3d631",
      "8a8977d69e6a09ae66a262b7494a0562b4e64105",
      "e76c2e2e7a261033e3b2f74aa1006e8ebc9ee5e0",
      "068200d88cfe6611a63fe96376771c777b5ee903",
      "af101478c223db79e6c3648d6d25d68f08c28e71",
      "f752429411e4af35e2d124adbca85f617f9c5593",
      "71a18b7f15bce54ad115ae52ec6457145ce9fcb5",
      "9d8ff7e110bd477011dfbdb3bf4b13673ee1b142",
      "8892555ed1e601d25980b3afca331be3924cacfa",
      "da0e26c5d34be18d2873003c70f971f2ef87cf79",
      "3a38931a9a06bd8a18793ce1461a8fccbf34755f",
      "705545a5a0047a84ad7393585d91a12e4b018304",
      "369dd9fca829606e435e4cf3217914f484e480b4"
    ],
    "message_list": [
      "Changed all open for io.open and specified UTF8 encoding",
      "Convert explicitly set byte strings to unicode when writing",
      "Fix more byte strings",
      "Fix JSON writing",
      "Fix JSON writing",
      "WIP - fixing failing tests",
      "Set some strings explcitly as unicode",
      "json.dumps doesn't always produce a unicode string (e.g. Python 2 with a bytestring as input), therefore we use six.text_type to convert it to unicode and pass it to io.open(demofile.txt, w, encoding='utf-8') which expects a unicode object and then it encodes it utf-8.",
      "Set literal strings as unicode",
      "Change byte string to unicode string for tcod tests; change HiddenPrints() to write in binary mode as we're sending theoutput to /dev/null anyway; remove the encoding arg from folder.open calls in orm/importexport as it's really a call to ZipFolder.open; change cStringIO to StringIO in MyWritingZipFile to allow for unicode",
      "Remove redundant io.open following tempfile open",
      "Remove redundant io.open following tempfile open",
      "Fix some formatting and typos for failing tests",
      "Change remaining instances of json.dump to json.dumps",
      "WIP Replace cStringIO with StringIO to allow for unicode",
      "Fix typo in dumps args",
      "Fix another cStringIO occurance",
      "Add type conversion to unicode",
      "Fix brackets",
      "Add encoding option to YANL to ensure that yaml.dump returns a bytes object in both Py2 and 3",
      "Add json.py helper function to replace system json",
      "Replace json imports with util.json imports; simplfy json dump invocations.",
      "Remove redunant flush() call - the buffer is flushed anyway when exiting the 'with open()' block",
      "Fix variable names from rebase mistake",
      "Add to documentation",
      "Add codec stream reader to decode the incoming streamio object",
      "Catch case where incoming StringIO filelike contains a unicode type already",
      "Improve documentation"
    ]
  },
  {
    "pr_number": 4318,
    "commits_list": [
      "b95e09282289ec1960aa27261acf98bc2b1b1bca",
      "ab639e6ec92d7a0bc2c77c743f2d3a5e499bd668"
    ],
    "message_list": [
      "Make `--prepend-text` and `--append-text` options properly interactive\n\nThe `--prepend-text` and `--append-text` options, used for both the\n`verdi computer setup` and `verdi code setup` commands were normal\noptions as opposed to all other options that are interactive options.\nThe `InteractiveOption` is a custom option class type we developed that\nwill present the user with a prompt if it was not explicitly provided on\nthe command line. The `--non-interactive` flag can be used by the user\nto prevent prompting, as long as a default value is defined.\n\nThe two options in question were implemented differently, presumably\nbecause they take a potentially multiline string, which is not easily\ndefined on a prompt and therefore instead these options would be defined\nthrough a text editor. The prompting of this text editor, if necessary,\nwas however not performed in the option itself, but in the command body.\nAt this point, the prompt cycle of the parameter cycle controlled by\nclick is already over.\n\nAdditionally, since the function checking whether the option had been\ndefined also considered an empty string as undefined, despite it being\nthe default, a specified empty string would still lead to the user to\nbe prompted, even when specifying `--non-interactive`.\n\nThe fix is to make both options proper interactive options just like the\nrest. To this end, we create the `TemplateInteractiveOption` that works\njust like the `InteractiveOption` with the only difference being that it\nuses a file editor instead of an inline prompt. This change does force\nus to have both options pop up their own file editor, whereas before\nthey were joined in a single file and both files were specified in the\nsame file, separated by a header that we defined.",
      "Merge branch 'develop' into fix/4300/verdi-accept-empty-prepend-append-text"
    ]
  },
  {
    "pr_number": 2632,
    "commits_list": [
      "b6c026efd266beecbabfa80a0f18991cd5fc416e"
    ],
    "message_list": [
      "There were problems when overriding the default behaviour of the rules. Moreover\nthey were moved to separate methods to resuce the complexity of the export method.\n\nVarious tests were added:\n- Export tests that are based on a real workchain were added to ensure that a complex\n  and real AiiDA graph based on workchains is exported correctly.\n- Export tests based on a synthetic graph were added that explicitly check every\n  flag that changes the default behaviour of the export set expansion rules."
    ]
  },
  {
    "pr_number": 2737,
    "commits_list": [
      "be694d853b671b87d04598218a18b7b81a9c61fd",
      "85442e88cdfbf0895e7feb00bccbd2b36eaeab43"
    ],
    "message_list": [
      "Refactor the `Orbital` code and fix bugs\n\nThe intended aim of this commit is to avoid breaking backward compatibility\nwhile refactoring the existing code to model atomic orbitals and add tests.",
      "Move `validate_link_label` to `aiida.common.links`\n\nThe function is general enough to justify it being defined in `aiida.common`\nalongside with the `LinkType` enum. In addition, this prevents cyclic\nimports problems between the `aiida.engine` and `aiida.orm` modules."
    ]
  },
  {
    "pr_number": 5111,
    "commits_list": [
      "623ef2b0450f37c54574c3a1d8814b10d7d40fe0",
      "0e33cf5ee3c5b0df250a7c069bc17ac68e4115fc"
    ],
    "message_list": [
      "Dependencies: update requirement to `click~=8.0`\n\nThe entire logic around parameter parsing, including the consuming of\nthe value, optionally prompting and then processing the value has\nchanged. Especially the prompting logic has changed, which now makes the\ncustom logic in our `InteractiveOption` largely unnecessary. It had to\nbe refactored significantly and it now no longer changes the prompt\nbehavior but just overrides certain methods to include the concept of\nnon-interactivity which is not native to `click`.\n\nIn addition to this major change, there were also various smaller\nchanges. The following adaptations had to be made for compatibility:\n\n * Parameter validators now have to return the value, whereas before\n   this was not required.\n * Custom parameter types need to start with checking for the value to\n   already have the expected return type and then return it. This is\n   necessary because the convert method can be called multiple times:\n   https://click.palletsprojects.com/en/8.0.x/parameters/#implementing-custom-types\n * The `aiida.cmdline.params.options.contextualdefault.ContextualDefaultOption`\n   has been removed as it was not used in `aiida-core` nor in any plugin\n   that is hosted on Github.\n * `Parameter.get_default` now takes the `call` argument\n * Remove explicit parameter name from `version_option`, this is now\n   by default set to just `--version`.\n * Add explicit `type` for `MultipleValueOption` options in `verdi run`.\n   This is necessary because without it the entire string would be\n   parsed as a single string and not a tuple of strings.\n * The `ConditionalOption` test `test_prompt_callback` had to be changed.\n   With the old `click`, as soon as wrong input was provided at the\n   prompt and the callback raised, the command would fail. With the new\n   behavior of `click`, the user will be prompted until the callback\n   validates, printing the error message each time. This required the\n   test to be changed to actually pass a valid input at the end through\n   the `user_input` or the test would hang as it was infinitely prompting.\n * The `Path` parameter removed the `path_type` attribute. It has been\n   more or less been replaced by `name`.\n * The `click._compat.filename_ui` utility was removed.\n\nNote that the lower requirement for `click` is set to `v8.0.3` since in\nlower patch versions the behavior of prompts for boolean type parameters\nis different.",
      "Dependencies: remove `click-completion`\n\nAs of `click==8.0` the full set of functionality that `click-completion`\nprovided for our tab-completion is now shipped with `click` itself, so\nwe can remove this dependency. The main difference is that the string\nthat is required to activate it, is slightly different and it is shell\ndependent. The documentation has been updated to correctly reflect this.\nSince it is now shell dependent, the old `verdi completioncommand`,\nwhich was already (unofficially) deprecated is no longer correct. To\nkeep it correct, one would have to try and detect the shell but this is\nclearly outside of the scope of `aiida-core` so we simply remote it."
    ]
  },
  {
    "pr_number": 4697,
    "commits_list": [
      "1d4e29d10b3dba3d1a88eb1c740db77eb3e1a536",
      "2b438ce2e32586d0c7b3bdeaefb7e7d7bb058173"
    ],
    "message_list": [
      "CI: Increase timeout for test-install test suite.\n\nThe current timeout of 35min is regularly exceeded resulting in test failures. This change increases it to 55min.",
      "Merge branch 'develop' into ci/increase-ga-test-install-tests-timeout"
    ]
  },
  {
    "pr_number": 5352,
    "commits_list": [
      "bc661db8f6089a713ed2b162b354579df18cbf94",
      "1f9924602cf446f3ab1e31f4674653cfcd2f1bc4"
    ],
    "message_list": [
      "Factories: do not explicitly check type of entry point if `load=False`\n\nThe recent release of `setuptools==60.9.0` caused the CI to start\nfailing. The `CalculationFactory` started raising, when invoked through\nthe `PluginParamType`, when `load=False` because the loaded entry points\nwere no longer an instance of `importlib_metadata.EntryPoint`. The\nreason for this change is that as of that `setuptools` release, it\nstarted vendoring the `importlib_metadata` package and so it overrides\nthe type to `setuptools._vendor.importlib_metadata.EntryPoint` for the\nloaded entry points.\n\nFor reasons unknown, this behavior of the changing of the type of loaded\nentry points, only manifested when invoking the CLI through `pytest`.\nWhen calling the failing CLI command directly, it would work just fine.\n\nOne solution to this problem would have been to extend the check to also\nallow the vendored entry point type of `setuptools` but this seems\nfragile. Really, it seems that the original design of checking the type\nof the entry point in the case of `load=False` is unnecessary. There is\nno reason to suspect that `get_entry_point` should return anything that\ndoes not behave as an entry point if it didn't except. It is better to\nbe Pythonic here and rely on duck typing. If what is returned by the\nfactory behaves like an `EntryPoint` instance, regardless of its exact\nmodule and class, then that is all that matters.\n\nThe test `tests/plugins/test_entry_point.py` was also explicitly\nchecking the type of the return value of `get_entry_point` but is\nshouldn't as its purpose is just to verify the deprecation warning is\nemitted for deprecated entry point names, and so the check is removed.",
      "`CodeBuilder`: do not rely on type of `input_plugin_name`\n\nThe `CodeBuilder` could receive the `input_plugin_name` keyword argument\nas an `EntryPoint` from the `verdi code setup/duplicate` commands, so it\nincluded a switch to get the entry point name from it.\n\nDue to a bug in setuptools, the type of the `EntryPoint` can change\nunder certain circumstances and so the `isinstance` check on the entry\npoint does not hit. This causes the `EntryPoint` to be passed to the\n`Code.set_input_plugin_name` method, which does not check the type and\ncasts the argument to `str`. However, the `__str__` of `EntryPoint` does\nnot return just the name, but a more verbose representation. This will\ncause the `Code` to be unusable.\n\nThe conversion of the `EntryPoint` to its name in the `CodeBuilder is\nremoved and instead the CLI commands are responsible for passing the\nstring entry point name.\n\nIdeally, `Code.set_input_plugin_name` is updated to only accept `str`\narguments and remove the cast. But this would be a breaking change and\nso is not done for now."
    ]
  },
  {
    "pr_number": 1083,
    "commits_list": [
      "4d5b084914602448c24f1ce6e5f4fbaef0982eea",
      "ffd90328b3e298cae877d101c299d95df463ada4",
      "396d489b6f5e73c774cc9b2cf56064e2015e6159",
      "c44d3f564e177f3311d647987885bee0eaaf2949",
      "417cb9d016e00dd17e55ed872b481fcbc5c508b1",
      "85bf11ee4ddef52c91019737525cca627297f1a0",
      "83b047850d2eca340961b56a3b26d500a628d5a2",
      "4878fadf12684d550f29b6b1c59bed993b01011d",
      "3fcab81fafe0f9aaa5418fdd060895ea48a4a522",
      "5f54b4df880fb0a481b755a86e07dc35aabc195d",
      "c0da5c4c4bc495c485dd1018d8d94bd9dc300498",
      "71cb8dfed2eac78f87fa3addf5c26ab882cce376",
      "c5baf65dca2855520631c007d53091996e0e29fe"
    ],
    "message_list": [
      "For #923 I added the 'verdi node delete' functionality into the command-line interface. A new module in utils takes care of the abstract part (querying of nodes to delete, with the provenance followed in reverse. Backend-specific utilities take care of the deletion in the DB",
      "Added tests that check the node deletion. They work as anticipated, which means that #923 is resolved",
      "Updates SQLAlchemy deletion since there was a bug",
      "Updated documentation, removing the secret script, and updating documentation for the verdi node subcommands",
      "Removed the node-deletion script from the cookbook",
      "Merge branch 'develop' into verdi_node_delete_923",
      "Added more martial warnings for node deletion. Also, rewrote the statement for node deletion for sqlalchemy because it wasn't liked, and actually because the cascading for dbgroup_dbnodes was failing (if node belonged to a group) in sqlalchemy",
      "Merge branch 'develop' into verdi_node_delete_923",
      "Updated command-line options for #923. Also fixed types in comments",
      "Merge branch 'develop' into verdi_node_delete_923",
      "This addresses some requests in #1083, specifically to have a logging mechanism for calculations that lose created data or called instance. Implemented a printout when this happens and storing to DbLog. To be reviewed for further improvements.",
      "This addresses the request for logging made in #1083. Calculations that would lose created data or called instance will get this action written into the log. Additional warnings are printed, but it is still allowed to delete data without its creator or called workflows without their callers, since there are usecases",
      "Merge branch 'develop' into verdi_node_delete_923"
    ]
  },
  {
    "pr_number": 4081,
    "commits_list": [
      "ec682b67bca50feefc80b4cc04c320824d9a4226",
      "5ff9a94b7744c4d0d3062853101e912f312218df",
      "9af7a2ed7d76f2e8ef184c1e58a0e62d5871e88b",
      "35ca5a1d15378740079da7548c95b323828456ba",
      "7e387092f386d552b3c65b9b497c7698b72949ab",
      "8819c0687261b371a5b4e9b2a074d25fa255a814",
      "e7010af3d4a3f446382101d11025abdc18390992",
      "4b743fe6adf349620f5effb5efd91117c08a5d02",
      "e3e233f6b3d70607fca23db70cf468c0414bfbea",
      "84c41e297aa97d857609a36edb3f05ebf3cb7e08",
      "36babd69462bbb0f9645f22aa1b0039eeff0b621"
    ],
    "message_list": [
      "Fix #3718: add border to origin node in prove graph.",
      "Add highlight_classes argument in graph visualization\n\nthe argument highlight_classes is added in method\n`recurse_ancestor` and `recurse_descendants` in\nProvenance Graph visualization for the purpose of\n quickly spotting the desire node.\n\nThe target classes to be highlight are passed to the\nargument as a class name string, it was compared with\nthe label of node. So the node label getting function\nis isolated to be reused.",
      "Merge branch 'develop' into graph-vis-enhance",
      "Merge branch 'develop' into graph-vis-enhance",
      "Update visualising_graphs.rst",
      "Update visualising_graphs.ipynb",
      "Update visualising_graphs.ipynb",
      "Create output_23_0.svg",
      "Update visualising_graphs.rst",
      "Merge branch 'develop' into graph-vis-enhance",
      "fix pre-commit"
    ]
  },
  {
    "pr_number": 2329,
    "commits_list": [
      "99ea5dd204f956abb8b5da57563c9f6ab0d61186",
      "6c525c8b0f59eb890e0aaaff0d18af3940dee434"
    ],
    "message_list": [
      "Work on Groups:\n\n1) aiida.orm.importexport: while importing data from the export\nfile allow to specify user-defined group and to put all the imported\ndata in this group\n\n2) aiida.common.utils:\n   a) remove get_group_type_mapping function which was mapping\nmachine-specific group names with the user-friendly ones\n\n3) aiida.orm.groups\n   a) Add GroupTypeString (Enum) which contains all allowed group\ntypes: data.upf (previously was 'data.upf.family') , auto.import\n(previously was 'aiida.import'), auto.run (previously was\n'autogroup.run'), user (previously was '')\n   b) Remove Group.query and Group.group_query functions, as they are\nredundant. Their functionality can be replaced by QueryBuilder()\n\n3) aiida.orm.data.upf:\n\n   a) Set UPFGROUP_TYPE from GroupTypeString.UPFGROUP_TYPE.value\ndefined in aiida.common.utils\n   b) Replace the usage of Group.query() by QueryBuilder() in\nget_upf_groups and get_upf_family_names functions\n\n4) aiida.orm.autogroup: set VERDIAUTOGROUP_TYPE from\nGroupTypeString.VERDIAUTOGROUP_TYPE.value defined in aiida.common.utils\n\n5) aiida.common.utils: add escape_for_sql_like that escapes\n% or _ symbols provided by user\n\n6) aiida.cmdline.commands.cmd_group.py\n\n   a) Add copy option\n   b) \"list\": replace Group.query() with QueryBuilder\n   c) Add option to show all available group types\n   d) Add defaulf for group_type option\n   e) remove usage of the get_group_type_mapping() function\n\n7) aiida.cmdline.commands.cmd_import.py: add a posibility to chose the\ngroup where all the imported nodes will be put in.\n\n8) aiida.cmdline.params.types.group.py add a possibility to the group\nparameter to create new group\n\n9) dj and sqla backends: in Group (and tests, resapi) replace name\nwith label and type with type_string\n\n10) Add Groups documentation\n\n11) Keep name and type for back-compatibility\n\n12) Improve documentation for django and sqla backends migration",
      "Groups: enforce type_string to be of GroupTypeString type"
    ]
  },
  {
    "pr_number": 4735,
    "commits_list": [
      "d0ccfb0d1d7e84d8a45ac4140d1cf9639c3ffdce",
      "041b78b4e47474e14d733baf0496eee1708505b5",
      "f60c846205e934765cfcde8a28d874063ef60e3d",
      "cf8114e5acfcb82af37198c0d604697e039eceeb",
      "613b911254db5f87ba142a56b90e70cf65cd65c5",
      "e1ec5a37f66c355e65c9ec4a954edc7c5be50431",
      "dd65b445fe1940be8f7b30901b690bc7dfcca1bd",
      "2c13c746df164f327365d182eb154cf118110b72",
      "0d116a51dede7cac0dc9f5463a1dc1b4ea4a88f3"
    ],
    "message_list": [
      "Fix hanging direct scheduler+ssh\n\nThe fix is very simple: in the ssh transport, to emulate 'chdir',\nwe keep the current directory in memory, and we prepend every command\nwith a `cd FOLDER_NAME && ACTUALCOMMAND`.\n\nOne could put `;` instead of `&&`, but then if the folder does not\nexist the ACTUALCOMMAND would still be run in the wrong folder, which is\nvery bad (imagine you are removing files...).\n\nNow, in general this is not a problem. However, the direct scheduler\ninserts a complex-syntax bash command to run the command in the background\nand immediately get the PID of that process without waiting.\nWhen combined with SSH, this hangs until the whole process is completed, unless\nthe actual command is wrapped in brackets.\n\nA simple way to check this is running these two commands, that reproduce\nthe issue with plain ssh, without paramiko:\n\nThis hangs for 5 seconds:\n```\nssh localhost 'cd tmp && sleep 5 > /dev/null 2>&1 & echo $!'\n```\n\nThis returns immediately, as we want:\n```\nssh localhost 'cd tmp && ( sleep 5 > /dev/null 2>&1 & echo $! )'\n```",
      "Adding a regression test for the hanging direct+ssh combination\n\nThis test checks that submitting a long job over the direct scheduler\ndoes not \"hang\" with any plugin.",
      "Merge branch 'develop' into fix-3094-ssh-direct-hanging",
      "Merge branch 'develop' into fix-3094-ssh-direct-hanging",
      "Merge branch 'develop' into fix-3094-ssh-direct-hanging",
      "Update tests/transports/test_all_plugins.py\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "Applied suggestions from code review",
      "Merge branch 'develop' into fix-3094-ssh-direct-hanging",
      "Merge branch 'develop' into fix-3094-ssh-direct-hanging"
    ]
  },
  {
    "pr_number": 5188,
    "commits_list": [
      "367fbcb54a4d234ac6649ea65f466a5385120212",
      "eca43f4947a162dd46da4ac91b58d9c39a222003",
      "ec4ef3a6ae78f4158d02834501af706d66d645de",
      "35201a5988abcfd2b08734d5dd94b5386bd0cadc",
      "bd6bbc829f0a3493b31a4ce119899597b9828be5",
      "7db02091ce708630bcdab6d528bfe8d2718e2368"
    ],
    "message_list": [
      "Add Python 3.10 to the test-install step.",
      "Add Python 3.10 to trove classifiers.",
      "Define all version numbers as part of the GH actions workflows as strings.\n\nSince version \"numbers\" are not actually numbers, which can lead to\nbugs, e.g., when a version number like \"3.10\" is interpreted as a\ndecimal or float and is thus equal to 3.1.",
      "Run CI tests against lowest and highest supported Python version.",
      "Apply suggestions from code review\n\nCo-authored-by: Chris Sewell <chrisj_sewell@hotmail.com>",
      "Automated update of requirements/ files. (#5241)\n\nCo-authored-by: csadorf <csadorf@users.noreply.github.com>"
    ]
  },
  {
    "pr_number": 2416,
    "commits_list": [
      "cc9c531298abc2326153e767d9a3af56bfdf4d8f",
      "79cbf6c30d8fa39cd0d66f87d6edb8a3bc9b7d38",
      "d6954e86ebee38ece03cc3f3e70f280a3ac38b26",
      "dbe0ae2c80c0eaeb4e80ed285531857ad2061890",
      "1571b3d3f178863172657454961c3bee7f258ecf"
    ],
    "message_list": [
      "Export/import of extras:\n\nfixes #1761\n\n1) Extras are now exported together with other data\n2) Extras are fully imported if the corresponding node did not exist\n3) In case imported node already exists in a database the following\nlogic may be chosen to import the extras:\n   - keep_existing (default): keep extras with different names (old and impoted\n     ones), keep old value in case of name collision\n   - update_existing: -/-/-, keep new value in case of name collision\n   - ask: -/-/-, ask what to do in case of the name collision\n   - mirror: completely overwrite the extras by the imported ones\n   - none: keep old extras",
      "Fix an unexpected unindent problem for a docstring",
      "additional features for the extras import/update:\n\n1) two options: --extras-mode-new and --extras-mode-existing\n2) skip hidden extra for codes and all extras starting with _aiida_",
      "Add checks for the mode parameter type:\n\n1) Except when it is not a string\n2) Except when it is too short or too long\n3) add some tests for those cases\n4) Improve related error messages",
      "Fix one import/export extras test"
    ]
  },
  {
    "pr_number": 5496,
    "commits_list": [
      "dfe799a2b6ef0070e3c55bbf0e4852d4b200d25b",
      "fbd1c222346560723a3b87bb4b0ca9c35e17818b"
    ],
    "message_list": [
      "`SandboxFolder`: decouple the location from the profile\n\nThe `SandboxFolder` by default would create the temporary directory in\nthe `sandbox` subfolder of the repository directory, defined by the\n`repository_uri` key in the profile configuration.\n\nHowever, this folder path should mainly serve to designate the location\nof the file repository of the storage backend. So far the sandbox folder\nhijacking the same key wasn't a problem as there was only one storage\nbackend implementation. However, when we allow other storage backend\nimplementations, having to keep this config key just for the sandbox\nfolder doesn't make sense. Additionally, the `SandboxFolder` can and\nshould be a generic utility that is not tightly coupled to the concept\nof a `Profile`, as it was, by directly using it in the constructor.\n\nThe original reason for storing the sandbox folder in the folder of the\nfile repository is that this path is configurable and so users could\ndefine a location on file system with plenty space available or a\nfilesystem that is in any other way more suited than the default\ntemporary directory of the operating system.\n\nTo keep the possibility for this configuration, the `SanboxFolder`\nconstructor now takes the optional `filepath` argument. If defined, the\nsandbox folder will be created in this directory. By default, the folder\nwill be created in the default temporary directory of the operating\nsystem. The `tempfile.mkdtemp` built in method is used for this purpose.",
      "Add the config option `storage.sandbox`\n\nThis config option can be used to define the directory into which all\nsandbox folders will be created. This can be useful if the default\ntemporary directory of the operating system that would otherwise be\nused is not suited, for example because it is on a slower file system or\nit has insufficient empty space.\n\nThe option is passed to the `SandboxFolder` constructor whenever it is\nused by AiiDA's internal code. Since the option requires the presence of\na configuration, the retrieval of the option is added as high up in the\nAPI as possible. This is in order to not couple components to the\nconfiguration that should remain independent."
    ]
  },
  {
    "pr_number": 2486,
    "commits_list": [
      "d695343ea3ffa8597439af656ffaff198f01d29f",
      "940e3004e7b37635d1317f24e30108e0b1d0402d"
    ],
    "message_list": [
      "Importing `metadata` for `Log`-entries properly.\n\nAdded ex-/import test: `test_reimport_of_logs_for_single_node`\n\nThe test tries to test the case in issue #2448,\nwhere a specific Node has a few Log messages.\nThen one wants to import a file that has more Log messages,\npertaining to the exact same Node.\nThe test checks that all Log messages are imported according\nto their UUID.\n\nAdded simple test: `test_export_of_imported_logs`.\n\nThe test checks if one can re-export imported Logs.\n\nCorrect import of Logs into sqla from django.\n\nCheck a Log's metadata is JSON serializable upon construction.\n\nAdd test of Logs to check correct metadata format.\n\nAdd type check of metadata when constructing new Log.\n\nMake sure uuid is a string when retrieving it for comparisons,\nduring import with SQLA.\nThis was already the case for import with django.\nThis is crucial, since Log.uuid returns a UUID() type.\nTo make a proper comparison and ensure we do not try to insert an existing Log,\nthis MUST be converted to a string, i.e. str(UUID())\n\nTests for importing existing comments.\n\nAdded `comment_mode` to import, which may be:\n - `'newest'`: Will keep the Comment with most recent mtime.\n - `'overwrite'`: Will overwrite existing Comments with the ones from the import file.\n\nAdded `delete_many` method to comments.\nActs within a session for SQLA.\n\nMinor fixes to import of extras.\n\nChanged default `extras_mode_new` from `'ignore'` to `'import'` in `import_data_sqla`.\nThis is to make it the same as for `import_data_dj`.\nFurthermore, `'ignore'` was not a valid value.\n\nUpdate docstrings of import_data* functions\n\nCheck `mtime` is the same before and after export/import.\nBoth for Node and Comment.\n\nCentralized import check of Comments in `_merge_comment` function.\nIt will update existing Comments if needed, NOT delete entries.\n\nUpdated `delete_many` method to accept filters.\nBoth for Logs and Comments and both for django and SQLA.\n\nAdd `comment_mode` to `verdi import`.\n\nAdd `metadata` check for exprot/import of Log.\n\nReplace all mentions of issue #2342 with issue #2426.\n\nRedesign `test_reimport_of_logs_for_single_node` and `test_reimport_of_comments_for_single_node`.\n\nCreate separate two new tests:\n`test_multiple_imports_for_single_node` for both Logs and Comments.\n\nCo-authored-by: Spyros Zoupanos <spyros.zoupanos@epfl.ch>",
      "Merge branch 'provenance_redesign' into fix_2448_import_several_log_entries"
    ]
  },
  {
    "pr_number": 5145,
    "commits_list": [
      "72b43771513f4006025000a48fb24943c2951028",
      "9f80c905aad082fcb26e271a713ced01bfa66d21",
      "a617829e03e085656064f70301c9cc79d120f978",
      "dcb95bc3027332d596b8297c4f17309664e34c20",
      "ff40dc92aaadab3153373e5498c2545a00bd3739",
      "888df2ac1a408546dac8a4408fe2db24d73a4dc1",
      "c5d9b487c4b265883b296336905738ce1c4bb957",
      "724bec5846c5462ad4fffe4a3878b47c878a75d7",
      "924af94baa9d5cada33a891f115f52e60999c962",
      "ecdbfe4ad750613714aebafe2da54ce3a7ba5d96",
      "7dcb075cdd400fccd86d926253d0259aeceb601e",
      "b0074fbe98517d6833312b1c1c0d3e39e0ef30dd",
      "32d111e3dfaf587daa1343c1334110877ab61583",
      "b1a7c465f247cb27146df5f23bfaf6b147511222"
    ],
    "message_list": [
      "\ud83d\udd27 MAINTAIN: Improve typing `Entity.objects`",
      "\ud83d\udd27 MAINTAIN: Remove duplicate `EntityTypes` enum",
      "\ud83d\udc4c IMPROVE: Add `AuthInfo` to `verdi shell`",
      "\ud83e\uddea TESTS: yield `aiida_profile` from fixtures",
      "\ud83d\udc1b FIX: `SqlaBackend.bulk_insert` AuthInfo metadata\n\nConvert from `_metadata` -> `metadata`",
      "\ud83d\udc4c IMPROVE: Allow for node links to be queried\n\nLink queries return `LinkQuadruple`",
      "\u267b\ufe0f REFACTOR: Extract `get_database_summary` to common\n\nThis will allow the code to be re-used with the new `verdi archive` commands",
      "\ud83d\udc4c IMPROVE: Add `glob` method for node repositories",
      "\u267b\ufe0f REFACTOR: New archive format\n\nImplement the new archive format,\nas discussed in `aiidateam/AEP/005_exportformat`.\n\nTo address shortcomings in cpu/memory performance for export/import,\nthe archive format has been re-designed.\nIn particular,\n\n1. The `data.json` has been replaced with an sqlite database,\n   using the saeme schema as the sqlabackend,\n   meaning it is no longer required to be fully read into memory.\n2. The archive utilises the repository redesign,\n   with binary files stored by hashkeys (removing de-duplication)\n3. The archive is only saved as zip (not tar),\n   meaning internal files can be decompressed+streamed independantly,\n   without the need to uncompress the entire archive file.\n4. The archive is implemented as a full (read-only) backend,\n   meaning it can be queried without the need to import to a profile.\n\nAdditionally, the entire export/import code has been re-written\nto utilise these changes.\n\nThese changes have reduced the export times by ~250%, export peak RAM by ~400%,\nimport times by ~400%, and import peak RAM by ~500%.\nThe changes also allow for future push/pull mechanisms.",
      "\u2728 NEW: `verdi archive import --import-group`\n\nAdd option to not create import group of imported nodes",
      "\ud83d\udc4c IMPROVE: `strip_checkpoints=True` for `create_archive`\n\n`ProcessNode` checkpoint attributes (for running processes)\ncontain serialized code and so can cause security issues, therefore they are now stripped by default.\n`create_archive` anyhow only allows export of sealed `ProcessNode`,\ni.e. ones for finalised processes which no longer require the checkpoints.",
      "\ud83d\udcda DOCS: Document archive format and database schema\n\nThe `sphinx-sqlalchemy` extension is used to dynamically generate documentation for the database schema.",
      "\u267b\ufe0f REFACTOR: Move implementation specific attribute code\n\n`BackendEntityAttributesMixin` leaked implementation specific variables and functions.\nThis has now been moved to the `DjangoNode`/`SqlaNode` classes.",
      "\u267b\ufe0f REFACTOR: Move implementation specific extras code\n\n`BackendEntityExtrasMixin` leaked implementation specific variables and functions.\nThis has now been moved to the `DjangoNode`/`SqlaNode` classes."
    ]
  },
  {
    "pr_number": 4891,
    "commits_list": [
      "81457037520e101e5360a4e48ce3bf4866b2eaa8"
    ],
    "message_list": [
      "`Profile`: do not leak repository implementation in interface\n\nThe `Profile` interface was leaking implementation details of the\nrepository. For example, the method `get_repository_container` returned\nthe `Container` class of the `disk-objectstore` library, but this is an\nimplementation detail. If this ever were to change all client code would\nbreak.\n\nInstead, we add the `get_repository` method which returns an instance of\nthe generic `Repository` class that will be common to all repository\nimplementations. The backend implementation can still be obtained\nthrough this object but this should only be done in exceptional cases.\n\nThe `Repository` class now has three additional properties and methods:\n\n * `def uuid(self) -> typing.Optional[str]:`\n * `def initialise(self, **kwargs) -> None:`\n * `def is_initialised(self) -> bool:`\n\nThese simply call through to the exact same method/property on the\nbackend instance. The `AbstractRepositoryBackend` also now has the exact\nsame attributes and they are implemented for the two currently existing\nimplementations `DiskObjectStoreRepositoryBackend` and the\n`SandboxRepositoryBackend`."
    ]
  },
  {
    "pr_number": 2640,
    "commits_list": [
      "dae856d84ff7488712f59636813ee3a0b8d8eab1",
      "5778a557466688f5fd6f3906af985a96b2eb960b",
      "d7417e503c1ee2965b168fac5610dd4ab86f6a8e"
    ],
    "message_list": [
      "pyproject: remove reentry from requirements\n\nThe problem is that reentry starts to scan the entrypoints after\ninstallation which picks up the current AiiDA directory as well, but\naiida-core is not yet installed.",
      "Merge branch 'develop' into bugfix/pip19",
      "Merge branch 'develop' into bugfix/pip19"
    ]
  },
  {
    "pr_number": 3835,
    "commits_list": [
      "36c5eaef639c88d06eef681166991b20deab8921",
      "bd7bbcbe1e8130e2e522b9da43e8a4f64f58b986"
    ],
    "message_list": [
      "Update coverage upload step\n\nRevert to the continuously updated v1 tag for the action.\nFail CI if failing to upload coverage, since we can upload coverage for\nall PRs.\nRemove secret token. It is now possible to upload coverage reports to\ncodecov without a token (from public GitHub repositories only).\n\nUpload coverage only if:\n- Python version: 3.5; AND\n- Is a push in or PR against repository 'aiidateam/aiida-core'.",
      "Use actions/checkout@master\n\nThis is an updated and more robust version of the checkout action.\nSee https://github.com/actions/checkout for more information.\n\nOriginally, v2 was attempted, but this clashes with being able to upload\ncoverage, hence `master` is used."
    ]
  },
  {
    "pr_number": 4195,
    "commits_list": [
      "923761b483e507ab492768df44645ebafd6b7dab",
      "6c1ed6642f00cfa2b19fdadb0a13c586862e79d8"
    ],
    "message_list": [
      "Only color directories in `verdi node repo ls --color`\n\nThe help string says that the `--color` flag can be used to color\ndirectories different from files to easily distinguish them, except the\nimplementation colored all files, regardless of type.",
      "Merge branch 'develop' into fix/4183/verdi-node-repo-ls-color"
    ]
  },
  {
    "pr_number": 5210,
    "commits_list": [
      "b8df84f9990fa7fb491b3b6e077e9586c40f104f"
    ],
    "message_list": [
      "CLI: deprecate `verdi config caching`"
    ]
  },
  {
    "pr_number": 5081,
    "commits_list": [
      "c69626747fa5456af199b57d3922947485999ea5",
      "0636376413973885d2a9fb2fa678856ada586dc7",
      "5e9dfaf46e6b9c4db76fb9f16823a9cc1ee1febb",
      "b5732328692dd39ddb3d87f2e7871db2841fcd64",
      "a70fa63d468aef3c506ec4e28395ecf06215bbad",
      "dceee09836676d39d11f08c27712233d5e4456cf",
      "9a6fb1b9d9ed2c4097ac6b28d0194e313565512d",
      "7cc5870855b4826cd639a79fd3ffded04bfc502a",
      "4b616d6e137d1936589cf39f2d8d7783b7e53ebd",
      "37b6f64b2c399424df09e2932631a04fa675c7d3",
      "f1f9c366b47cfd9554389821e2aa5ac84de35176",
      "e1da060ed6582bc18bce083d3b684729c073206b",
      "f2573720feb3c934759d1b857ebb13f1fb697cdf",
      "49bd7727fb8d0c51edb9eeaa465cae1539e7bfc3",
      "5636329c0d286195b1ef23769c43b3b475ba5278",
      "164d17a77602862beb64fe61c2ebff5836db089d",
      "afe7ab191f5597a05722624bfdd415115d8ce55d",
      "2ef4177571d025f1102e74cbbc7610672d5cddfd",
      "06c1e6094a48e4d62ef8aab4b9284e653ff91790"
    ],
    "message_list": [
      "\ud83d\udc4c IMPROVE: QueryBuilder representations",
      "remove queryhelp",
      "Merge branch 'develop' into querybuilder-repr",
      "fix test",
      "Merge branch 'querybuilder-repr' of https://github.com/chrisjsewell/aiida_core into querybuilder-repr",
      "fixes",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "add `analyze_query`",
      "fix literal rendering",
      "fix pre-commit",
      "_kw -> kwargs",
      "Apply suggestions from code review\n\nCo-authored-by: Casper Welzel Andersen <43357585+CasperWA@users.noreply.github.com>",
      "Update aiida/orm/querybuilder.py",
      "Update aiida/orm/querybuilder.py",
      "add __repr__",
      "minor update",
      "minor fixes",
      "Use typing_extensions.TypedDict for python 3.7",
      "Update docs/source/topics/database.rst\n\nCo-authored-by: Casper Welzel Andersen <43357585+CasperWA@users.noreply.github.com>"
    ]
  },
  {
    "pr_number": 3063,
    "commits_list": [
      "0f3d14b9d985c7261b91aa1de0b572380d799f66",
      "7fe351393d8e99484946755d2c7a837052409380",
      "564297107f600514983d01e0a50045062c8dc222"
    ],
    "message_list": [
      "fix a bug for printing scheduler outputs\n\nReplace `format` with string concatenation to avoid interpreting unicode under python2.",
      "Use format instead of string addition\n\nEnsure unicode format for scheduler stdout/stderr in python2. This is prefered compared to string addition for code readability reasons.",
      "Merge branch 'develop' into issue_3062_process_report"
    ]
  },
  {
    "pr_number": 1352,
    "commits_list": [
      "8de588d194e08dbb044a6224051a3866d33169b8",
      "66b4cca5e44243d884bd594371720cac9b6ed358",
      "f0ee2e0455e23b296a6903288cde6ead58a00a1f",
      "d13a57f0d408aaed603801b4446462dca2d775c2",
      "9ce5d7f5fe6c9b6d356d8fdb150a1f0884d467c5",
      "09f1b8ad5d7090d969b4fe92f41fa7d4899680d2",
      "747e353cfab3bb64aefaaadaa08b052b01d60ef7",
      "4958bb90796efc4e3e70e8ad0e273596dafd620e",
      "9f9e1a2c0863a97829bc04ae75039509cc50cfb3"
    ],
    "message_list": [
      "attempt to run sphinx-apidoc on rtd",
      "remove leftover comments",
      "Merge branch 'develop' into rtd_fix",
      "Merge branch 'develop' into rtd_fix",
      "add chainmap dependency to rtd\n\n + remove duplicates in rtd requirements\n + make run_apidoc less OS dependent",
      "fix pathlib dependency",
      "try to fix aiida.settings problem\n\naiida.settings was not documented because it failed writing the\nconfig file\n\nWARNING: autodoc: failed to import module u'aiida.settings'; the following exception was raised:\nTraceback (most recent call last):\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/aiida-core/envs/rtd_fix/local/lib/python2.7/site-packages/sphinx/ext/autodoc/importer.py\", line 140, in import_module\n    __import__(modname)\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/aiida-core/checkouts/rtd_fix/docs/source/../../aiida/settings.py\", line 22, in <module>\n    confs = get_config()\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/aiida-core/checkouts/rtd_fix/docs/source/../../aiida/common/setup.py\", line 96, in get_config\n    return check_and_migrate_config(_load_config())\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/aiida-core/checkouts/rtd_fix/docs/source/../../aiida/common/additions/config_migrations/_utils.py\", line 39, in check_and_migrate_config\n    store_config(config)\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/aiida-core/checkouts/rtd_fix/docs/source/../../aiida/common/setup.py\", line 141, in store_config\n    with open(conf_file, \"w\") as json_file:\nIOError: [Errno 2] No such file or directory: '/home/docs/.aiida/config.json'",
      "second try to fix aiida.settings",
      "fix typo"
    ]
  },
  {
    "pr_number": 4428,
    "commits_list": [
      "e276ab15c88e67a7479d62a0af99f21c4270510d",
      "99b98debec6591c52e4a277b1cdcf800a0ecb24b",
      "41c55f938e3e027aa04822ad3e9d847b743a52db",
      "d50cf901b7ab3aedb66ace43f6b06e660a018c56",
      "d1d98c9a858000a09f23e9f5c2f467a433396f56"
    ],
    "message_list": [
      "Add `verdi group move-nodes` command\n\nAdd an extra subcommand `move-nodes` to `verdi group` that moves nodes\nfrom a source group to a target group, instead of having to use the\n`remove-nodes` and `add-nodes` subcommands in order to achieve this.\n\nSimilar to the `remove-nodes` and `add-nodes` commands, the move can be\nmade without prompts by adding the `--force` flag, and the nodes are\npresented as a list of arguments.\n\nAdd corresponding test method to the `TestVerdiGroup` class of the\ntest_group.py suite.",
      "Add several warnings and critical failures.\n\nHere we add several warnings and failures depending on the provided\ninput:\n\nWarnings:\n\n* If some of the specified nodes aren't in the source group.\n* If some of the specified nodes are in the target group.\n\nCritical Failures:\n\n* If the source and target group are the same.\n* If no node identifiers are provided to move.\n* If none of the specified nodes are in the source group.\n\nAll of these can be skipped by using the `--force` flag.",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "Apply reviewer suggestions",
      "apply reviewer suggestions"
    ]
  },
  {
    "pr_number": 4855,
    "commits_list": [
      "fe874fec24436a414596e88e8154a7b6dc29c736"
    ],
    "message_list": [
      "Dependencies: update `sqlalchemy-utils` dependency requirements\n\nIn `767f57a02b1ba40421ab9b17a67b2532aff556c6` the `sqlalchemy`\ndependency was pinned to the `v1.3` minor version, because the recent\n`v1.4.0` release would break `sqlalchemy-utils`. The latter has now\nreleased `v0.37.0` which is compatible with `sqlalchemy==1.4`. In theory\nthis would allows us to support `v1.4` as well, except our own code\nbreaks with this version since we access quite a few protected\nattributes in the query builder implementation.\n\nSince `sqlalchemy-utils` does not follow semver, we have to pin it to\nthe `0.37.x` series for now. Since version `0.37.0` and `0.37.1` were\nyanked as they did not specify the `python_requires` keyword, we set the\nlower boundary to `0.37.2`.\n\nThe constructor of the `DbLog` class was simplified. Its only purpose is\nto make sure that the `_metadata` column is properly initialized. The\ncolumn has an underscore prepended because otherwise it would clash with\nan attribute of the SqlAlchemy base class, however, the constructor\ntakes just `metadata` so this has to be transferred. In addition, it\nshould be initialized to an empty dict if it is `None`. All other\ninitialization code that was in the constructor will be taken care of by\nsimply calling the parent class' constructor.\n\nFinally, the `force_instant_defaults` listener of `sqlalchemy-utils` was\nupdated in v0.37.5 which caused certain tests to fail. The fix ensured\nthat the `kwargs` passed into the constructor of a model instance would\nbe updated with the defaults of the column definitions that are set on\nthe instance by the listener. The problem is that some of our models,\ne.g. `DbComment`, rely no distinguishing between the column value being\nexplicitly passed by the caller or through default. Most notably this is\nfor the `mtime` which our implementation wants to have undefined before\nstoring, unless explicitly passed in the constructor. Due to the change\nin `sqlalchemy-utils` this distinction could no longer be made and so\nthe only solution was to copy the original implementation of the\nlistener utility."
    ]
  },
  {
    "pr_number": 1737,
    "commits_list": [
      "4fe68dffd096bfcb671b6a140ddcf476e8c58669",
      "3f04725461e463414d1367f7e0f023d208c75d5c",
      "bad161779e538cf026879530a24224648a82efdb",
      "825919bccedb754092affbd55ce2ab9b3b1187b1",
      "0a4c3dc10c02f3cefdcd47102318eedf9541483e",
      "4a2cb7c2d80b4606c68aa3f04f05c5d0c867d52d",
      "57ec177c04d5032903ddca43d2b5c2e1fdcfe766",
      "414aa76eb01c381bb0039fec4e38e05277977d21",
      "ccb3cd977492c7cfb0a5f436827dad3c4ce67abf",
      "ad7b954024d2c3998602d32d30b609575479a983",
      "bb98f6619540854fef67457a15dddd7580c371aa",
      "f00491b796bf6a089441f0b158572e379a8ff616",
      "dbcd78dd2fe209943262378e602b1f5016226858",
      "38aa99872392990af59d1272cb7a29385b9f3615",
      "6be4dfdd71036d51029e7b49e7dea5fb7b089bd9",
      "a74c6050e098acb5ba357fcebf0bb92fce05bb30",
      "24c246d437e87963c86ad1741e7dd1ceb984a7f0",
      "3ea33cd1f299c30e2462593d8ab771ff7d09e235",
      "8031ea3c3c80f3ec1723368d1f9bf79938263d80",
      "47308838ae7848b63817d7bbe7c832d95b8c6dcc",
      "8d080e4eb6fa83d5b45be6a17a59f8af379cf7fa"
    ],
    "message_list": [
      "first version of verdi code duplicate\n\nStill to do:\n * add possibility to override parameters\n * store duplicated code\n * add tests",
      "add functionality to update label",
      "interactive updating of duplicated code\n\nstill to do\n * add option to hide previous code\n * add tests",
      "verdi code duplicate: add --hide-original",
      "try to fix travis bug",
      "Merge branch 'verdi' into add_verdi_code_duplicate",
      "Dereference name attribute of plugin variable in CodeBuilder\n\nThe `.name` attribute call was removed, leading the EntryPoint instance\nto be set, instead of its `simpleplugins.templatereplacer` name",
      "Merge remote-tracking branch 'origin/verdi' into add_verdi_code_duplicate",
      "Merge branch 'develop' into add_verdi_code_duplicate",
      "add failing test",
      "fix non-interactive code duplicate\n\n * also: fix info msg for `verdi code delete`",
      "add failing interactive test",
      "fix interactive unit test\n\nfix interactive unit test for verdi code duplicate",
      "Merge branch 'develop' into add_verdi_code_duplicate",
      "reuse code-specific options in verdi code duplicate",
      "Merge branch 'develop' into add_verdi_code_duplicate",
      "fix bug in verdi user",
      "fix prompting behavior",
      "Merge remote-tracking branch 'origin/develop' into add_verdi_code_duplicate",
      "update docs",
      "try fixing python3.6 pre-commit tests"
    ]
  },
  {
    "pr_number": 639,
    "commits_list": [
      "1c407dbbbd9384301ad8321012e8d60a3b9dcbad",
      "17f0802114a86034038991b810e69afe1d9d3d82",
      "49ea8ef64c885cb3c7ed35719aa9c2466bc8d347",
      "637ffba2aca0ac554bb5728b73b61d55a9c6ac80",
      "b5c9aa77aaa24b3931772b7d6654907b99436dac",
      "668d0772cf627ea7fc5e010d013d3d00b3a1cb56",
      "56d7053bdd52eacd3c1ff0497941e022bd4e500f",
      "1c987e540b86e9c2f27053a2d5b4415b47ef35db",
      "79e751b9e154ffb7cae4602cdde2d02cbf7382ee"
    ],
    "message_list": [
      "changed possible job_states according to 'ps' man page, only the first letter of the job state will be considered from now on",
      "Merge branch 'develop' of https://github.com/aiidateam/aiida_core into develop\nmerge",
      "fixes issue 638 _label and _description in job_process input was not dealt with",
      "checked out old direct scheduler file of aiida_core",
      "Added testcase (from Shuber) for setting a label and a description of a job_process",
      "added missing import in job_process test",
      "Merge branch 'develop' into broeder-j_fix_issue_638",
      "bugfix to job_process test, job in test was not killed",
      "merge of develop"
    ]
  },
  {
    "pr_number": 4148,
    "commits_list": [
      "fe3a8a3de89089b95abdb7e35b8b2721def0cf57",
      "8512eec95d7df2abe400e8a285cd34725c1ef05c",
      "170b546c648547f65d59d6a771fc1ce19bddb123"
    ],
    "message_list": [
      "Add repository issue templates",
      "Pre-commit fix",
      "Merge branch 'develop' into issue-templates"
    ]
  },
  {
    "pr_number": 4750,
    "commits_list": [
      "234fd3f9793853ffe77b0652c562a3a4fed12bf7",
      "ce2310078a5959df703c6dcb3a835eb2f84d39ae",
      "fdb936452b1c7f4194158c779fb3252bef8b6a91",
      "6d4e7314c551918f9b185919c2758af189b1402e",
      "38f95fc2342b27ad924df641856e6cfd1c3e7a7f",
      "8824b5e224572d6862f8533c4502869ca966b55c",
      "4beeb70cf49ff4575003e03043cf526ecffb5c6c",
      "6ff24866e2d12fde87ea98c86af367b99c7477f7"
    ],
    "message_list": [
      "Add migration + tests for the django backend",
      "Add migration + test for sqla (+ rename django)",
      "Join simple change queries + clean test",
      "Fix django: set last migration",
      "Fix django migration file name",
      "Improve performance for function_number_of_lines",
      "Add corrections from PR review.",
      "Correct interpretation of number of lines"
    ]
  },
  {
    "pr_number": 4117,
    "commits_list": [
      "57a7bed0e89d43e94b54d604f6e92b2b0545d725",
      "6842a610ab8e4a501d0c07b8f28d2c7a0c327eb2"
    ],
    "message_list": [
      "Add support for process functions in `verdi plugin list`\n\nSo far, only actual `Process` subclasses were supported, however,\nprocess functions can be turned into a `Process` subclass just as well.\nThe wrapped function gets a new attribute `spec`, which when called\nreturns the `ProcessSpec` that is dynamically built based on the\nfunction signature of the decorated function.",
      "Adapt interface of `run_cli_command` and test `raises` argument"
    ]
  },
  {
    "pr_number": 3308,
    "commits_list": [
      "23ea93ee7859d4496224a33b9f4c59c6fe8ef82e",
      "0b4d77b2b9ad1113ea428f0bab854fa0128d0e3f",
      "194eb0cd4208bce8649d7d33d37a6e4d1d10c3be",
      "212cfdf8d86c4fa57311713e4154105d2f4b1ec9",
      "5d325d3811947dcf08e655242e70db82ca0a2e44"
    ],
    "message_list": [
      "add upf to json converter\n\nUPF format 1 and 2 are supported. SIRIUS requires pseudopotentials in json\nformat.",
      "upf2json: add unit tests",
      "add upf export to cli\n\nExport a upf to json using:\n\nverdi data export upf",
      "add UPF for carbon to TestVerdiDataUpf",
      "test_data_exporters: add a dummy instance for UpfData"
    ]
  },
  {
    "pr_number": 3768,
    "commits_list": [
      "416313b4c425649171ef48e15de1b4208929e043",
      "1a7014acccae506889794c5619226c022b03434f",
      "ffd5d2aa4d147788b7da8e9d32e0275860b99a6d",
      "651c421d1b9d1ff15cf52454979d3806d4ecbbd6",
      "c2fe4d984d1faa3857c63a8990198baa89b3d2a5",
      "7b31c2db25a484f38b7fb1565eeb3e09d390d816",
      "8f74af44a31df4f2e7156d6454d8447ce6ae8621"
    ],
    "message_list": [
      "Added Installation via conda",
      "Removed my changes and then checking for errors in file",
      "Merge branch 'develop' into Installation_via_conda",
      "Added Installation via conda Instructions",
      "Added conda instructions",
      "Merge branch 'develop' into Installation_via_conda",
      "Update prerequisites.rst"
    ]
  },
  {
    "pr_number": 4829,
    "commits_list": [
      "076fc53c2dcc5f80c970ba373fc2d41457029a0d",
      "358cf46dfcb09061e6781415c1c7c1b7a16abd99",
      "5bf01ae3ab4e43f2f71b43596705fba400ddcf09",
      "26c94b58371a76549063e857aa5bf27cc7dd1387",
      "0a54de32c819bcf9cbd1be499133f6da969fc0bd"
    ],
    "message_list": [
      "Merge pull request #1 from aiidateam/develop\n\nMerge Updates",
      "Add __str__ to Orbital class",
      "Adding test",
      "Merge branch 'develop' into issue_4347_RecursionError_for_orbital_class",
      "Merge branch 'develop' into issue_4347_RecursionError_for_orbital_class"
    ]
  },
  {
    "pr_number": 4341,
    "commits_list": [
      "c53698fa690851659b7919257aacc67e6cf70f58",
      "ffb3475244b1a1f32a335b1fc68e0d09597ab52d",
      "41b245f507dff9e6ebbfb08687f7ed5c9d187044",
      "af905ec3ddd31651ef3d2405181d8897a352277e",
      "aba3f33a7e825da6e9db3bbbe2b5e16bb470a71a",
      "be2f20c60d87b94c4be70f57eb1ff4b7a78f80ba"
    ],
    "message_list": [
      "Make the RabbitMQ connection parameters configurable\n\nUp till now, the URL to connect to the RabbitMQ server was hardcoded.\nThis means it could only connect to the localhost over the standard port\nand with the default credentials. Certain users, will require to deploy\nRabbitMQ on a different machine than the AiiDA instance so the server\ndetails should be configurable. Since this will no longer guarantee that\nthe RabbitMQ server is running on localhost, it should also be possible\nto use SSL by changing the protocol from `amqp` to `amqps` and provide\nspecific user credentials.\n\nThe `aiida.manage.external.rmq.get_rmq_url` is responsible for\nformatting the correct URI. The method takes the values that form the\nscheme, netloc and path as arguments, whereas optional query parameters\ncan be specified through the keyword arguments. The supported arguments\nare:\n\n * protocol\n * username\n * password\n * host\n * port\n * virtual_host\n\nIn addition, the following keyword arguments can be specified:\n\n * heartbeat  # heartbeat timeout in seconds\n * cafile  # string containing path to ca certificate file\n * capath  # string containing path to ca certificates\n * cadata  # base64 encoded ca certificate data\n * keyfile  # string containing path to key file\n * certfile  # string containing path to certificate file\n * no_verify_ssl  # boolean disables certificates validation should be \"0\" or \"1\"\n\nNote that the hearbeat, unless explicitly specified, will be set to 600\nseconds as a default.",
      "`Profile`: add message broker configuration getter and setter properties\n\nAdd property getter and setters to the `Profile` class for the\nconfiguration parameters of the message broker, that is currently\nfurnished by RabbitMQ. These parameters determine the URI that needs to\nbe used to connect to the message broker.",
      "`verdi`: add message broker configuration options to profile setup\n\nMost installations will just need the defaults, which have been set to\nthe default localhost configuration of RabbitMQ, but this now offers the\noption to administrators to also use a RabbitMQ server that does not run\non the same machine as the AiiDA instance itself, or requires actual\nuser authentication.",
      "`Config`: add migration to add default message broker configuration\n\nThe migration simply adds the new broker related fields to each profile\nusing the default value, unless the profile already defines it. The\nmigration version is upped, but the last backwards compatible version is\nkept the same. This is because if the configuration is used with version\n3 of the config file, the new keys are simply removed as the config file\nis parsed. When the code is updated again, the new keys are added again\nusing the defaults.",
      "`verdi status`: fix broker URL and add `--print-traceback` option\n\nThe `verdi status` command now reports the proper URL that is used to\nconnect to the RabbitMQ message broker. In case the connection fails,\nthe exception message is printed. Passing the `--print-traceback` option\nwill force the command to print the entire stack trace as well.",
      "Docs: add section to installation guide on configuring RabbitMQ\n\nWith the new feature of the RabbitMQ URI being fully customizable, the\nsetup guide needed a new section on how to configure those settings\nfor a given profile through `verdi setup`."
    ]
  },
  {
    "pr_number": 1236,
    "commits_list": [
      "e00b1f0eb756b7dd2d6fba287aacd7c9b2fd87b1",
      "ca7f4072de4d242d17a41cef3bd9fa9df07ca107",
      "17ca7d4e11bafaedbaf8ccff489221dcc06056e8",
      "e601d44b906b62e34b8f540d31dddf142baebce2",
      "2ba86a5e196597027317e74f95007eb654549ee4",
      "c2df68b71ad39d68350dcd9e39a6f3c1b3599b48",
      "2c05d2fe89dbc458b5969a24b08f3324d7bdc52e",
      "93851ba3f484e02123d98ec42ce0a0ac9241d6a9",
      "b120e3867f55efc3e219f24c09b5fcdb8f9248e3",
      "11bb839db8d0680c83df1c7582c5b5968ed10654",
      "1984f1ea30bdab01b17ab14855dbbb610cb33327",
      "8112b780a69b9cd1b52800bb744de17721157ed2",
      "b5cc7477f2c4afac8d004d138a70a28fffbaa556",
      "535f2ce55f8bd4504c58acb01fb1516fbe2f0871",
      "e8e0ba110f33ac57ec11ccb0a2b098fb5c31c57a",
      "66e70adf40ccd6bbed2090628c6083848d8863ad",
      "600ec49eb1bc7955b782fb5acada20c16480cfea",
      "97a53ce8f53ca0969fbf3d1fe3638ae57bcd8d34"
    ],
    "message_list": [
      "Allow passing 'non_db' inputs in dynamic input namespaces.\n\nWhether a dynamic input is 'non_db' or not is decided by whether\nit is a 'Node' instance.",
      "Merge branch 'workflows' of github.com:aiidateam/aiida_core into feature_allow_nondb_in_dynamic",
      "Remove 'valid_type' attribute for Processes (non_db inputs allow any type).",
      "Add test for submitting dynamic non_db input.",
      "Change input validation test to reflect allowing non_db inputs.\nFix output validation test to us 'validate_outputs' instead of 'validate_inputs'.",
      "Merge branch 'workflows' of github.com:aiidateam/aiida_core into feature_allow_nondb_in_dynamic",
      "Fix DynamicNonDbInput workchain.",
      "Change order of workchain submits.",
      "Merge branch 'workflows' of github.com:aiidateam/aiida_core into feature_allow_nondb_in_dynamic",
      "Revert valid_type change",
      "Merge remote-tracking branch 'upstream/workflows' into feature_allow_nondb_in_dynamic",
      "Merge branch 'workflows' of github.com:aiidateam/aiida_core into feature_allow_nondb_in_dynamic",
      "Revert test for dynamic input validation.",
      "Change order of valid types.",
      "Remove WithNonDb mixin from PortNamespace",
      "Use 'getattr' to check for 'non_db' on port.",
      "Put 'non_db' inputs in the same paragraph, mark them as 'non_db' explicitly.",
      "Also use 'getattr' to check non_db in job_processes.py"
    ]
  },
  {
    "pr_number": 3977,
    "commits_list": [
      "56dabd0d7399cc5fd857ba251f3499a3ab6b55e7"
    ],
    "message_list": [
      "enable loading config.yml files from URL\n\nIn the view of the plans of starting online repositories that maintain\nyaml configuration files for computers and codes, it becomes very useful\nto be able to directly import a configuration file from a URL.\n\nVersion 0.6 of the config-file-option now respects the `type` parameter,\nand file handles can be passed directly to the configuration file\nprovider.\nThis allows handling opening files locally and from URLs on the same\nfooting."
    ]
  },
  {
    "pr_number": 4692,
    "commits_list": [
      "918e0a187ee3775703256eafe01346613f2f340d",
      "6270c5e24abcd7d377acf1fa7280d2fc0d902424",
      "03d5b5e54fb95fde79d66d545bad3338bdf57ffd",
      "2c7c119d04fec2c12d65826229d7a5c52829abda",
      "17d9c34803f1b392f72aa8c6c149ee8385cba1e7",
      "fa4ffebc616537882431f10f1e137f71abeb4b45",
      "761090545cf3b239aa289868ba30411bca18e6fc",
      "60883e7bd7b987fa67bceeb27a7cf2d154e8a0a0",
      "d2bdf9cbf1a143f1c32bff84208add3584a8c32f",
      "06be5499cdc98dd245df1cce40ff235973770bae",
      "0b495867dd9446d9b111c75467e7dbbb8f0b47dd",
      "34ab76322d1bd0ed629bb811b3bc668a9346600a",
      "3cdc02d619b9f4af0cf64b41318c21ef5c17dc9e"
    ],
    "message_list": [
      "Fix: Allow interruption of `do_upload`\n\nAs per the other transport tasks,\n`do_upload` should be inturruptable,\ne.g when the process is killed.",
      "reduce coverage diff",
      "apply consistency fixes",
      "Merge branch 'develop' into fix/kill-doupload",
      "pre-commit fix",
      "Update tasks.py",
      "Merge branch 'develop' into fix/kill-doupload",
      "pre-commit fix",
      "Merge branch 'develop' into fix/kill-doupload",
      "Merge branch 'develop' into fix/kill-doupload",
      "Merge branch 'develop' into fix/kill-doupload",
      "Merge branch 'develop' into fix/kill-doupload",
      "Merge branch 'develop' into fix/kill-doupload"
    ]
  },
  {
    "pr_number": 4099,
    "commits_list": [
      "b528a5e7b436b5985a958962bf0502b2e95275e3",
      "832600a58ecbc77a2f38f9b5390a0871208c3020",
      "0139e188dab2ab19dcd86328442a080e7c8c6beb",
      "95cb95a91cd5d7093e7f5e670f604312c49072a4",
      "1d9d32c92b9017ea1846c312f6d234b699f44a0b",
      "9bebcb7623ddb107dde1dfb59ed2025193e8aa3a"
    ],
    "message_list": [
      "Move to CircleCI for CI Documentation builds",
      "Remove github ci docs build action",
      "trigger build",
      "disable warning-to-error",
      "Update .circleci/config.yml\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "remove comments"
    ]
  },
  {
    "pr_number": 4147,
    "commits_list": [
      "8648d651ec89f88612a5001c4b120bcdeac18b7b",
      "9948337b1307fb70a9fc30926d2eda2bb16fe0f3"
    ],
    "message_list": [
      "Add release version pinning to install instructions",
      "remove version pinning for pip"
    ]
  },
  {
    "pr_number": 4562,
    "commits_list": [
      "873d658bfdfd91d82f066cbd2c582d67b7e1523c",
      "12e83fc56dcbc931d6b56fceda2ae788329ae707",
      "65cc745077b6226d22405c5da784e9e37d804724",
      "b7a364e0ef2150a7df87692f6520ec2ea71d198a",
      "ee4d04897abca4d68843a07742c63fab7601e1a0",
      "3bf09771dbf0a4854fc8cc533eb3ca8bd3c7e9f2",
      "6e01696c522532fe88a6ddda68459167549a201d",
      "7291cacc47ae791268429e12c6c774065c8bdba5",
      "d383c1e4358d5868e1cad9ad2fe8d71bcea785f7",
      "4bc2920f9f1a12f301b0da803cb8aa19384effb4",
      "ae2a2b6250432f6cdb5046f68152bd52c11ec432"
    ],
    "message_list": [
      "Docs: Add \"How to extend workflows\" section\n\nAdd a section on how to extend workflows to the \"How to run multi-step workflows\" section.\nThis section continues with the `MultiplyAddWorkChain` example and covers:\n\n* How to submit the `MultiplyAddWorkChain` within a parent work chain.\n* How to expose the inputs using the `expose_inputs` method and a proper namespace.\n* How to use the exposed inputs with the `exposed_inputs` method.\n* How to expose outputs and pass them to the outputs of the parent work chain.",
      "Docs: Move launching workflows to top of howto section",
      "Apply suggestions from code review\n\nCo-authored-by: ramirezfranciscof <ramirezfranciscof@users.noreply.github.com>",
      "Apply reviewer suggestions",
      "Apply reviewer suggestions round 2",
      "Reorganise workflow sections",
      "Update links for BaseRestartWorkChain section",
      "Apply suggestions from se\u00f1or Ramirez\n\nCo-authored-by: ramirezfranciscof <ramirezfranciscof@users.noreply.github.com>",
      "Add change made in PR #4789 again\n\nI missed this one in when dealing with the merge conflict.\nShould be the only difference.\"",
      "Merge branch 'develop' into fix/3993/howto_extend_workflows",
      "Merge branch 'develop' into fix/3993/howto_extend_workflows"
    ]
  },
  {
    "pr_number": 5097,
    "commits_list": [
      "7a849f00fe5ca715eaf4a3446520adc4688277a2",
      "fdfa555cc7bd43f99ae83e78cb2635245a3d6d3e",
      "3ab7065b496943a0cd92445b006e30de6c46d0b3",
      "969b8b6c0b05f902d33927038e53148b4c7b694d",
      "35621dc61bef016a68307628218945c8071f2e5d",
      "72a4b9f7c60097f550a7161486153c3ef50855bc",
      "cfdfef2e0e721355324bb9310231fccff1eca290",
      "f94129c78fd590bd1b83eb1d17b4b3485e7d5b4a",
      "cbecb79a0638ed0ae8ee198f5d8f090ba8c63c0a",
      "95ed9d03135a9ad2f0395517f824a09a1b000846",
      "effaac2087a2854235d9d0f951e1d7454a0601ec",
      "f5a138892c082f1eab4d9fc0ed17d1a2ba81c11f",
      "5719b07a5c8db2b13155350f08a6f0b8e1b1b258",
      "7b3ea1367cf488c4bec896d8c65a67f1b3ec18da",
      "32ab71c3e041aba72598a3d315236bcc5d9cbf6b",
      "fa55f63eafdc8b9220c1a1de47fe81081ee63e29",
      "3631a8f31ff0682bc8c1939bf8b3e6b5eeffe187",
      "2e0877dbe90b8046a02798c936979ebee7e94f50",
      "74b3f068da884d84863f9647a62e9f1400598a3e",
      "e5d8282b82c306a355adf5d7a29fc2fed92aea85",
      "95f80bb92ead89128caa9dbdc2196b734ab66280",
      "d522b4446062c10ae2971c89cb1927358b91844b",
      "a519eff154eda098a11d392c7a027a7598b9fda0",
      "857d9980d25c48bf54939315fda2167ff75428e2",
      "9df317c59deb0eae5ea7cccfc3436ce6da9bdb54",
      "855958fadfa1f565bbbcf88f9117050b9a9bb32e",
      "959924f466b202c71727c350d025459c93bcef82",
      "c07f90786aa172f63106d0e838889be654ba8658",
      "be0c2bf698c4c5de3871448f344459302b5d5b5b",
      "f208bcdb088d2af8c29d43b34fa157c106c832f3"
    ],
    "message_list": [
      "Change SQLA models for parity with Django",
      "add migrations",
      "add tests",
      "Update 1de112340b17_django_parity_2.py",
      "Add example index with `varchar_pattern_ops`",
      "Merge remote-tracking branch 'upstream/develop' into sqla-migration",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "update initial revision",
      "add index migration",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "fix schema diff",
      "Merge branch 'develop' into sqla-migration",
      "Add missing indexes",
      "update",
      "Merge branch 'develop' into sqla-migration",
      "update",
      "update",
      "fix postgresql_ops",
      "add test",
      "\ud83d\udc1b FIX: SQLA migration regression\n\nIn #3582, SQLA migrations were broken,\nby starting a top-level transaction,\ninstead of letting alembic open per-migration transaction.",
      "Add db_dblog.levelname truncation",
      "fix migration freezing",
      "move logic higher up",
      "Merge branch 'develop' into sqla-migration",
      "Merge branch 'develop' into sqla-migration",
      "Merge branch 'develop' into sqla-migration",
      "add comment on the reason for calling transaction.close",
      "Merge branch 'develop' into sqla-migration",
      "Try reverting change to pre v2 migration",
      "Apply suggestions from code review\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>"
    ]
  },
  {
    "pr_number": 4964,
    "commits_list": [
      "758ebf1b962493ec98359e494060f3276d00287b",
      "dd5d3596ebe15f58cd7f9a7ef38fbf363dd013b4",
      "0af0ffc463d21cf91dec7e0c85227695ea5f4888",
      "c553039f873a780ecb5045eca24c0c5f1e44a680",
      "9992bf1133cba01f8dd78f97ec2b41ec02657ffb"
    ],
    "message_list": [
      "backends/django: use simplejson for consistency with SQLA\n\nWhile SQLA has been using simplejson for some time (via `aiida.common.json.dumps` in `aiida.backends.utils`), the `JSONField` from Django was using the native `json` module from Python (they have been using simplejson at some point). This becomes clear as soon as the decimal.Decimal is allowed which simplejson can natively serialize to JSON while the builtin json module does not.",
      "orm: permit Decimal in attributes",
      "Merge branch 'develop' into feature/permit-decimal",
      "move `JSONEncoder` to `aiida.common.json`",
      "remove comment"
    ]
  },
  {
    "pr_number": 5138,
    "commits_list": [
      "a26376df5427b05b843cc85fb3df0d9f95070933",
      "b694b861af67521cdbc22679d7b80be4619d2c46",
      "09f7a3b5161829d1a96ff27824822a958e236d22"
    ],
    "message_list": [
      "common: fix dateutil.parser import in timezone\n\nfixes #5137",
      "tests/cmdline: test \"process list\" without \"-r\"",
      "Add tests for timezone utilities"
    ]
  },
  {
    "pr_number": 2590,
    "commits_list": [
      "d6fce8cc32277b9a5d3bcd1c0e292d2c1b7d2b77",
      "3d25b0c0d2afa026936588f4288ba67ca6bbe6cd",
      "394b134d1c1de66dfc27acc97d330deedeefeeab",
      "ea079fb8c9c08ebb872f4d1bf4eb667c5b27e77e",
      "a17fcce377a5563537a0cf685bfdfc3baef20c5e",
      "6f2d3853c4570e7676e42561c3a70414d8c02f01",
      "1f4b86edb3ca38421b455869c59577e18c6d8a9c",
      "28ecda96e685623fbb4deb045eb2734fc63c0e11",
      "a045c451166eca0ab221a309c39c81edee76177b",
      "bcdb46cd002a287a1f95e244e9189528f4275acc",
      "4d0c7877d34cd91a0ea6a8ddcfac2aabe724d6f0",
      "a86dfed4040d29c7c037131feef6edf32c3c0bd8",
      "3e96d166deeb15df1a6ed12850ac504ec310ea92",
      "040b34e58e904ae1e4bda99bfbf0276bd6276114",
      "d4ddb6b5e1d60e7c8d155e9ebacb179338ea3abe",
      "9b7654fa69a0c4edbfffa745b5f2f922003dbf3b",
      "2d3a80a9f3024571839ec7470ac949c86e3df0a8",
      "d8f8ff189b048fa37d01dc04ab0e1615e450875c",
      "dde378e9048bb0d4255e78b7e12d42a51a2921f4",
      "e9d4e03708276d74fed94daaf0fe0958fc372691",
      "dd25fd19e3a4929c726d0d5f7fd18567daa528e7",
      "6c01cae0ceb1187f6d23caa512f5804ab3f13e1c",
      "c6426c0d5ee7fba6a81c67c621207cf75944c8ff",
      "bbfc73ce554398f5811e95e0d6adc14cc6c9af55",
      "28eea8c4c5f5556f89620055f606e6b1dad457fc",
      "f757e55a4356977fe715de17d4b7b4522ae3a2f9",
      "29498f2052f45326ec5cf59557795dc5d5a6cc63",
      "4806ba10ebec56cd7cb50141807719fca8210e3e",
      "2db295e23927ba0aed8076c2318d96291a12cd5d",
      "8bf2445ddb8a674598f633cadb04ba7316fffc36"
    ],
    "message_list": [
      "Small typo fix (#1955)",
      "Switch from coverage.io to coveralls.io (#1969)\n\n* switch from coverage.io to coveralls.io\r\n\r\n* adding badges to README\r\n * travis\r\n * coveralls\r\n * rtd\r\n * pypi",
      "Update reentry to 1.2.1 (#2005)",
      "Update `paramiko` version requirement to `2.4.2` (#2043)\n\nThe version `2.4.1` had a high severity vulnerability CVE-2018-1000805\r\nthat has been fixed in `2.4.2`",
      "Add Max and MARVEL acknowledgements to docs (#2053)",
      "updated io tree response to add link type information (#2033)\n\n* updated io tree response to add link type information\r\n\r\n* updated further to handle multiple links between two nodes",
      "AiiDA conda environment (#2081)\n\n* add an `environment.yml` file for installing AiiDA's dependencies using conda\r\n* add a pre-commit hook that updates this file based on `setup_requirements.py`\r\n* add a Travis test that tests the conda install and prohibits adding new dependencies if they are not available on conda (either the `defaults` or the `conda-forge` channel`)\r\n* update reentry to 1.2.2, which is now available on `conda-forge`\r\n\r\nNote: Using 'defaults' channel (anaconda channel is currently breaking installs)\r\nhttps://github.com/conda/conda/issues/7872",
      "Add util function get_strict_version (#1686) (#2099)\n\nThe utility function `aiida.get_strict_version` will return a StrictVersion\r\ninstance from `distutils.version` with the current version of the package.",
      "updated rest api tests to fix issue of random sorting list entries of same type (#2106)",
      "add license file to manifest\n\nrequired by license and by conda-forge",
      "Merge pull request #2339 from ltalirz/add_license_to_manifest\n\nadd license file to manifest",
      "remove broken verdi-plug entry point (#2356)\n\nverdi-plug entry point does not work (likely a leftover)\r\n\r\n$ verdi-plug\r\nTraceback (most recent call last):\r\n  File \"/Users/leopold/Applications/miniconda3/envs/aiida_master/bin/verdi-plug\", line 5, in <module>\r\n    from aiida.cmdline.verdi_plug import verdi_plug\r\nImportError: No module named verdi_plug",
      "Disable caching for `InlineCalculation`. (#1872)",
      "add instructions when verdi import fails (#2420)\n\n* when verdi import fails because of an old/new export file version, add a hint on what to do\r\n\r\n * fix issue with pip 19.x (go back to pip 18.1)\r\n\r\n * fix issue with pre-commit failing to install. discussed in #2362\r\n\r\n * fix nasty travis issue with build of pymatgen (built using incompatible numpy version provided by travis)\r\n\r\n * upgrade pymatgen to 2018.12.12 (last version with py2 support)",
      "Add example for pytest fixtures with processes (#2432)\n\nExample kindly provided by @chrisjsewell",
      "let verdi help return exit status 0",
      "Merge branch 'release_v0.12.3' into issue_2355_verdi_help_exit_status",
      "Merge pull request #2434 from ltalirz/issue_2355_verdi_help_exit_status\n\nlet verdi help return exit status 0",
      "Backport postgres improvements to deal with setups without sudo (#2433)\n\nAlso updated `yapf` to recent version",
      "decode dict keys only if strings (#2436)\n\nbackport fix #2017",
      "Fix upper limit for dependency to `pg8000<1.13.0` (#2456)\n\nThe library `pg8000` which is used by `pgtest` to interface with\r\nPostgres and is used by us for testing purposed released a new minor\r\nversion `v1.13.0` on Feb 1 2019, in which it dropped `py2` support\r\ncausing our tests to fail miserably. For the time being, we put an upper\r\nbound on that dependency.",
      "Disable some SSH transport tests until Travis issue is solved (#2470)\n\nBackport of #2464",
      "rest api: iotree limit (#2458)\n\n* added tree_in_limit and tree_out_limit filters to tree endpoint of REST API\r\n* tree endpoint now returns total no. of incoming and outgoing nodes",
      "Added node label in the tree end point response (#2511)\n\n* Added node label in tree end point response\r\n\r\n* Returned number of passed incomings and outgoings in tree rest end point\r\n\r\n* updated tree rest endpoint test to check the returned node attributes",
      "Providing a non-ORM way to add nodes to a group. This speeds up a lot the group addition procedure and import method of SQLA should benefit from it (#2471)\n\nFast group addition, skipping the ORM\r\n\r\nThis PR solves issue #1319 and provides a fast way to add nodes to a group that circumvents the ORM. Import/export benefit from this; particularly import under SQLA.\r\n\r\nIn order to achieve this, it uses RAW SQL statements. It was agreed by @giovannipizzi  @sphuber \r\n and @szoupanos that this is acceptable for aiida 0.X. sqlalchemy >= 1.1 provides better solutions for this in aiida 1.0 (see comments in the code).\r\n\r\nFurthermore, it adds a new migration `7a6587e16f4c`.\r\nFor consistency, PR #2514 inserts this migration also in the upstream branch for aiida 1.0 (and handles collisions with other following migrations).",
      "Fix BandsData issue related to the k-points labels (#2492)\n\nfixes #2491 \r\n\r\n* fix problem with empty labels array\r\n* Fix bands plotting\r\n\r\nIf the first and/or the last kpoint label does not point to the first and/or\r\nthe last kpoint - the plotted band structure was cut on the left and/or right\r\nsides. To avoid this I add empty labels to the first and/or the last kpoints\r\nif they are not labeled.\r\n\r\n* Add documentation on how to use the BandsData object",
      "bug fix for `verdi node delete` (#2545)\n\nFix a bug such that if no argument is passed to `verdi node delete`. It will try to delete all notes in the database.\r\nThis is because `QueryBuilder().append(Node, filters={'or': []}, project='id').all()` match to every single node and projects its id. \r\nPerhaps this behaviour of `QueryBuilder` is not as expected?",
      "Update changelog for release `0.12.3` (#2455)",
      "Update authors and bump version to `0.12.3` (#2553)",
      "Update install docs (#2555)\n\nBackport streamlined quick install instructions from `provenance_redesign`"
    ]
  },
  {
    "pr_number": 1666,
    "commits_list": [
      "42281d07fe39dc6b645df104ff9c4067b536fe04",
      "5b8f2411e731055dcf359880fdbf216b25d65a3f",
      "44c5fe4de16ebb02234be1367145ec75feac7e0d",
      "c0d795034a5568cacec34945a06d98a6506e8cf7",
      "eeab898b79a2e5dbe82b9be6fcca71013873b20e",
      "03d5775ba2e4343716518af656f14762a366f3ac",
      "09b046a9aaf8396c10e9ac0450945acfeb5bb344",
      "b6acc081372d33a223dbdb968a0a1ecaa9ed091f",
      "de6a4737d8e259d729be647c2443f77cea1c8bb2",
      "cd12859dff9d67738614c0147771b5bb9a44990f"
    ],
    "message_list": [
      "add tests for missing behaviour",
      "Fixing a bug with click.edit\n\nClick uses the timestamp to see if the file has been modified\nin `click.edit()` but on some filesystems the timestamp has\na precision of 1 second",
      "Changing the logic of Interactive options and Conditional options\nto comply with  the logic we expect. Tests have also been adapted\nas they were not always correct.\nAlso, tests on empty_ok substituted with tests on empty string defaults\nas empty_ok has been removed from the code.",
      "Add some nitpick exceptions for click types and fix the logic to check that all parameters passed to a builder are used.",
      "The nitpick exception was incorrect",
      "Added prompt_fn option not to prompt interactive prompts\n\nE.g. when a previous option decides if the option should be printed or not\nAlso fixed the tests.\n\nNOTE: if the wrong number of interactive options is passed in a\ntest, this will enter an infinite loop and will hang the tests!\n\nTo decide if we want to have a way to prevent this (e.g. a max_prompt_count variable?)",
      "Merge branch 'verdi' of github.com:aiidateam/aiida_core into fix-interactive",
      "Merge branch 'verdi' of github.com:aiidateam/aiida_core into fix-interactive",
      "The change done in the default for description was making a test fail\nbecause now the 'verdi node description' was getting by default -D \"\"\nand therefore was setting an empty string rather than getting the current description",
      "Added documentation for `prompt_fn`\n\nAs required in the comments of #1666, and also\nfixed a couple of other minor things required in the comments\n(mainly about comments)"
    ]
  },
  {
    "pr_number": 606,
    "commits_list": [
      "2d8cff9ecfd588e199da03bb7bc3e7dc3618f56b",
      "47bb2a0b39b4d0d9d0484608202903960770dea0",
      "a59e93d315efdd19b7621e04985fe82a2dd063ca",
      "8f9f24e6cdf24369e648759f15e1633603c462fa",
      "fedc731a1e227d4184abbb9d2a8ee0078df93a38",
      "3eefe1901d695442e6c3b78712a3e40e500e654b",
      "5e4da13e94992f5b8bb23d33d4c99fdb187d89ec",
      "0432d15a50a32e644f38578d025f6f55e77f6f9c",
      "3dbcfc285cefb153f79e3c6035c6efb8faf7de8c",
      "d5efa7d78ad838d0df68978194f52b608b7392ff",
      "f76d43106e4480dcc6605c336753dcd014e30588",
      "9078c6950adf42ffe1c6233c6576e3e5738fc75b",
      "bc75b05064cee66a352bbfa0504bd2035c699a18",
      "fd76925775ddb6a86c02d73ce79c55a1cd7edfc1",
      "43ddd644301c5844d523531606b8fdb36b90a1c2",
      "157fbc2008d4a775fbccf02d2a39a5302d467746",
      "604413f7589fb969134bd26a6429c022de83c4d3",
      "6c8f05bf43ba2b1137e75c35a97a09046ac06772",
      "9bbf2a5570a6f80ddd47d238834fbb3e9e43fd6a",
      "be214246ee0a005918f1b5c0434cfa469f87e8fb",
      "dbc9f8c31d763e86358d4d73997436e172384276",
      "0783ce18592a2b488170e9aaa7ed0508d91a7279"
    ],
    "message_list": [
      "added prompt for profile name to verdi quicksetup\n\nSo far, the procedure for choosing a custom profile name\nin verdi quicksetup was a bit complicated:\nit first checked whether a profile 'quicksetup' was already present\n(which already requires sudo access);\nif it existed, the user was asked whether to overwrite the profile\nor not. by selecting no the user could then choose a new profile name.\n\nThis commit adds a prompt for the profile name in verdi quicksetup.\nIt is the very first prompt and it already has a default value\n(i.e. one can simply press \"enter\" and doesn't lose time).\n\nThis is important in particular for setting up test profiles,\nas those need to start with 'test_'.\n\nThe default name for the database was changed from\n    \"aiida_qs_<login-name>\" to\n    \"<profile>_<login-name>\"\nThis ensures that for test profiles the database name also starts with\n'test_'.",
      "Merge remote-tracking branch 'upstream/develop' into fix_XXX_verdi_quicksetup",
      "improved verdi quicksetup help\n\nand reverted default database user name back to aiida_qs_<login-name>",
      "fixed underscores and added documentation",
      "verdi help now shows command line options\n\nin addition to showing the doc strings of the functions,\nverdi help now shows the dynamically generated documentation\nof the 'click' options for verdi quicksetup and verdi setup.\n\nAdding this to other commands is achieved simply by adding\na static '_ctx' method that returns the appropriate context.",
      "removed default email in verdi setup",
      "added --set-default, --no-set-default options\n\nwhen specified, quicksetup will *not* ask whether to override\ndefault profiles for shell/daemon",
      "add verdi profile delete\n\nFinally a way to delete verdi profiles.\nAsks user whether to delete database & database user",
      "Merge branch 'develop' into fix_XXX_verdi_quicksetup",
      "minor improvements in documentation",
      "now deleting profile repository as well\n\nExample of command line usage:\n\n$ verdi profile list\n* default\n* test_mynewtest\n* test_aiida\n* my_test\n> tdevelop (DEFAULT) (DAEMON PROFILE)\n(aiida_ltal) leopold@tsf-428-wpa-7-059:~/.aiida\n$ verdi profile delete test_aiida\nDelete associated database 'test_aiida'?\nWARNING: All data will be lost. [y/N]: y\nDeleting database 'test_aiida'.\nDelete database user 'test_aiida'?\nWARNING: This user might be used by other profiles as well. [y/N]: y\nDeleting user 'test_aiida'.\nDelete associated file repository '/Users/leopold/.aiida/repository-test_aiida/'?\nWARNING: All data will be lost. [y/N]: y\nDeleting directory '/Users/leopold/.aiida/repository-test_aiida/'.\nDelete configuration for profile 'test_aiida'?\nWARNING: This permanently removes the profile from the list of AiiDA profiles. [y/N]: y\nDeleting configuration for profile 'test_aiida'.",
      "Merge branch 'develop' into fix_XXX_verdi_quicksetup",
      "Merge branch 'develop' into fix_XXX_verdi_quicksetup",
      "add checks, trim overly long lines\n\nverdi profile delete now checks whether the database of the profile\nactually exists and whether the database user is used by any other\nexisting profiles.\n\nfurther adjustments to coding style following Sebastiaans suggestions.",
      "Merge branch 'develop' into fix_XXX_verdi_quicksetup",
      "Merge branch 'develop' into fix_XXX_verdi_quicksetup",
      "add --yes option for verdi profile delete\n\nfor running in interactive mode",
      "Merge branch 'fix_XXX_verdi_quicksetup' of github.com:ltalirz/aiida_core into fix_XXX_verdi_quicksetup",
      "fix bug verdi profile delete\n\nargs was a tuple, needs to be list",
      "Merge branch 'develop' into fix_XXX_verdi_quicksetup",
      "improve comments",
      "switch from --yes to --force\n\nas pointed out by Rico, --force conveys that the questions\nare answered in a way such as to lead to removal of the profile"
    ]
  },
  {
    "pr_number": 5277,
    "commits_list": [
      "32a839cd4ed9f3f1c58430b14c1534830a31d344",
      "0cd1c3c9557da781c340dc9eed202bb945d80655",
      "14111e50a668a333058f6962d82ad5b6ed97c86f"
    ],
    "message_list": [
      "CI: move Jenkins workflow to nightly GHA workflow\n\nUp till now, we have been running a Jenkins server to run certain tests.\nThe idea was to use this server to run more heavy integration tests that\nwould clog the CI pipeline, which was running on Travis at the time.\n\nThe only test that was added and has been running since are the Reverse\nPolish Notation (RPN) workchains. These have uncovered a bug or two, but\nit has been rare and the burden of maintaining the Jenkins server is not\nneglible. In addition, since then we have moved from Travis to GitHub\nActions as the CI platform, and throughput has not been a problem there.\n\nTherefore, we move the RPN tests to a GHA workflow that runs nightly.\nThis allows us to decommission the custom Jenkins server while\nmaintaining the additional coverage that the tests provide, without\noverloading the CI by not having them run on every commit.",
      "Move nightly to the `ci-code.yml` workflow\n\nUltimately, the RPN workchains don't take that long to run, so they are\nbest included with the rest of the tests of the CI flow.",
      "Merge branch 'develop' into fix/5276/move-jenkins-to-gha"
    ]
  },
  {
    "pr_number": 5061,
    "commits_list": [
      "351684c340d1ef36cf58a7a0457cf0ea032cb0bb",
      "5a1ca687e7835d9896ed36299971c468fae80c6d",
      "9d2e4d72d5fb0208fc2f80184396f97a854daad2",
      "d5f41419901fb57fed5c0d81a92dcfa2c1825b71",
      "1462dff1e12df67b813cc36ff86d3b531d795271",
      "0e4d6368308ecba057cb4421e8eed2591b28f441",
      "1a55259f5e46445e245a5b692b2cb37093980d02",
      "e23b6b4f51f94ef2edb9470c771c5b3ec4f35081",
      "5b29b38e4be9f913d5fc9056b8275dada8006f3d",
      "797b7f64a6d144d57ccd842cb8217736ec9227dd",
      "e3b386196eb9d5b4be67b4bf979210269eca1900",
      "fce7f62c2b4426c1db06c0005a14519f5de70925",
      "70868bf00d56dc16edb96461997bc8e9cddd4a52",
      "7e2dc6a68f0a49ae201af7a18af8d72482320ca5"
    ],
    "message_list": [
      "\u267b\ufe0f REFACTOR: Make `__all__` explicit",
      "improve",
      "pre-commit fixes",
      "add `load_entity` to __all__",
      "Update __init__.py",
      "allow `*` skip",
      "\u2b06\ufe0f UPDATE: mypy v0.910",
      "Merge branch 'develop' into __all__",
      "Merge branch 'develop' into __all__",
      "Apply suggestions from code review\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "Merge branch 'develop' into __all__",
      "Update aiida/common/log.py",
      "Move loaders",
      "remove VALID_DICT_FORMATS_MAPPING"
    ]
  },
  {
    "pr_number": 4615,
    "commits_list": [
      "f057efbf1314c2d9ff8c3be6987dadfd61e36920"
    ],
    "message_list": [
      "CI: manually install `numpy==1.19.4` to prevent release candidate\n\nThis is the usual problem that occurs when `numpy` releases a candidate\nrelease. Even though we specify limits to not install this, `pymatgen`\ninstalls it as a dependency in their build process where limits are\nignored and so the latest release gets installed. In this case\n`numpy==1.20.0rc1` is getting installed and that dropped support for\nPython 3.6 and so our builds on that version brick.\n\nThe workaround is to manually install the compatible version, in this\ncase `numpy==1.19.4` before installing `aiida-core`. This way `pymatgen`\nwill simply use that in their build process."
    ]
  },
  {
    "pr_number": 2757,
    "commits_list": [
      "30a9329155fa8573c1d2f76b2b7377c568e88c61"
    ],
    "message_list": [
      "Print formatted spec for `verdi plugin list <entry_point>`\n\nWhen a valid entry point is passed an the corresponding class is a sub\nclass of `Process`, the `get_description` message will return the\nprocess spec which is then passed to a function that will print it in a\nhuman-readable formatted way to screen. It will give an overview of the\ninputs, outputs and exit codes defined on the spec."
    ]
  },
  {
    "pr_number": 4011,
    "commits_list": [
      "d44f7dfcad3dfa048fe5e7534538b3939620f0ce",
      "8d1806b180020f48f59af40f2c8c4bf028a8c05a",
      "c50570051de5dc1cfeb7bd127b43408401703989",
      "31426bfd9c2eaf1e42ea7f561b1ef7c4dfa36d2e",
      "c946c9f245374e739f9bc25044f1df8d6ab22521",
      "19bfdfb2666ab2b7641bb8f6ca7a8447838a633e",
      "60335de44de48bd6161cbc9da24c91023c2179d5",
      "e765709a3eede6acd69ed451496e766b8d423d14",
      "3e506b9e0fe74d9b7717fb7b0f2d5b03efe8fbe3",
      "0b5dc40be471b772d72f71fe11284651b138ab87",
      "40fa5e1f3b3fb6060bf9de46747c75ff41f207f4",
      "c6fdeb39ded24ffaa21d8387d7976ecf1d465749",
      "80449ce81809c41d11ebf223a1311f9c7fa42e6e",
      "5209849e0dc00acc69c7609351b6bf2d3af01d7b",
      "c2af9340adee5acdbb60385723725a96ab05daf4",
      "0e5a0a89bda1c20cb105be203612c766cb139776",
      "4a68f395fff6c5d640edcb742df6d914399eebc2",
      "f5e089588ccc0bc2adfe70d9a11a4f620495fedd",
      "76270cac2d5413e2095ebc2c9e098311991d8836"
    ],
    "message_list": [
      "Intial draft of installation guide",
      "Add basic accordian directive",
      "correct javascript",
      "add indicator",
      "add panels sphinx extension",
      "Update docs/source/_extensions/_static/accordian.css\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "improve directives and add link-box",
      "add to doc string",
      "Update landing page",
      "update toc pages",
      "move panels extension to external dependency",
      "Adress review suggestions",
      "fix typo accordian -> accorion",
      "Update docs/source/intro/get_started.rst\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Update docs/source/index.rst\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "update",
      "Merge branch 'docs-revamp-intro-install' of https://github.com/chrisjsewell/aiida_core into docs-revamp-intro-install",
      "pre-commit fix",
      "add open issues"
    ]
  },
  {
    "pr_number": 5033,
    "commits_list": [
      "e473aace2d967f56828276c8ed4d1b7474f5747b"
    ],
    "message_list": [
      "Add the `ElectronicBandsData` data plugin\n\nThis data plugin is a subclass of the `BandsData` plugin. It adds\nsupport to define a fermi energy, which is set as an attribute. This is\na typical use case for electronic band structures, which so far were\nusing the `BandsData` type, but the latter was also being used for other\nband structures, such as phonons, which do not share this property.\n\nBy making it a subclass, existing processes that accept `BandsData`\ninstances will automatically also accept `ElectronicBandsData` instances\nand queries for `BandsData` will also match the subclasses, unless the\nuser turns of subclassing explicitly.\n\nThe constructor would have used the `/` marker to make the `kpoints`,\n`bands` and `fermi_energy` arguments positional only. The reason is that\nthe constructor needs to allow for arbitrary keyword arguments to be\npassed to the parent constructor. This is necessary to be able to change\nthe `user` of the node for example. Without making the other arguments\npositional only, a caller could define them as a keyword argument,\ncausing them to be passed to the constructor of the parent class, which\nwill raise an exception since it does not define those arguments.\n\nHowever, we unfortunately still support Python 3.7 and `/` was not\nadded until Python 3.8, so for now we cannot use it yet."
    ]
  },
  {
    "pr_number": 3882,
    "commits_list": [
      "9c9737685a92ab5435e39b3e45805673ded1c250"
    ],
    "message_list": [
      "Make `Group` sub classable through entry points\n\nWe add the `aiida.groups` entry point group where sub classes of the\n`aiida.orm.groups.Group` class can be registered. A new metaclass is\nused to automatically set the `type_string` based on the entry point of\nthe `Group` sub class. This will make it possible to reload the correct\nsub class when reloading from the database.\n\nIf the `GroupMeta` metaclass cannot retrieve the corresponding entry\npoint of the subclass, a warning is issued that any instances of this\nclass will not be storable and the `__type_string` attribute is set to\n`None`. This can be checked by the `store` method which will make it\nfail. We choose to only except in the `store` method such that it is\nstill possible to define and instantiate subclasses of `Group` that have\nnot yet been registered. This is useful for testing and experimenting.\n\nSince the group type strings are now based on the entry point names, the\nexisting group type strings in the database have to be migrated:\n\n * `user` -> `core`\n * `data.upf.family` -> `core.upf`\n * `auto.import` -> `core.import`\n * `auto.run` -> `core.auto`\n\nWhen loading a `Group` instance from the database, the loader will try\nto resolve the type string to the corresponding subclass through the\nentry points. If this fails, a warning is issued and we fallback on the\nbase `Group` class."
    ]
  },
  {
    "pr_number": 5003,
    "commits_list": [
      "733f1e89d5857eb2e54871662b4aa6d478d37bf4"
    ],
    "message_list": [
      "Docs: fixing links and linkcheck on Github action"
    ]
  },
  {
    "pr_number": 3392,
    "commits_list": [
      "523c33657abb2c3332d185ec558f374e97ca0040",
      "56787de022350b5937379994aad60f0304ab38fb"
    ],
    "message_list": [
      "Add test on Travis to check the loading time of `verdi`\n\nThe loading time of `verdi` needs to be kept to a minimum in order to\nkeep it snappy and tab-completion from workable. This means that the\ndatabase environment should not be loaded until it is absolutely\nnecessary and only within the function body of commands. This means that\n`aiida.orm` cannot be imported in the top level of any `aiida.cmdline`\nfile, as that will_ trigger the loading of the database environment.\nHowever, since this is not easy to spot, this happens all the time,\nbreaking `verdi` essentially.\n\nWe add a test to the Travis build that will use `time` to measure the\nloading time. If the given limit is exceeded a certain number of times\nconsecutively, the test will fail. The limit is set rather loosely,\nbecause we do not want numerical noise to fail this test. The typical\nmistake causing the loading time to increase, should largely exceed the\nlimit so should always be caught.",
      "Add `verdi devel check-load-time` to detect loading slowdown\n\nThe command will check for two indicators that commonly slowdown the\nloading time of `verdi` significantly:\n\n * the database environment is loaded\n * the `aiida.orm` module has been imported\n\nIf either of these conditions are true, the command will raise a\ncritical error. This can then be used in CI as an additional tool to\ndetect `verdi` load time problems."
    ]
  },
  {
    "pr_number": 4355,
    "commits_list": [
      "27498d4506667f610fd754c4e28fe34460606d8e",
      "8ed6cd3f056d6cb688c2d671c8c6763500434ec3",
      "ae5e294899360ccf05cc32d28ab76c30aec8ad8c",
      "4821096893d42866e01f18113e955a182f85a6de",
      "60a9a6ec02391bfdf65aac43404a1da54a1a34d4"
    ],
    "message_list": [
      "\ud83d\udd27 Add tox configuration",
      "\u2b06\ufe0f UPGRADE: Move to tomlkit",
      "Merge branch 'develop' into add-tox",
      "Apply suggestions from code review",
      "fix pre-commit"
    ]
  },
  {
    "pr_number": 4671,
    "commits_list": [
      "01a0f3e8ec4432dedc55b48fea734be65a54a074",
      "e53f6ab4298ea4fb9f4ab4b528e2c8dc4106f199",
      "dad68aeccd5cc033b27b0c1b2b607dfcec330d73",
      "0241319357712bc229481b78d255594fc8c9fc3b",
      "020e0086bccbeec30157cd858dc2cf61d429d140",
      "7d243ef2130efd92db647d9145faa24fe19b8210"
    ],
    "message_list": [
      "Chore: moved `test_pause_play_kill` to pytest",
      "Command `play all` only affects active processes.",
      "Adapted the test to the temp fix.",
      "Apply corrections from review",
      "Add run_cli_command",
      "Merge branch 'develop' into playall"
    ]
  },
  {
    "pr_number": 3934,
    "commits_list": [
      "b26dbbaa4d429dad6083da962ae4df5b2689b141",
      "03baa9830a75356b3e30c02f6ef2d7a2acd2c22c",
      "8156bb14a6da241d4e9ad7d2508ab80bb1a7df22",
      "ba83912416dc06b367ea19c82e13119939c267d1",
      "ed7a0b60ab7d2132fb521fa800685f66413febb8",
      "1dde8ea538b4c5f7193d07cd8ee3f621343e54e0",
      "8e309352e5085012124a18586129c26157a25a68",
      "c61ef49d0acf4609d3736e3cd1e703578edbb9fa",
      "035fbba2017785ebc93320740d8b05ea917ca449",
      "7e99c068faeabe7ff8fdc9168f2d70dad247a902",
      "87e22f41797781641c6712f1c408d39dd0f40d9f",
      "8202ab152cc1668907f38636e53e792d753f8784",
      "19365fa78077375ae55789dfe8bbaf018a13ea7d",
      "e234fae15265a912076ffacaf51a05315a68f604",
      "4a49c054766568a1b8a6f186067c1ab0f05ecfea",
      "733861a6e6fd2ea50c4f50a90553f876e7662518",
      "12465534b578bee9fdd44984bbe4ad089a955126",
      "04f5b51d239de944815ae727aa0927b8c4c255e8",
      "44e12d70412d80711a50435c944f62d6f0d6e501",
      "d0b89c19a306901f67b78def6344f76a1212d01e",
      "b90a6298c7843d33c0fc1f0099d71f54bca6eca6",
      "9ce2183bbac741a230e7ff46149ba6a0fcd6bfde",
      "87fdf09c2969f30e0dd35a30de5cb94fd2a8472c",
      "d2af96c3b14506e0ca4023993e91fa79a45e2525",
      "57b8a5a3b26b5dd4f4edd2194871dafbff9c074c",
      "2c2f3cfec1d7cd71537769ec42854ebc241f42a5",
      "d12a424e0ae47616df26a90ce2f6f093f9618fa2",
      "58bace313a7dffdd576679feae87fb2a029bd4cf",
      "0f069a70035337b880a0750f94adb0ada0d1f45f",
      "5e226e7035d339a907fc8c2edfd8e5956513a1b4"
    ],
    "message_list": [
      "(re-)implement coverage reports via codecov.io\n\nfixes #3602",
      "Update README coverage badge\n\nOnly upload coverage for Py3.5",
      "Remove omissions from coveragerc and reorder tests",
      "Build status badge: move to github actions (#3825)\n\nReplace outdated travis build status badge with github actions one.",
      "Add `prepend_text` and `append_text` to `aiida_local_code_factory` pytest fixture (#3831)",
      "Update coverage upload step\n\nRevert to the continuously updated v1 tag for the action.\nFail CI if failing to upload coverage, since we can upload coverage for\nall PRs.\nRemove secret token. It is now possible to upload coverage reports to\ncodecov without a token (from public GitHub repositories only).\n\nUpload coverage only if:\n- Python version: 3.5; AND\n- Is a push in or PR against repository 'aiidateam/aiida-core'.",
      "Use actions/checkout@master\n\nThis is an updated and more robust version of the checkout action.\nSee https://github.com/actions/checkout for more information.\n\nOriginally, v2 was attempted, but this clashes with being able to upload\ncoverage, hence `master` is used.",
      "Pin the Click version to 7.0.\n\nThe recent update to Click (7.1) breaks our tests and this CI workflow,\nbecause the help output formatting was slightly changed.\n\nUntil we have resolved other issues we should pin the Click version to\n7.0.",
      "Merge pull request #3837 from aiidateam/dm/pin-click-7.0\n\nPin the Click version to 7.0.",
      "Revise dependency management workflow (#3771)\n\nRevise dependency management workflow in accordance with AEP 002.\r\n\r\n * Use .github/CODEOWNERS file to trigger reviews from\r\n dependency-manager team.\r\n * All jobs related to testing the installation of aiida-core with\r\n different tools (pip, conda) on different python versions and backends\r\n are moved into the dedicated 'install-tests' workflow.\r\n * The test jobs run as part of the continuous integration on every push\r\n are executed against pinned environments (e.g.\r\n 'requirements/requirement-py-37.txt').\r\n * The 'install-tests' workflow is triggered for the 'master', 'develop',\r\n and release branches ('release/*') as well as branches related to\r\n dependency-management (prefixed with 'dm/'). In addition, the workflow\r\n is triggered nightly to ensure that changes within the Python ecosystem\r\n that break our toolchain and possibly even tests, are automatically\r\n detected within 24 hours.\r\n * A new 'update-requirements' workflow is run on release branches and\r\n automatically creates a pull request with revised versions of\r\n requirements files.\r\n * The issues caused by pymatgen's use of setup_requires and previously\r\n addressed by installing numpy prior to aiida by default, are now\r\n addressed by specifically checking the setuptools version for Python\r\n 3.5 only. We fail the installation process with a descriptive message\r\n to the user in case that the installed setuptools version is\r\n insufficient.\r\n * All dependency-management related utility functions, such as\r\n generating and validating non-authoritative dependency specification files\r\n (e.g. 'environment.yml') are concentrated into the\r\n 'util/dependency_management.py' scripting file.",
      "Fix workflow syntax for test-install and update-requirements workflows.\n\nUsed incorrect 'on.push.branch' instead of the correct\n'on.push.branches' triggers.",
      "Update paths triggers for test-install workflow.",
      "Only create update-requirements PR on the primary repository.\n\nAnd not on forks.",
      "Update requirements directly on same branch.",
      "Update requirements files.",
      "Merge pull request #3839 from aiidateam/update-requirements-workflow-triggers\n\nUpdate requirements workflow triggers",
      "Use actions/checkout@v2 (instead of @master) (#3846)\n\nThe actions/checkout@v2 GitHub action seemed to cause issues with the codecov/codecov-action@v1 before, but it seems this is not (or no longer) the case.",
      "Dm/revise update requirements workflow (#3847)\n\n* Implement the dependency_management 'check-requirements' command.\r\n\r\nTo check whether the environments frozen in the 'requirements/*.txt'\r\nfiles are matching the dependency specification of 'setup.json'.\r\n\r\n* Implement 'check-requirements' GitHub actions job.\r\n\r\n* Checkout specified head_ref in update-requirements workflow.\r\n\r\n* Execute 'update-requirements' workflow only upon repository_dispatch.\r\n\r\nWith type 'update-requirements-command'.\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Dm/auto generate all (#3848)\n\n* Auto-generate all dependent requirements files on commit.\r\n\r\n* Add packaging to 'dev_precommit' extra requirements.\r\n\r\nRequired by the dependency_management script.",
      "Suggest running `reentry scan` when entry point cannot be resolved (#3765)\n\nThis is the case either when the entry point cannot be found at all or\r\nwhen multiple entry points are registered with the same name.",
      "Add the `-l/--limit` option to `verdi group show` (#3857)\n\nThis is a very useful flag that used to be there at some point.",
      "Docs: consistent use of \"plugin\" vs \"plugin package\" terminology (#3799)\n\n * Apply naming convention of \"plugin\" vs \"plugin package\" consistently\r\n   across the whole documentation\r\n * Update plugin design guidelines with suggestions on how to think\r\n   about which information to store where and further improvements",
      "Docs: remove extra `advanced_plotting` from install instructions (#3860)\n\nThis requirement extra does not exist, and has not existed since a very\r\nlong time.",
      "Add the `--order-by/--order-direction` options to `verdi group list` (#3858)",
      "`ExitCode`: make the exit message parameterizable through templates (#3824)\n\nOften one would like to define an exit code for a process where the\r\ngeneral gist of the message is known, however, the exact form might be\r\nslightly situation dependent. Creating individual exit codes for each\r\nslight variation is cumbersome so we introduce a way to parameterize the\r\nexit message. We implement a `format` method that will format the message\r\nstring with the provided keyword arguments and returns, importantly, a\r\na new exit code instance with the new formatted message.\r\n\r\n    exit_code_template = ExitCode.format(450, '{parameter} is invalid.')\r\n    exit_code_concrete = exit_code_template(parameter='some_specific_key')\r\n\r\nThe `exit_code_concrete` is now unique to `exit_code_template` except\r\nfor the message whose parameters have been replaced.\r\n\r\nTo enable to implement a new method, we had to turn the `ExitCode` from\r\na named tuple into a class, but to keep backwards compatibility, we sub\r\nclass from a namedtuple, which guarantees that the new class behaves\r\nexactly like a tuple.",
      "Convert argument to str in `aiida.common.escaping.escape_for_bash` (#3873)\n\nWithout conversion to string, any non-string type will cause the\r\nfunction to raise an `AttributeError` since it won't have the method\r\n`str_replace`.",
      "Remove unused `orm.implementation.utils` module (#3877)\n\nThe `get_attr` function defined here seems to be used nowhere in the\r\ncodebase.",
      "Docs: enable `intersphinx` mapping for various libraries (#3876)\n\nAdd intersphinx mapping to python standard library documentation and of\r\nvarious direct dependency libraries:\r\n\r\n * `click`\r\n * `flask`\r\n * `flask_restful`\r\n * `kiwipy`\r\n * `plumpy`\r\n\r\nAdding this documentation interconnection allows references in our docs\r\nlike :py:exc:`ValueError` to now directly link to the relevant external\r\ndocumentation. This also allows to cut down the \"nitpick exceptions\"\r\nquite dramatically.\r\n\r\nCo-Authored-By: Sebastiaan Huber <mail@sphuber.net>",
      "REST API: fix the interface of `run_api` (#3875)\n\nThe current `run_api` interface for running the AiiDA REST API had\r\nunnecessarily many parameters, making it complicated to use in WSGI\r\nscripts, e.g.:\r\n\r\n    from aiida.restapi import api\r\n    from aiida.restapi.run_api import run_api\r\n    import aiida.restapi\r\n\r\n    CONFIG_DIR = os.path.join(os.path.split(\r\n                      os.path.abspath(aiida.restapi.__file__))[0], 'common')\r\n\r\n    (app, api) = run_api(\r\n        api.App,\r\n        api.AiidaApi,\r\n        hostname=\"localhost\",\r\n        port=5000,\r\n        config=CONFIG_DIR,\r\n        debug=False,\r\n        wsgi_profile=False,\r\n        hookup=False,\r\n        catch_internal_server=False\r\n    )\r\n\r\nWhile all but the first two parameters are keyword arguments, the code\r\nwould actually crash if they are not provided.\r\n\r\nIn reality, there is no reason to have to specify *any* parameters\r\nwhatsoever and one should simply be able to call `run_api()`.\r\nThis commit accomplishes this by defining the appropriate default\r\nvalues.",
      "Bump bleach from 3.1.1 to 3.1.4 in /requirements (#3880)\n\nBumps [bleach](https://github.com/mozilla/bleach) from 3.1.1 to 3.1.4.\r\n- [Release notes](https://github.com/mozilla/bleach/releases)\r\n- [Changelog](https://github.com/mozilla/bleach/blob/master/CHANGES)\r\n- [Commits](https://github.com/mozilla/bleach/compare/v3.1.1...v3.1.4)\r\n\r\nSigned-off-by: dependabot[bot] <support@github.com>\r\n\r\nCo-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>"
    ]
  },
  {
    "pr_number": 4835,
    "commits_list": [
      "6a23ac464272481a7ed7b03d44af8dc4aa93e1a7",
      "46720d545ec6ae6a2b571ada0ce5459a977545e0"
    ],
    "message_list": [
      "Add versatility to auxiliary tool",
      "Proof of concept for grafana"
    ]
  },
  {
    "pr_number": 5280,
    "commits_list": [
      "ccc5329dbc7b9b9c7a231acf0f5de2eff8db7952",
      "581695693b9758ba21ee84e6aebe188a8193188d",
      "e61fd73abb5a4a962fc3887dffb182f6c6f4cbe1",
      "56fd860373781395a7a754feba48f04c3d86ff63",
      "034e77b382f759e22d5f4f08d3dc324d02e143f4",
      "54ad5688825b63f8fabfcc46996b98bc1bee6fca",
      "77e996dbcb101d86c565afccb1d7f649b1ee6c61",
      "2e968b43adc39e3e269da36f7b5f7efba7978c30",
      "a8c5ce1985dcdd107815c2da7c909de2a2259493",
      "2f5b53f48f6cf70a62ee933077ac76e58188defa",
      "02067c33f4a4586401d7a80e6e9b5948f0e06eda",
      "549f43f73bbe8f11f2cdba81a5b516e9011a63f6",
      "e1e758d256f5604ae00c4aadff40182f10eddfaf",
      "9e23623321e5f0ecc1913c84ef74f9c3d5c88e1b",
      "ba40f5aec77eb0eed8f31d5bee0bf09607735e00",
      "0b2db9ded19acffedd8905e751ad6719a66263ba"
    ],
    "message_list": [
      "escape bash with double quotes as an optional",
      "address escape issue for ENV variable",
      "review",
      "double quotes set from computer and to exec command line",
      "update test",
      "seperate std input output",
      "computer_cmdline_params",
      "add use_double_escape to Code and CodeInfo",
      "unittest",
      "add computer_cmdline_params to code_info",
      "Add custom string for code info to maximum flexibility",
      "rename to prepend_cmdline_params",
      "code_info use_double_quotes as a tuple",
      "explicitly set use_double_quotes in every scheduler test",
      "code setup",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci"
    ]
  },
  {
    "pr_number": 5194,
    "commits_list": [
      "ac19d1757992b4a7fc9f1c71997b6e36d3bb4482"
    ],
    "message_list": [
      "CLI: Warn for already existing ``code@computer label`` in ``verdi code setup`` and ``duplicate``\n\nImplement reprompting mechanism in ``verdi code setup`` for code label/computer combinations that are already in the database. The Computer option gains callback checking the DB for the set code label/computer combination\n\nIf it is found the user is asked if he would like to change the label. If that is the case the label is set to `None`, which triggers a second label argument, which will perform the same check as the computer option but it will raise an error and reprompt instead. This second label argument is ``hidden=True, expose_value=False`` to avoid duplicate help messages and name clashes with the other label option\n\nWorks both for remote and local codes. For non-interactive commands an error is thrown"
    ]
  },
  {
    "pr_number": 616,
    "commits_list": [
      "3f65613f71c3a66b30d79dc25b25142dbd9f020b",
      "871466fb154a808ac6eee666a42cd440590c1f2f",
      "edca45cec0a7e1a225950d3a7b06ee02406aa133",
      "ee01a7db67bf643e16e914baf766a536aefcb6af",
      "680df5748de1ab0d863de63af765560002487593",
      "a85146f3a3587deab6cafab76100eb42c3abc966",
      "4dba7e4d27c30c1e39aa4d97981ae230e2d0f635",
      "c58b7a087e40c72fec3e483d972f845ba922e437",
      "ef00185d994a756749431a3299c289315531486c"
    ],
    "message_list": [
      "Add logic to get AIIDA_CONFIG_FOLDER from AIIDA_PATH environment variable",
      "Change how __version__ is evaluated in setup.py",
      "Make sure AIIDA_CONFIG_FOLDER is the same when starting daemon",
      "Fix incorrect access to AIIDA_CONFIG_FOLDER",
      "Merge branch 'develop' into configure_aiida_config_folder",
      "Print config folder in 'verdi profile list'",
      "Merge branch 'configure_aiida_config_folder' of github.com:greschd/aiida_core into configure_aiida_config_folder",
      "Merge branch 'develop' of github.com:aiidateam/aiida_core into configure_aiida_config_folder",
      "Add documentation about the AIIDA_PATH option"
    ]
  },
  {
    "pr_number": 3985,
    "commits_list": [
      "ee0166911af75c322ad40c231edbce5c0a602d19"
    ],
    "message_list": [
      "Fix autocompletion for LinkManager and AttributeManager\n\nIn the managers, exceptions were not caught properly.\nFor instance, `getattr(calc.inputs, 'value')` was not return\nan AttributeError if `'value'` does not exist, and as a\nconsequence `getattr(calc.inputs, 'value', None)` was raising\ninstead of returning `None`.\n\nSimilarly, I fixed `calc.inputs['value']` to raise a KeyError\nfor not-existing `value`.\n\nThis is now addressed by defining a new compound exception,\nthat inherits both from NotExistent (for backwards-compatible reasons)\nand from the correct base exception of python (AttributeError or\nKeyError).\n\nThe AttributeManager was not having Tab completion for a different\nreason, `__dir__` was returning tuples of dict items, rather than\njust the keys.\n\nNow these are fixed and tests are added (I cannot really test\nthe TAB-completion functionality, but at least I test that the\nvalues of `dir()` and the exceptions raised are the correct ones).\n\nFixes #3984"
    ]
  },
  {
    "pr_number": 3952,
    "commits_list": [
      "047643f65e38b54ab2215ed13a967efd98231cc1",
      "9badb63170a4864b07e0184b822f44231f07c5ff",
      "e56269626b20d6214918420f197a99168378b447",
      "ad7b4c95f1621084ae3638159dfce41dfbdf987a",
      "2470e1cc2b817880548d7e3dc6466c033de7e55a",
      "9572be11baa43d0885902afcd5f341475c31bc24",
      "d92f1d7ea5beb526ddf39b2ae052d39e487c60ca",
      "bbca5b52a69734dbbb2151c6516177c8bdcf508a",
      "b0f2ef5c0c74be29945074e1d46c74e2cb37c994",
      "9b2f22299d7c6793f76489773414de98c7256a00",
      "2eb222e76a6a5ea4cdf4b8c14b60ecd9ce242679",
      "506a6c676a8f96592bd2b6c930e3ffb536279203",
      "3cc9a039d4f8e923884afedc64513dfc8e4f9bfb"
    ],
    "message_list": [
      "Add documentation for `GroupPath`",
      "Merge branch 'develop' into fix-3913-grouppath-docs",
      "fix trailing whitespace",
      "Merge branch 'develop' into fix-3913-grouppath-docs",
      "Fix typo",
      "Merge branch 'develop' into fix-3913-grouppath-docs",
      "Update docs/source/working_with_aiida/groups.rst\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Merge branch 'develop' into fix-3913-grouppath-docs",
      "Merge branch 'develop' into fix-3913-grouppath-docs",
      "Fix error in example",
      "Merge branch 'develop' into fix-3913-grouppath-docs",
      "add more info on cls",
      "one line per sentence"
    ]
  },
  {
    "pr_number": 3892,
    "commits_list": [
      "ffbe6a268c9a67275c582921fa78228798586aab",
      "dc98acd66d6d1c7984ba0cfcd14756bad57f313e",
      "a7c4ad5fde9e633486b0eb2269d164a0f34943c0"
    ],
    "message_list": [
      "move PGSU out to separate package\n\nOne of the main issues with `verdi quicksetup` is that it is non-trivial\nto figure out how to connect as the PostgreSQL superuser in a wide\nvariety of operating systems and PostgreSQL setups.\nThis PR factors out the code which takes care of this connection into a\nseparate package, which can then be tested on a wide variety of setups\nusing CI.",
      "update requirements files manually",
      "fix docs"
    ]
  },
  {
    "pr_number": 4894,
    "commits_list": [
      "59eedf393f73c745eda3d35d0895e5a29fcb5e9d"
    ],
    "message_list": [
      "Reinstate `aiida.manage.tests.unittest_classes`\n\nAlthough this was officially deprecated and slated to be removed for\n`v2.0` the deprecation warning only showed when using the `TestRunner`\nclass. However, many plugins only used the `PluginTestCase` run directly\nthrough `pytest` and so didn't see the deprecation warning. Therefore,\nwe reinstate the module for now and will remove it in a couple of\nmonths."
    ]
  },
  {
    "pr_number": 1764,
    "commits_list": [
      "15a4fa292886ceb9d06fcaf36b9ee821cbdb789b",
      "16b7bc0b0b9b2209cc91f0b8e4d27ca225238d2f",
      "0fbc0c1bb3a6db3646ade00cc2fbdc80d0ce4002",
      "f4a89637267d55738cde20556593db9907cf0ca8",
      "708e97b2d822337cb3dc1110ed5238b9d216f2c8",
      "e1d8ba799e192597dc2ecf83b945eefe30aac764",
      "4390d8940285cf2eb6a88af1388c0c4559a1e540",
      "ae8d43608419185b2435e9469f9419f9c9255a3e",
      "36e97004873f2d5c421f929ecae9877307d1ddbf",
      "1dcd7501a196b66ae36a69060a02a5b26df355e6",
      "130e99ed8b0fbf52ae0571ffb43ccea1f026df66",
      "1cbadfeb2f5ab8f73359482e95109e6eb3dca624",
      "b2bbf670875d1c6f4c8a0d35b9f67679a7838ef8",
      "3234389c2c43bef0ddd6675fb4974fbaf1637ef1",
      "3b4de69fe7958c69ab4481d5eb0d11237b5bc9ae",
      "5f366cb90a13ee3e85a2a084c87d8f248ea3c66f",
      "a0a959036d0aa36dd8e122460a8ed68fded0931e",
      "06490d20e5d6d5dd7ba6a6e77828b6e26d01741d",
      "bf1f961338ba3749f016c4e081006e01bdb4e978",
      "19d00422fa9eb483a949b5a981628138e9aadd81",
      "4e82a784aac4da84c3f795fa46162515d62410ac",
      "831fa18ed36875c5d69ff96e6ffabe1bd5f1a4f7",
      "dec0183a5536a86b41756758a44c850c8b5f4b76",
      "d66db315e7392d1d7dc36a5c8abcfc2f86a0fb8f",
      "cb76cf0f1db9180573bd5b144f2bb8b5f48490ab",
      "7668131028ed7e98d348e154ba373203a71d89f8",
      "bfbc086c2dc7962297c93ace4bf10c1e7734b9c0",
      "d8ba932999bf2ca77483fd8ba11d337c4dd4ab44",
      "042029e75557dd69b3844ea21f07dbcb9af6cdc8",
      "de903772871d541356edf6136b04e0031a112d7d"
    ],
    "message_list": [
      "Reorganising/extending the import/export tests",
      "More progress on export graph traversal",
      "Changing the export to accept AiiDA objects instead of Db objects",
      "Adding the new graph traversal logic to the export function",
      "Small corrections",
      "Various corrections to the graph traversal algorithm. We have to think what we do about the exported links since not all the predecessors are now exported (e.g. Calc-Calc in the reverse way is not exported)",
      "More progress on link export",
      "Link export seems to work?",
      "All tests pass",
      "Activating some more tests",
      "Cleaning up",
      "Small changes to the make the TCOD tests to use the new import/export parameter format",
      "Removing not needed code and modifying export_tree signature",
      "Cleaning the code, making docs to run without warnings",
      "Adding optinal rules to graph exporation. It remains to be done for the links",
      "Adding optional rules for link extraction",
      "A Code is treated as a Data node (and can be exported if it is referenced by a Calc or if it is exported independently). Tests added",
      "Adding correct export of code related links. Tests added",
      "Merge remote-tracking branch 'origin/release_v0.12.2' into issue_1102_b",
      "Small changes to make tests work again after the merge of 0.12.2",
      "Proper handling of empty import files. There is no need for strict exceptions that make the import of a bunch of import files to stop",
      "Minor fix for the previous commit.",
      "Cleanning the code, adding missing cli arguments",
      "Merge branch 'release_v0.12.2' into issue_1758",
      "Fixing unindent issue to make doc tests to pass",
      "Merge branch 'issue_1758' of https://github.com/szoupanos/aiida_core into issue_1758",
      "Small identation changes to make shinx tests pass",
      "Merge remote-tracking branch 'origin/release_v0.12.2' into issue_1758",
      "Merge branch 'release_v0.12.2' into issue_1758",
      "Merge remote-tracking branch 'origin/release_v0.12.2' into issue_1758"
    ]
  },
  {
    "pr_number": 3849,
    "commits_list": [
      "5c2ea08e20feebcf62f09ac9d9f9a4784b084def"
    ],
    "message_list": [
      "Do not restrict numpy to '<1.18'.\n\nPrevious issues with the broken release should be resolved now."
    ]
  },
  {
    "pr_number": 4127,
    "commits_list": [
      "a25648b4928528db7069fb0a1cbed951ceb730fd",
      "2643a3a38956057da86457430e7b8cf675431c9c",
      "353b9693e6b03c1efbfe060b0378cce99da5e869"
    ],
    "message_list": [
      "Add brief intro text to \"About\" page.",
      "Add link to about page from landing page.",
      "Update docs/source/intro/about.rst\n\nCo-authored-by: Chris Sewell <chrisj_sewell@hotmail.com>"
    ]
  },
  {
    "pr_number": 2214,
    "commits_list": [
      "24e446f9f548dbd7dac624c7ac4391398b2cbce6",
      "2ff392aa4c66c384f4dc69809d16fcdd45d00b42"
    ],
    "message_list": [
      "Added caching of entity collections and default user\n\nNow if a new collection is created via .objects it is cached for the\nnext time in a lazy store.  Similarly the default user for a User\ncollection is cached after being created for the first time.  This may\nlead to issues if the default user can change for a given profile during\nruntime, however for now we don't support this.",
      "Moved AiiDAManager to be returned by get_manager()\n\nThis way we have greater control over its creation and we don't have to\nimplement a reset method which starts to get complicate.  We just throw\nthe object away and instsantiate a new one when we need to reset\n\n * Removed construct_backend() (replaced by get_manager().get_backend())\n * Changed all tests to not specify the backend, they should just do\nwhat user facing code does which is use the default one and if we ever\nwant to test with different backends we just implement a function to\nload a new backend from a profile globally\n* Moved getting default user email to the `Profile` class"
    ]
  },
  {
    "pr_number": 3839,
    "commits_list": [
      "87e22f41797781641c6712f1c408d39dd0f40d9f",
      "8202ab152cc1668907f38636e53e792d753f8784",
      "19365fa78077375ae55789dfe8bbaf018a13ea7d",
      "e234fae15265a912076ffacaf51a05315a68f604",
      "4a49c054766568a1b8a6f186067c1ab0f05ecfea"
    ],
    "message_list": [
      "Fix workflow syntax for test-install and update-requirements workflows.\n\nUsed incorrect 'on.push.branch' instead of the correct\n'on.push.branches' triggers.",
      "Update paths triggers for test-install workflow.",
      "Only create update-requirements PR on the primary repository.\n\nAnd not on forks.",
      "Update requirements directly on same branch.",
      "Update requirements files."
    ]
  },
  {
    "pr_number": 3839,
    "commits_list": [
      "87e22f41797781641c6712f1c408d39dd0f40d9f",
      "8202ab152cc1668907f38636e53e792d753f8784",
      "19365fa78077375ae55789dfe8bbaf018a13ea7d",
      "e234fae15265a912076ffacaf51a05315a68f604",
      "4a49c054766568a1b8a6f186067c1ab0f05ecfea"
    ],
    "message_list": [
      "Fix workflow syntax for test-install and update-requirements workflows.\n\nUsed incorrect 'on.push.branch' instead of the correct\n'on.push.branches' triggers.",
      "Update paths triggers for test-install workflow.",
      "Only create update-requirements PR on the primary repository.\n\nAnd not on forks.",
      "Update requirements directly on same branch.",
      "Update requirements files."
    ]
  },
  {
    "pr_number": 5140,
    "commits_list": [
      "ba2ae47882e04c71117441ab8e16ec9042263aa0",
      "57e169e2d7cc90a1b09edeb191b1e46b20d6870a",
      "d2128859f35e2032e00e044f55321b71d3e13442"
    ],
    "message_list": [
      "CMD: not mandatory to set plugin for code setup",
      "test_code pass",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci"
    ]
  },
  {
    "pr_number": 2214,
    "commits_list": [
      "24e446f9f548dbd7dac624c7ac4391398b2cbce6",
      "2ff392aa4c66c384f4dc69809d16fcdd45d00b42"
    ],
    "message_list": [
      "Added caching of entity collections and default user\n\nNow if a new collection is created via .objects it is cached for the\nnext time in a lazy store.  Similarly the default user for a User\ncollection is cached after being created for the first time.  This may\nlead to issues if the default user can change for a given profile during\nruntime, however for now we don't support this.",
      "Moved AiiDAManager to be returned by get_manager()\n\nThis way we have greater control over its creation and we don't have to\nimplement a reset method which starts to get complicate.  We just throw\nthe object away and instsantiate a new one when we need to reset\n\n * Removed construct_backend() (replaced by get_manager().get_backend())\n * Changed all tests to not specify the backend, they should just do\nwhat user facing code does which is use the default one and if we ever\nwant to test with different backends we just implement a function to\nload a new backend from a profile globally\n* Moved getting default user email to the `Profile` class"
    ]
  },
  {
    "pr_number": 3817,
    "commits_list": [
      "490a8f476874dfd6669815fa4e38593cc9ad950f",
      "49ee9b59a1503196edbeb4c150561b2625fe1f5d",
      "cb31c611dbd8f3e3d0a9f8b87169214cd780fcfe",
      "e90e15024c558d81d3a4953065882ec5f9c8497d"
    ],
    "message_list": [
      "isolated bandsdata list as 'NOT FOUND' and some test\n\nboth backend supported",
      "Merge branch 'develop' into iso_bands_list",
      "PR #3817: add docstring to _extract_formula",
      "Merge branch 'develop' into iso_bands_list"
    ]
  },
  {
    "pr_number": 4894,
    "commits_list": [
      "59eedf393f73c745eda3d35d0895e5a29fcb5e9d"
    ],
    "message_list": [
      "Reinstate `aiida.manage.tests.unittest_classes`\n\nAlthough this was officially deprecated and slated to be removed for\n`v2.0` the deprecation warning only showed when using the `TestRunner`\nclass. However, many plugins only used the `PluginTestCase` run directly\nthrough `pytest` and so didn't see the deprecation warning. Therefore,\nwe reinstate the module for now and will remove it in a couple of\nmonths."
    ]
  },
  {
    "pr_number": 1764,
    "commits_list": [
      "15a4fa292886ceb9d06fcaf36b9ee821cbdb789b",
      "16b7bc0b0b9b2209cc91f0b8e4d27ca225238d2f",
      "0fbc0c1bb3a6db3646ade00cc2fbdc80d0ce4002",
      "f4a89637267d55738cde20556593db9907cf0ca8",
      "708e97b2d822337cb3dc1110ed5238b9d216f2c8",
      "e1d8ba799e192597dc2ecf83b945eefe30aac764",
      "4390d8940285cf2eb6a88af1388c0c4559a1e540",
      "ae8d43608419185b2435e9469f9419f9c9255a3e",
      "36e97004873f2d5c421f929ecae9877307d1ddbf",
      "1dcd7501a196b66ae36a69060a02a5b26df355e6",
      "130e99ed8b0fbf52ae0571ffb43ccea1f026df66",
      "1cbadfeb2f5ab8f73359482e95109e6eb3dca624",
      "b2bbf670875d1c6f4c8a0d35b9f67679a7838ef8",
      "3234389c2c43bef0ddd6675fb4974fbaf1637ef1",
      "3b4de69fe7958c69ab4481d5eb0d11237b5bc9ae",
      "5f366cb90a13ee3e85a2a084c87d8f248ea3c66f",
      "a0a959036d0aa36dd8e122460a8ed68fded0931e",
      "06490d20e5d6d5dd7ba6a6e77828b6e26d01741d",
      "bf1f961338ba3749f016c4e081006e01bdb4e978",
      "19d00422fa9eb483a949b5a981628138e9aadd81",
      "4e82a784aac4da84c3f795fa46162515d62410ac",
      "831fa18ed36875c5d69ff96e6ffabe1bd5f1a4f7",
      "dec0183a5536a86b41756758a44c850c8b5f4b76",
      "d66db315e7392d1d7dc36a5c8abcfc2f86a0fb8f",
      "cb76cf0f1db9180573bd5b144f2bb8b5f48490ab",
      "7668131028ed7e98d348e154ba373203a71d89f8",
      "bfbc086c2dc7962297c93ace4bf10c1e7734b9c0",
      "d8ba932999bf2ca77483fd8ba11d337c4dd4ab44",
      "042029e75557dd69b3844ea21f07dbcb9af6cdc8",
      "de903772871d541356edf6136b04e0031a112d7d"
    ],
    "message_list": [
      "Reorganising/extending the import/export tests",
      "More progress on export graph traversal",
      "Changing the export to accept AiiDA objects instead of Db objects",
      "Adding the new graph traversal logic to the export function",
      "Small corrections",
      "Various corrections to the graph traversal algorithm. We have to think what we do about the exported links since not all the predecessors are now exported (e.g. Calc-Calc in the reverse way is not exported)",
      "More progress on link export",
      "Link export seems to work?",
      "All tests pass",
      "Activating some more tests",
      "Cleaning up",
      "Small changes to the make the TCOD tests to use the new import/export parameter format",
      "Removing not needed code and modifying export_tree signature",
      "Cleaning the code, making docs to run without warnings",
      "Adding optinal rules to graph exporation. It remains to be done for the links",
      "Adding optional rules for link extraction",
      "A Code is treated as a Data node (and can be exported if it is referenced by a Calc or if it is exported independently). Tests added",
      "Adding correct export of code related links. Tests added",
      "Merge remote-tracking branch 'origin/release_v0.12.2' into issue_1102_b",
      "Small changes to make tests work again after the merge of 0.12.2",
      "Proper handling of empty import files. There is no need for strict exceptions that make the import of a bunch of import files to stop",
      "Minor fix for the previous commit.",
      "Cleanning the code, adding missing cli arguments",
      "Merge branch 'release_v0.12.2' into issue_1758",
      "Fixing unindent issue to make doc tests to pass",
      "Merge branch 'issue_1758' of https://github.com/szoupanos/aiida_core into issue_1758",
      "Small identation changes to make shinx tests pass",
      "Merge remote-tracking branch 'origin/release_v0.12.2' into issue_1758",
      "Merge branch 'release_v0.12.2' into issue_1758",
      "Merge remote-tracking branch 'origin/release_v0.12.2' into issue_1758"
    ]
  },
  {
    "pr_number": 2518,
    "commits_list": [
      "c386e06834ac09879f3582cbb31d660bba7b8cc0",
      "ccfbcb2d7f828ce8a4f287a08e4cd8e41f777d28"
    ],
    "message_list": [
      "Adding to aiida.orm.implementation.sqlalchemy.groups.SqlaGroup#add_nodes the option to bypass the SQLA\nORM and speed up the addition of nodes to a group.\nThis option is only available at the Backend and therefore only to users who need this speedup (the ORM level\nusers should not be aware of such options).\nImport SQLA was modified to use this optimisation.",
      "Merge branch 'provenance_redesign' into issue_1319_for_provenance_redesign"
    ]
  },
  {
    "pr_number": 2741,
    "commits_list": [
      "d6947d7443ba6af6d86d2a330ea6711fa6f63cf3",
      "a24308827544273d22ff730d9e1a91fbe2ce11ee",
      "d7ccd0726aec92fd0677d5509b910c1957928733",
      "18393daa072347f5d39c97787e3a9ec934afa5e9",
      "7f1bf5d4ad2e46cf40a7465e52b7ace4414225f6",
      "078c6125e00702b8b5f73a835c9e2901bdc14541",
      "9554485fcde3e9b5ca062c6ad99358ee912ed153",
      "7c7d1b7ea2d0a518d2142d9d796eb8c2d5019d77",
      "659ca7244b5ed0f697a903769309798d5a82da0c",
      "78459f045ccf61bb4a221d01a9a465594231599b"
    ],
    "message_list": [
      "upgrade reentry dependency",
      "remove reentry_scan (no longer works)",
      "remove pip pinning in pyproject.toml",
      "remove pip guards from setup.py",
      "remove reentry checks from travis",
      "remove commented pip guard",
      "fix environment.yml",
      "switch to reeentry 1.3.0",
      "fix conda dep",
      "fix imports in setup.py"
    ]
  },
  {
    "pr_number": 5659,
    "commits_list": [
      "1cb9e3120721a000ce0e7ce49366a2588cda9b9a",
      "d2a2b7218a6dd73de8ea9548026c78c878eed438",
      "2d92d35e42c295910fb083396e4b330ba4a1e4ed",
      "36d4acd9d191a37e495df4a6120af0d373da280b",
      "eebb81136ec6e27a02623170c637ee4252fb566a",
      "99e201477c1345792ece44eca6ec82089776b122",
      "842f18e2993fbc4595d21e42c51dcfd5aac619f3",
      "59b8f5a0c85e2eb883e568cf7c130a0b962a3994",
      "465d499a094353409d8182250a505d53b0402441"
    ],
    "message_list": [
      "`CalcJob`: Add functionality that allows live monitoring\n\nInstead of killing the process, it is preferable to have the engine\nretrieve the files from the job and call the parser, if one was defined.\nHowever, this would result in the process to be marked as terminated\nnominally with the exit status returned by the parser. It would be\nimpossible to see that the job was stopped by a monitor other than from\nthe process report. It is important that one can query for processes\nthat were stopped by a monitor. To do so, a dedicated exit code is\ndefined on the `CalcJob` called `STOPPED_BY_MONITOR` which is set on the\nnode, overriding any exit status returned by the parser. There is no way\naround this since only one exit status can be set on a node.\nNevertheless the result from the parser is logged and so is visible in\nthe process report.\n\nIf monitors are defined for the `CalcJob` the corresponding node will\ncontains information of the package versions from which the monitors\ncome. The information is added to the `version` attribute which already\ncontains version information of `aiida-core` and the `CalcJob` plugin\nitself.",
      "`CalcJobMonitor`: Add the `priority` attribute\n\nThe `priority` attribute takes an integer and is zero by default. It\nallows the user to define the order in which monitors need to be called\nin case multiple are defined.\n\nThe ordering is implemented in the `CalcJobMonitors` utility class. This\nis mostly so that it is easy to unit test. It ensures the monitors are\nordered by their priority, going from high to low. In case of identical\npriorities, the monitors are sorted alphabetically by the keys in the\nimonitors` input namespace.",
      "`CalcJobMonitor`: Add the `minimum_poll_interval` attribute\n\nThe `minimum_poll_interval` is an optional positive integer that can be\ndefined for a monitor. If defined, the engine will ensure that the\ninterval between two successive calls of the monitor will at least be\nthis long.\n\nIn order to track the last time a monitor was called, the timestamp is\nadded to the `call_timestamp` attribute when it is called by the\n`CalcJobMonitors.process` method.",
      "Add the `CalcJobMonitorResult` dataclass\n\nThis dataclass can be returned by a monitor to communicate to the engine\nthe course of action to take. Returning a str from a monitor remains\nsupported as it is automatically converted to a `CalcJobMonitorResult`.\n\nThe first attribute that is added is `action` which takes an instance of\nthe `CalcJobMonitorAction` enum. Currently the only value is `KILL`\nwhich is therefore also the default and instructs the engine to kill the\njob and stop monitoring.",
      "`CalcJobMonitorResult`: Add the `parse` attribute\n\nBy default it is `True`, but if set to `False`, the engine will skip the\nparsing step. In this case, the `STOPPED_BY_MONITOR` exit code will be\nset on the node.",
      "`CalcJobMonitorResult`: Add the `retrieve` attribute\n\nBy default it is `True`, but if set to `False`, the engine will skip the\nretrieval step and terminate the process straight away. The exit code\nthat will be set is `CalcJob.exit_codes.STOPPED_BY_MONITOR` and there\nwill not be a `retrieved` output node.",
      "`CalcJobMonitorResult`: Add the `override_exit_code` attribute\n\nBy default it is `True`, but if set to `False`, the engine will not\noverride the exit code returned by the parser with the default exit code\n`STOPPED_BY_MONITOR` that is set when a job is stopped through a monitor.\nNaturally, this attribute is ignored when the `parse` and or `retrieve`\nattribute are set to `False` as in that case the parser is never even\ncalled so there is nothing to not override.",
      "`CalcJobMonitorAction`: Add the `DISABLE_ALL` option\n\nSo far the `CalcJobMonitorAction`, the enum that can be returned by a\ncalcjob monitor, only supported a single option `KILL`. When the engine\nreceives this instruction, the job will be killed immediately.\n\nAn alternative use case is where a monitor has performed a check and an\noptional action and now simply wants to let the job run its course.\nOften in this case it is important that the monitor itself, and any\nothers that may have been registered, are no longer run for the lifetime\nof the job.\n\nThe `CalcJobMonitorAction` now adds the `DISABLE_ALL` option, which when\nset on the `action` attribute of a `CalcJobMonitorResult`, the engine\nwill disable all monitors for the remainder of the duration of the job.",
      "`CalcJobMonitorAction`: Add the `DISABLE_SELF` option\n\nThe default action for a `CalcJobMonitorResult` is the option\n`CalcJobMonitorAction.KILL` which immediately kills the job. However,\nsometimes, one wants to simply disable the monitor and continue running\nthe job nominally. The `CalcJobMonitorAction.DISABLE_SELF` option\ninstructs the engine to not call the monitor that returned it again in\nfuture monitor evaluations."
    ]
  },
  {
    "pr_number": 4761,
    "commits_list": [
      "47ce1fabe86b234246c741d6e4ff562d2a231771",
      "2e485290d34742cf4c2365b0384ef4faf88cf10f",
      "fd7e641c113a1e6df9065fd55f4b6b092b3be7ac"
    ],
    "message_list": [
      "`verdi calcjob gotocomputer`: Add proxy command\n\nCurrently the `verdi calcjob gocomputer` command does not work in case\nthe connection to the remote computer is via a proxy. Here we add the\n`ProxyCommand` option to the parameters for the `ssh` command.",
      "Fix test to appease the CodeCov gods",
      "Fix test and issue with `open` method popping connect arg keys"
    ]
  },
  {
    "pr_number": 4379,
    "commits_list": [
      "0f881ca0e9743f93e34e5402a67ef95f4d4412e4",
      "474f04d25909c4ce8e59d72fa6594d1933916f88",
      "212234b232f7a1cbf47d08fdbc18de20394046b0",
      "bdca007f3b066849f7afd1c2938cdc75cbeab2d2"
    ],
    "message_list": [
      "Skip `retrieved_temporary_folder` removal if None",
      "Merge branch 'develop' into tests/fix-osx",
      "Merge branch 'develop' into tests/fix-osx",
      "Merge branch 'develop' into tests/fix-osx"
    ]
  },
  {
    "pr_number": 3164,
    "commits_list": [
      "afafcc449c5d55efcb671885127dac6cbce8f1dc",
      "78cf91e3beb542d394a7bd1e72632878a06fb919"
    ],
    "message_list": [
      "Refactor link validation to improve efficiency and scaling\n\nThe slowness of the link validation stems from the fact that to validate\nthe indegree and outdegree of a proposed link, all incoming and outgoing\nlinks are retrieved including the neighboring node instance. However, to\ndetermine whether the new links violates the uniqueness constraint only\nthe UUID of the node is necessary and only a sub set of existing nodes\nneeds to checked, depending on the link type.\n\nThe number of nodes that are checked is reduced by using more specific\nqueries for each case instead of getting all neighboring nodes and doing\nthe check on the python level. This fixes the super-linear scaling of\nthe previous solution.\n\nThe `get_stored_link_triples` now takes the argument `only_uuid` which\ncan be used to project only the node UUID which prevents having to load\nthe entire instance.",
      "Simplify the link triple uniqueness determination  in `validate_link`\n\nIn the case of a `unique_triple` degree, the link triple should be\nunique. This means that to check this, we just have to find a single one\nthat already exists to know that the condition is validated. It is not\nexpected to reduce the computation time by much, but the logic is a lot\nclearer."
    ]
  },
  {
    "pr_number": 4700,
    "commits_list": [
      "ceca0f02e334a35cf4a0a748c0f57b46723d9f0b",
      "d1aadde245a32a9e4088d621a159ac47e1b266a4"
    ],
    "message_list": [
      "\ud83d\udc1b FIX: typing failure",
      "Update graph_traversers.py"
    ]
  },
  {
    "pr_number": 1640,
    "commits_list": [
      "015fa4a377240bde3e666424edb0442a0356d190",
      "13810f94f95549cc577300b5d38dd8102d78cbae"
    ],
    "message_list": [
      "Add built-in support and API for exit codes in WorkChains\n\nCurrently it is possible to return non-zero integers from a WorkChain\noutline step to abort its execution and assign the exit status to the\nnode, however, there is no official API yet to make this easier.\n\nHere we define the concept of an ExitCode, a named tuple that takes\nan integer exit status and a descriptive message. Through the\nProcessSpec, a WorkChain developer can add exit codes that correspond\nto errors that may crop up during the execution of the workchain. For\nexample the following spec definition:\n\n\t@classmethod\n\tdef define(cls, spec):\n   \t\tsuper(CifCleanWorkChain, cls).define(spec)\n    \tspec.exit_code(418, 'ERROR_I_AM_A_TEAPOT',\n        \tmessage='workchain found itself in an identity crisis')\n\nIn one of the outline steps, the exit code can be used by retrieving\nit through either the integer exit status or the string label:\n\n\treturn self.exit_code('I_AM_A_TEAPOT')\n\nor\n\n\treturn self.exit_code(418)\n\nand return it. Returning an instance of ExitCode will trigger the exact\nsame mechanism as returning an integer from an outline step. The engine\nwill detect the ExitCode and return its exit status as the result,\ntriggering the abort and the exit status to be set on the Node.\n\nNote that this addition required name changes in parts of the existing\nAPI. For example the Calculation attribute `finish_status` was renamed\nto `exit_status`. This is because it can now be accompanied by an string\n`exit_message`. The `exit_status` and `exit_message` together form the\n`ExitCode`.",
      "Make the exit codes collection an AttributeDict namespace\n\nThis allows one to both access any particular exit code from the\ncollection through attribute dereferencing:\n\n\tself.exit_codes.ERROR_I_AM_A_TEAPOT\n\nas well as through calling it with the string label or integer\nstatus, given that the ExitCodeNamespace implements the call method\n\n\tself.exit_codes(418)\n\tself.exit_codes('ERROR_I_AM_A_TEAPOT')"
    ]
  },
  {
    "pr_number": 4127,
    "commits_list": [
      "a25648b4928528db7069fb0a1cbed951ceb730fd",
      "2643a3a38956057da86457430e7b8cf675431c9c",
      "353b9693e6b03c1efbfe060b0378cce99da5e869"
    ],
    "message_list": [
      "Add brief intro text to \"About\" page.",
      "Add link to about page from landing page.",
      "Update docs/source/intro/about.rst\n\nCo-authored-by: Chris Sewell <chrisj_sewell@hotmail.com>"
    ]
  },
  {
    "pr_number": 642,
    "commits_list": [
      "8ccf6351c53dfdba70ae0a61a6a7c9f175768149",
      "c890586e7b8b9b7e068763f942013e351b3f7d03",
      "fc3fedb64a30fe207cc899cb523c6064baf11055",
      "a0676046bab6a22cc99d31a0f07830ed91ce5dbe",
      "c7990a8f99a4911dac7884ed19deb54657a2924e",
      "93183b3f591dad0e07d303c16d2ee024ad28406c",
      "091c93f367f2fc129759cc53fb74d701933657b1",
      "3987c732818b382ec95f8d35857be68572d97d02",
      "346ba7feefad07cc63c54baa3b07fc031b01fea7",
      "75c68b0f56a5fe36496084d25c2da20a82905eba"
    ],
    "message_list": [
      "Add test which fails if the same node is given as input for two different inputs",
      "Ignore if a exact copy of a duplicate link is made",
      "Drop check for uniqueness of links\n\nAllows multiple links between two nodes, either of the same or of\na different link type.",
      "Remove failing UniquenessError check",
      "Fix comment about replace_link_from test",
      "Remove uniqueness check from _replace_link_from",
      "Merge branch 'develop' into double_link",
      "Merge branch 'develop' into double_link",
      "Merge branch 'release_v0.10.0' into double_link",
      "Merge branch 'release_v0.10.0' into double_link"
    ]
  },
  {
    "pr_number": 929,
    "commits_list": [
      "fe48bd56488fa673e9791a6ec794b3fac10ffd18"
    ],
    "message_list": [
      "Add 'quote_strings' option to conv_to_fortran"
    ]
  },
  {
    "pr_number": 4638,
    "commits_list": [
      "5147ed00ed9078e25b54b5e82d4958f7c4a46809",
      "e58771754bf62c9bedfdfcdcbfe7c382ce147b99"
    ],
    "message_list": [
      "CI: remote the `numpy` install workaround for `pymatgen`\n\nThe problem occurred due to an outdated version of `setuptools` which\nwould be invoked when `pymatgen` gets installed from a tarball, in which\ncase the wheel has to be built. In this scenario, the build requirements\nget installed by `setuptools`, which at outdated versions did not\nrespect the Python requirements of the dependencies which would cause\nincompatible version of `numpy` to be installed, calling the build to\nfail. By updating `setuptools` the workaround of manually installing a\ncompatible `numpy` version beforehand is no longer necessary.",
      "CI: skip `restapi.test_threaded_restapi:test_run_without_close_session`\n\nThis test has been consistently failing on Python 3.8 and 3.9 despite\nthe two reruns using flaky. For now we skip it entirely instead."
    ]
  },
  {
    "pr_number": 5667,
    "commits_list": [
      "d620d3c744acfdf753af60f7e3735aa8d5cce6aa",
      "0620b73fca3e7d4c1c93a78eba5281b170d2bb58",
      "c3298dd7e82b85db6628cfa32efbc3f52ecdb5e7"
    ],
    "message_list": [
      "Add the `ContainerizedCode` data plugin\n\nThis implementation of the `AbstractCode` interface allows running a\n`CalcJob` within a container on a target computer. The data plugin\nstores the name of the container image, the executable within that\ncontainer, the command with which the container should be launched and\nthe `Computer` on which the container can be run.",
      "Address PR review comments",
      "after review: rephrase docker exclu and and sarus engine command"
    ]
  },
  {
    "pr_number": 3892,
    "commits_list": [
      "ffbe6a268c9a67275c582921fa78228798586aab",
      "dc98acd66d6d1c7984ba0cfcd14756bad57f313e",
      "a7c4ad5fde9e633486b0eb2269d164a0f34943c0"
    ],
    "message_list": [
      "move PGSU out to separate package\n\nOne of the main issues with `verdi quicksetup` is that it is non-trivial\nto figure out how to connect as the PostgreSQL superuser in a wide\nvariety of operating systems and PostgreSQL setups.\nThis PR factors out the code which takes care of this connection into a\nseparate package, which can then be tested on a wide variety of setups\nusing CI.",
      "update requirements files manually",
      "fix docs"
    ]
  },
  {
    "pr_number": 2656,
    "commits_list": [
      "9ce874b809ccf498d2e27034b791c52977f30b81",
      "6be0390ec58348c1de19e2e8baf6bc98cc488974",
      "0e2f0cddc56e339f2979bf7e533beb3725c495a4",
      "70a3150d430eeb4defe0c327dd75563c60129098"
    ],
    "message_list": [
      "Activate publish with travis\n\nNow we an publish to pypi by tagging a commit in the master branch,\nthe code is published if:\n\n- The whole build matrix passes\n- The build is a tagged commit\n- The tag is eaither v{major}.{minor}.{patch} or\n  v{version}(a|b|rc){number}.\n- The repository matches ours (doesn't run in other contributors\n  travis).",
      "Remove duplicate condition",
      "Only control the tag in the stage condition",
      "Merge branch 'develop' into automated-publish"
    ]
  },
  {
    "pr_number": 5225,
    "commits_list": [
      "34cab288039589d770b75c66477112d6f8e74e58"
    ],
    "message_list": [
      "Add the `EnumData` data plugin to easily store `Enum` members\n\nAn `Enum` member is represented by three attributes in the database:\n\n * `name`: the member's name\n * `value`: the member's value\n * `identifier`: the string representation of the enum's identifier\n\nThe original enum member can be obtained from the node through the\n`get_member` method. This will except if the enum class can no longer be\nimported or the stored value is no longer a valid enum value. This can\nbe the case if the implementation of the class changed since the\noriginal node was stored.\n\nThe plugin is named `EnumData`, and not `Enum`, because the latter would\noverlap with the Python built-in class `Enum` from the `enum` module.\nThe downside is that this differs from the naming of other Python base\ntype analogs, such as `Str` and `Float`, which simply use a capitalized\nversion of the type for the data plugin class."
    ]
  },
  {
    "pr_number": 5279,
    "commits_list": [
      "f76d3be4fbc0d8013e86121bfb99af9f14ab1576",
      "f96df077ab7a71d8f1354e34cc9e4e923138d5c9"
    ],
    "message_list": [
      "Docs: replace CircleCI build with ReadTheDocs\n\nThe documentation was being built on CircleCI instead of GHA like the\nrest of the CI pipeline, because the former would allow the built\nHTML documentation to be inspected as an artifact, whereas for the\nlatter this wasn't possible.\n\nIt is now possible to do the same with ReadTheDocs, and since we already\nuse that to host the documentation, that means we can now drop CircleCI.",
      "Merge branch 'develop' into fix/4104/replace-circle-ci"
    ]
  },
  {
    "pr_number": 4294,
    "commits_list": [
      "4096206bc3f6d0a6245fafdd3aa8b158a258d2b2",
      "ce0b058b6afe0bcc0acc59739ad9cdad8e10427f",
      "e637794e2ffa35e80b98a6d08a44b872eac20a19"
    ],
    "message_list": [
      "fixed few typos/issues in docs",
      "Merge branch 'develop' into fix_docs",
      "Merge branch 'develop' into fix_docs"
    ]
  },
  {
    "pr_number": 3676,
    "commits_list": [
      "c3ee0e8a4fe3b32714fee63eb885aba436044d89",
      "490759aeaefd94b14fe387a0dc57e4c1149e5577",
      "45697b63781f619b8b7e7c5ff9d0f3561d8e021b",
      "94ac8815cedf26ef0e13116368ba991923e480fa"
    ],
    "message_list": [
      "Reset backend functionalit for django\n\nExtend the current reset backend functionality to also be valid for the\ndjango backend.\nCentralize common static methods.",
      "Import `sqlalchemy` directly",
      "Reverse centralization of reset backend",
      "Merge branch 'develop' into fix_3645_reset_db_and_centralize"
    ]
  },
  {
    "pr_number": 4578,
    "commits_list": [
      "b5cea43ce1554f9ab0bf8f012f8ef15de90cdc7a",
      "ca22ad530f984875bd0f5f37101b9da96969e188",
      "6353de7476c89bf114aa54e62d8a74b929318ea2",
      "4bdf261f3985b910e8df1780253fa682c1717895",
      "6ce0398d883809ab86654e2a5c54a888103f27f0",
      "0f6dc67c9ce954f27f7f796b810dfc0b54a44421",
      "d6d9154fff12454745b2d159a6320a2f349e6815",
      "4deb19375ac710bbee3ce6d3ca5171396dbe158a",
      "17f60afb3c633a27c66f70a5a5421fbf9eb1b86d",
      "81b4cb54b2b8a7cfd3f5c9a46fe160900c7ce259",
      "e95ae72ce7ef0b0f2c588cbf6f145f54c33c9a52",
      "3601ba423a39920497e8720b78c94ce26a1eb798",
      "3d0f94ec5ae01de6f1ab183648588ae2dadb36bb",
      "4b50e318f0a125ae6ed16a5803a4332803545e0d",
      "9b9e07dac4196cd9c389cc35576bc701e45fa7e7",
      "65a99de86f1704bd971530aab57f081df859c2e7",
      "62385f5a7c5570b3c13e9cb3c48404850c87421e",
      "c8c153ab7c609a9185a10e3c59ac824d576e6bcb",
      "8c9975933ad68042c97e6e53279b81e678443cb1",
      "d6eec5e7d82454b202713c0b22744cd37ec2e4b1"
    ],
    "message_list": [
      "add code",
      "add tests for API",
      "improve log messages",
      "don't make force warning only log",
      "donlt delete group on dry-run",
      "move clear to bottom of help",
      "improve help string",
      "add cli tests for `verdi group delete`",
      "add to test",
      "fix pre-commit",
      "update `verdi code delete`",
      "Merge branch 'develop' into improve-delete",
      "\ud83d\udc4c IMPROVE: Expose functions from `aiida.manage.database`\n\nand add to documentation",
      "pin minor sphinx version",
      "fix tests",
      "move functions to `aiida.tools.graph.deletions`",
      "add deprecation warning",
      "pin pylint-django < 2.4.0",
      "Apply suggestions from code review\n\nCo-authored-by: Marnik Bercx <mbercx@gmail.com>",
      "remove duplicate"
    ]
  },
  {
    "pr_number": 4307,
    "commits_list": [
      "75ad46880044be538e2797babf00cf845bf4f4f6"
    ],
    "message_list": [
      "`Runner`: close loop when runner stops if runner created it\n\nIf the loop is not closed, the file handles it managed during its\nlifetime might not be properly cleaned up, leading to file leaks.\nTherefore, if the loop is created by the `Runner` upon construction, it\nshould also close it when the runner closes. It cannot be done for loops\npassed into the constructor, because they migth actually still be in\nuse by other parts of the code."
    ]
  },
  {
    "pr_number": 2615,
    "commits_list": [
      "72e460ae9199027f38d859cc34d9de41211f1e56"
    ],
    "message_list": [
      "Fixes issue #2611.\n\nImplemented logic for `verdi calcjob [in/out]putcat NODE path`:\n\n1. If no path specified: get path from `NODE.get_option('[in/out]put_filename')`\n2. If path still None (no '[in/out]pyt_filename' specified in node).\n   Get path from `NODE.process_class.spec_options.get('[in/out]put_filename').default` if exists.\n   This maps to the process class of the node, i.e. to the current version of the plug-in.\n3. If path still None: Throw critical message that `NODE.__class__.__name__` and its process class\n   `NODE.process_class.__name__` do not specify an [in/out]put file.\n   Else: show content\n\nAdd `test_calcjob_inoutputcat_old` to make sure `verdi [in/out]putcat` still works,\nwhen the node never stored options 'input_filename' or 'output_filename',\nbut the newest process class / plug-in has a default value set for this."
    ]
  },
  {
    "pr_number": 4963,
    "commits_list": [
      "9db0ca09c3b2522641168ab94eacae7ca803195e",
      "8032a8afd9d1a910b37113d1ea7d87f86b426f9d"
    ],
    "message_list": [
      "Process functions: Support class member functions as process functions\n\nThe `FunctionProcess` class that is generated dynamically from the\nprocess function signature used the function's `__name__` attribute to\nconstruct the new dynamic type. This prevented process functions from\nbeing defined as class member methods. By using `__qualname__` instead,\nthis is now enabled and allows for example the following:\n\n    class CalcFunctionWorkChain(WorkChain):\n\n        @classmethod\n        def define(cls, spec):\n            super().define(spec)\n            spec.input('x')\n            spec.input('y')\n            spec.output('sum')\n            spec.outline(\n                cls.run_compute_sum,\n            )\n\n        @staticmethod\n        @calcfunction\n        def compute_sum(x, y):\n            return x + y\n\n        def run_compute_sum(self):\n            self.out('sum', self.compute_sum(self.inputs.x, self.inputs.y))\n\nThe changes also allow these class member process functions to be valid\ncache sources.",
      "Tests: Add tests for calcfunction as `WorkChain` class member methods\n\nThe test defines a workchain that defines a calcfunction as a class\nmember in two ways:\n\n * A proper staticmethod\n * A class attribute\n\nBoth versions work, but the former is more correct and is the only one\nthat can be called from an instance of the workchain. The other has to\nbe invoked by retrieving the calcfunction as an attribute and then\ncalling it. The former is the only one that is offcially documented.\n\nThe tests verify that both methods of declaring the calcfunction can be\ncalled within the `WorkChain` and that it also works when submitting the\nworkchain to the daemon. A test is also added that verifies that caching\nof the calcfunctions functions properly.\n\nThe changes also broke one existing test. The test took a calcfunction,\noriginally defined on the module level, and modifying it within the test\nfunction scope. The goal of the test was twofold:\n\n * Check that changing the source code would be recognized by the\n   caching mechanism and so an invocation of the changed function would\n   not be cached from an invocation of the original\n * Check that it is possible to cache from a function defined inside the\n   scope of a function.\n\nThe changes to the dynamically built `FunctionProcess`, notably changing\nthe type from `func.__name__` to `func.__qualname__` stopped the second\npoint from working. In the original code, the type name would be simply\n`tests.engine.test_calcfunctions.add_function`, both for the module\nlevel function as well as the inlined function. However, with the change\nthis becomes:\n\n `tests.engine.test_calcfunctions.TestCalcFunction.test_calcfunction_caching_change_code.<locals>.add_calcfunction`\n\nthis can no longer be loaded by the `ProcessNode.process_class` property\nand so `is_valid_cache` returns `False`, whereas in the original code\nit was a valid cache as the process class could be loaded.\n\nArguably, the new code is more correct, but it is breaking. Before\ninlined functions were valid cache sources, but that is no longer the\ncase. In exchange, class member functions are now valid cache sources\nwhere they weren't before. Arguably, it is preferable to support class\nmember functions over inline functions.\n\nThe broken test is fixed by moving the inlined `add_calcfunction` to a\nseparate module such that it becomes a valid cache source again."
    ]
  },
  {
    "pr_number": 5069,
    "commits_list": [
      "9a30f332a4f229bdb4e0616de7d4a08e5c263cd4"
    ],
    "message_list": [
      "add initial modification"
    ]
  },
  {
    "pr_number": 2844,
    "commits_list": [
      "b110478744674fd7e33ae67e8077d745129e8274",
      "cb6dfa9280a72c4aa9f03d9eb83f9a906eb45590",
      "20a13707fe1db3320dfbbb45a94a1c9b847fe31f",
      "ff19bbd59c2132f27c26f70ae00635fb940cb99b",
      "7dacf4cf7fd9ef38a1a95aec2b1e437f32b47276"
    ],
    "message_list": [
      "refactoring and testing of OrbitalData",
      "added entry point for generic orbital",
      "Update 'module_name' -> '_orbital_type' in class ProjectionData",
      "Merge branch 'develop' into fix_orbitaldata",
      "Merge branch 'develop' into fix_orbitaldata"
    ]
  },
  {
    "pr_number": 4203,
    "commits_list": [
      "a716b1de2fbf2e8806ca102fcb13340077052a7d",
      "84cc7d3218b14ed6759e2151f5ef37d5aef1f3fe",
      "8ee62eae2a7cb66edaf2128818747d8a22ea09ea"
    ],
    "message_list": [
      "Docs: update ReadTheDocs configuration file\n\nThis forces RTD to install the `aiida-core` package such that the entry\npoints are installed and it will now fail the build if warnings are\nemitted.\n\nAlso remove the changes of the previous commit to manually call reentry\nscan and the `ON_RTD` variable, which was a remnant of the old\ndocumentation build.\n\nFinally, delete folder `aiida/sphinxext/tests` which became obsolete\nwhen all tests were moved to the `tests` folder.",
      "Docs: remove the `docs/requirements_for_rtd.txt` file\n\nThis also allows to remove the relevant pre-commit commands to\nautomatically update it.",
      "Docs: remove adding `aiida` to `sys.path` in `docs/source/conf.py`\n\nSince we are now installing the package, even on ReadTheDocs, it is no\nlonger necessary, nor preferable, to hack it into the `sys.path`."
    ]
  },
  {
    "pr_number": 4150,
    "commits_list": [
      "e6054306e3a5acc476ccd9bdfa29b3efad58cf59",
      "8b401f0e3eb5cdbd25b72cff32447fbbc50fba37"
    ],
    "message_list": [
      "Increase the default for `runner.poll.interval` config option to 60\n\nThis option is used to define the `poll_interval` argument of all\n`Runner` instances that are created to run all processes. It determines\nthe interval with which the state of a subprocess, that a `WorkChain` is\nwaiting on, is checked whether it is terminated. If that is the case, a\ncallback is called which will signal to the `WorkChain` that it can\ncontinue.\n\nThis polling is a backup mechanism in case the broadcast by the process\nwhen it terminates is missed by the caller, which would cause it to wait\nindefinitely. The original default of 1 was causing unnecessary load on\nthe CPUs as well as the database that each time had to query for the\nprocess state. When running many calculations this would spin the CPUs\nnoticeably. Since this is supposed to be a fail-safe mechanism and it\nshould only be required rarely, it is fine to increase the time\nsignificantly to reduce the load.",
      "Merge branch 'develop' into fix/4149/runner-poll-interval-default"
    ]
  },
  {
    "pr_number": 4812,
    "commits_list": [
      "300aa771fc4a3a490b333487c3344ae5de64314a",
      "dfbe99e746ce65cae56d659d267ada447427c904",
      "10fdd755602485af80b6e0942605861f986e93e5",
      "b4cdc4c5017dd6e0e122681229ef94d233e280cf",
      "4f28f9d7c28f158b13444890c3bd488ee6d99d37",
      "800ef86d271c256c5682e7583107577ac085fa7c"
    ],
    "message_list": [
      "\ud83d\udcda DOCS: Add documentation on stashing",
      "Merge branch 'develop' into docs/stashing",
      "add link for transfercalcjob feedback",
      "add versionadded to TransferCalcjob docs",
      "Apply suggestions from code review\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "Update docs/source/topics/calculations/usage.rst"
    ]
  },
  {
    "pr_number": 5283,
    "commits_list": [
      "6d9199e6289450bf8e0c9be92a6687fb44404706",
      "5af8dcefabb7f5cc1e0f216268e1c757c05a437b"
    ],
    "message_list": [
      "`Scheduler`: abstract generation of submit script env variables\n\nThe environment variables defined in the `JobTemplate` need to be\nwritten to the submit script header. The `Scheduler.get_submit_script`\nrelied on the `__get_submit_script_header` abstract method to do this.\nThis forces each plugin to write this code, even though this is very\nlikely to be scheduler independent.\n\nTherefore it is best to abstract this functionality to the base class\nsuch that each scheduler plugin automatically has this implemented. If\nreally needed, the plugin can still override the behavior by explicitly\nreimplementing the `_get_submit_script_environment_variables` method.\n\nNote that it looks that `_get_submit_script_environment_variables` could\nbe a `staticmethod`, but unfortunately it is not possible to call the\nsuper when overriding the staticmethod in a subclass.",
      "Alternate implementation\n\nHave `_get_submit_script_environment_variables` include the starting and\nend markers, and do not call it in `Scheduler.get_submit_script`, but\ninstead let the subclasses call it in `_get_submit_script_header`\nthemselves. This prevents existing external plugins printing the\nenvironment variables twice, and still the logic is provided in a single\nplace.\n\nThis still require external plugins to update to use the new method to\nprint the environment variables for it to pick up the changes if\n`aiida-core` decides to improve the implementation."
    ]
  },
  {
    "pr_number": 2841,
    "commits_list": [
      "1c31d5d7a71549da90a21d4df0a0b5dc8b459189"
    ],
    "message_list": [
      "Update dependency requirement `plumpy==0.14.0`\n\nThis version fixes two bugs in process port validation."
    ]
  },
  {
    "pr_number": 2774,
    "commits_list": [
      "df3fe69990a9d8fb422109403a577db5e2debcd2"
    ],
    "message_list": [
      "Add slots warning to verdi process list\n\n- Adds a warning to be printed by verdi process list when the number\nof active processes exceeds 90% of the available worker slots,\ncalculated by _RMQ_TASK_PREFETCH_COUNT * the number of active workers.\n- Adds a test to make sure it fires the warning in the correct conditions.\n- Also adds a get_numprocess() utility method to the AiiDA circus client class\nto simplify getting the number of active workers"
    ]
  },
  {
    "pr_number": 2782,
    "commits_list": [
      "0461cf592c6b052e1aca885dbce297e4cefd345b",
      "1fb4c99e2f9f5640164db5f7fa78087505aeea76",
      "fe6e7a087b2b681e64313b95f413a386d5a61719",
      "6a5aa0b4dabf4550dae4b2ae9f71c0d2ca90cf09",
      "8ead76bfb1a33211eec6bf2c3bb0b85ff14c1319",
      "aa697fe52b62035e939349ef80f7a37e1ecab938",
      "dd2947e7a400d8f45d11c53734fda8a72960dfbb"
    ],
    "message_list": [
      "Change the way hash is calculated for float\n\nHash the float by converting it to string and only use the first 14 significant\ndigits. This is consistent with the way rounding error may occur is lost when\nsaving a float to the database.\nThis fix the issue where the hash of a node containing float\nattributes may change when it is stored.",
      "Merge branch 'develop' into fix_2631",
      "Added tests for float to text conversion",
      "Merge branch 'fix_2631' of github.com:zhubonan/aiida_core into fix_2631",
      "Merge branch 'develop' into fix_2631",
      "Merge branch 'develop' into fix_2631",
      "Merge branch 'develop' into fix_2631"
    ]
  },
  {
    "pr_number": 5439,
    "commits_list": [
      "38cfc5143ebfda908733bae120dbdaab746f5389",
      "079b3bfc4de04fcd2d77d168337818b306e09ce2",
      "84058dd37b6edf5f0aa26068075e7451606eaabf"
    ],
    "message_list": [
      "\ud83d\udd27 MAINTAIN: Add `warn_deprecation` function",
      "\ud83d\udd27MAINTAIN: Add `Node.ctx`\n\nThis is a pre-cursor to moving methods off of the `Node`",
      "\u267b\ufe0f REFACTOR: `NodeRepositoryMixin` -> `NodeRepository`"
    ]
  },
  {
    "pr_number": 2639,
    "commits_list": [
      "5f656324aa4f6d08bc1cc94d14b729c6683e57ea"
    ],
    "message_list": [
      "Make sure exit code != 0 if incompatible/old archive version.\n\nFor `verdi import`:\n- Change the warning to a critical if exception \"IncompatibleArchiveVersionError\" is raised.\n  Both for an archive URL and local archive file.\n- Use new subclass `ImportPath` of `click.Path` for ARCHIVE.\n\n`ImportPath` first do the checks of `click.Path`,\nthen subsequently does URL check using six's urllib.\n`ImportPath` has been implemented at aiida.cmdline.params.types.path\n\nAdded tests in `aiida.backends.tests.cmdline.commands.test_import`:\n- `test_import_old_local_archives`:\n  It checks a non-zero value is returned, when local archives with old versions are imported.\n- `test_import_old_url_archives`:\n  It checks a non-zero value is returned, when remote archives with old versions are imported.\n- `test_import_url_and_local_archives`:\n  Supplying `verdi import` with valid archives from both local and remote sources should still work.\n- `test_import_url_timeout`:\n  Make sure a valid URL is recognized, while any HTTP, URL, or timeout error fails correctly.\n- `test_raise_wrong_url`:\n  Test the correct error is raised for a wrongly typed ARCHIVE.\n\nAdded tests in `aiida.backends.tests.cmdline.params.types.test_path`:\n- `test_default_timeout`:\n  Test default timeout_seconds value is correct\n- `test_valid_timeout`:\n  Test valid timeout_seconds value\n- `test_none_timeout`:\n  Test TypeError is raised when passing None as timeout_seconds\n- `test_wrong_type_timeout`:\n  Test TypeError is raised when passing wrong type as timeout_seconds\n- `test_range_timeout`:\n  Test implemented range of timeout_seconds"
    ]
  },
  {
    "pr_number": 4888,
    "commits_list": [
      "84ac26b8b47716f5d67bbd4f4b5d93a36466a372"
    ],
    "message_list": [
      "ORM: replace `InputValidationError` with `ValueError` and `TypeError`\n\nThe front-end ORM code threw an `InputValidationError` in a number a\nplaces. However, this exception was intended for calculation plugins to\nthrow in the case of invalid input nodes. Where it was being used in the\nORM plain `ValueError` or `TypeError` exceptions should have been used.\n\nSince the `InputValidationError` does not subclass either of the base\nexception types, code that was catching this particular AiiDA-specific\nexception will break after these changes. However, a scan of the most\nused plugins revealed that this exception is actually not being caught\nso this change should have little to no actual consequences.\n\nIn addition, the `CodeValidationError` custom exception of the class\n`aiida.orm.utils.builders.code.CodeBuilder` was changed to subclass the\n`ValueError` instead of the base `Exception`."
    ]
  },
  {
    "pr_number": 5682,
    "commits_list": [
      "1f801e3a72be84a99f2be6fe1652792f25e86e6c"
    ],
    "message_list": [
      "[pre-commit.ci] pre-commit autoupdate\n\nupdates:\n- [github.com/pre-commit/pre-commit-hooks: v4.1.0 \u2192 v4.3.0](https://github.com/pre-commit/pre-commit-hooks/compare/v4.1.0...v4.3.0)"
    ]
  },
  {
    "pr_number": 4366,
    "commits_list": [
      "4a857d8acb2735d11dd85d84a059855a6e8f389f",
      "9595febb2526738f98c503d4ea9508131007bd37",
      "fa54e44df62f60838ce18a604606340ec5ea4e9a",
      "1866b74d9a1a37da40281e632509277a89b7aaac",
      "1f5a0a5412461f57477a67388140b82c78c7c155",
      "69a14552fffb2ef7f30aaba19a7ca130262a8e7e",
      "8c4bb4a284a0930c8885661e38c62ce752a69897"
    ],
    "message_list": [
      "Starting implementation of GaussianCubeData data type.\n\nThe class is meant to deal with Gaussian Cube files. The current\nimplementation is capable of reading and exporting the cube files.\nAdditionally, a minimal checker is added to verify the consistency\nof data.",
      "Update setup.json\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "Set all checks in the constructor to `is not None`.",
      "When reading/writing cube files specify encoding explicitely",
      "Rename everything_ok with _validate.",
      "Remove encoding parameter from the export function\n\nThis is needed for consistency with the overridden method.",
      "Fix pylint error"
    ]
  },
  {
    "pr_number": 716,
    "commits_list": [
      "20d9f38bee355b03d774e6402bdbf503aa7aa089",
      "60f8808f4893934facaae5b2bfae9556aaa8d14f",
      "db81004da961e775ab6c070b7602589a09e92c0e",
      "3cbf5f4b9f3a19d434ca51479ce2ee4283029e2f",
      "0c3f1e33ea0a293742bb7d27c172cd2ec8243190",
      "40b27b0f320f1f534d683c71fe6483242e739622",
      "6856823bde7d8d6dc256d6f19bf6506c7cda2d53",
      "f6b0ba7ffe9e7ead9883814c96781de9c979c5f7",
      "ea4034f754c547957e970fc0eb5b02160f78e57c",
      "792dc03703c3ca25745c2d9b9660098384dabb90",
      "43e7a38b90ecb2b0fd2af439c796f37dbcd763dd",
      "85216288d31b7e537eabf7114ca028e43a9a4497",
      "1809f37f6e9bab4bbf5d3327e94a93c6fbf7c92a",
      "12e6cb8932126873281becf60afc231e52996bb1",
      "4ec861540f6f7a4dfa1c3845c6c5fad27b418209",
      "20975224463ed00489b9ce57a504fc4675fa37b4",
      "fe57f0e329c8ea2dda2033fdd9a94f76965270c8",
      "1908856954bdd92c51d98fb3efab1ca7e690f503",
      "33f1c6b416f626809585fa155a99c839b28def7d",
      "46661fcdbf7a874c52b302a952df1b17b98c628e",
      "93243cefb086141ae25c09227472a393cda5b039",
      "20a382dfcfd07c60c6f220791e9c176cd136c426"
    ],
    "message_list": [
      "wip: add test fixture builder",
      "wip: add FixtureManager, no tests yet",
      "fix minor bugs",
      "fix pre-commit config and add check",
      "change name of cleanup function, add contextmanager",
      "updated docstrings with examples",
      "add copy_db to postgres",
      "add reset_db method",
      "make setup be truly noninteractive in non_interactive mode",
      "begin adding tests for FixtureManager",
      "complete fixtures test case",
      "FixtureManager: restore internal AiiDA settings",
      "move fixtures test out of main test suite",
      "plugin-fixtures: get sqla backend to work",
      "enhance docstring for FixtureManager",
      "test the PluginTestCase",
      "docs & actually run plugin test case test",
      "fix plugin test case test",
      "plugin test case: fix sqla db cleaning",
      "fixtures: bump max_connections for pgtest cluster",
      "bump pgtest requirement to >= 1.1.0 (for max_connections)",
      "simplify fixtures docstring"
    ]
  },
  {
    "pr_number": 5368,
    "commits_list": [
      "a96a79332f93c99354445aa89ee210ff374d5497"
    ],
    "message_list": [
      "switch to sphinx-book-theme"
    ]
  },
  {
    "pr_number": 5104,
    "commits_list": [
      "b8c60f8214c3f8272948d325c1bfc1b93755117a",
      "6ad79a05787fcc3dcc571e7b91bee7c3bfb5d773",
      "0f85a6c03776e69246811c700af5c276ca8eae8e"
    ],
    "message_list": [
      "\u2b06\ufe0f UPGRADE: psycopg2 allow v2.9\n\nFixed in https://github.com/aiidateam/pgsu/pull/27\n\nfixes #5002",
      "close psycop2g connections",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci"
    ]
  },
  {
    "pr_number": 4181,
    "commits_list": [
      "5377df84df7a1394a55b83d2f8c9dcb3d1270e46",
      "f06b5b5e8472482ecc2c99506f6ab30faebf4004",
      "0465b35047e61459f2119034da1bb725d856d65c",
      "60c7f5bc3d0f7176ba7bf4fabfc5cfa3e7fadaef",
      "90a66c43b0de919ae3f52892e58c023de9a63f3c",
      "d1e44f9a91607e1e638a4bb1647ff64a740b05e9",
      "826cf9038f3624261ddac01fdc200fa9fc5003b6",
      "5a7660f8be1547d9d030e64f096b3feb5ec406a4"
    ],
    "message_list": [
      "Allow `verdi status` to skip checks",
      "Fix unintentional change",
      "invoke super in setUp",
      "fix test",
      "fix test",
      "fix test",
      "Make `verdi status` return 0 even if daemon is not running (#1)\n\nThe daemon not running is not the same as not being able to connect to\r\nthe Postgres database or the RabbitMQ message broker. So it shouldn't\r\nreturn a non-zero exit code either. If the user wants to poll this, the\r\ncommand `verdi daemon status` should be used instead.",
      "Merge branch 'develop' into verdi-status"
    ]
  },
  {
    "pr_number": 1715,
    "commits_list": [
      "d92c573b5ad13189facc373c07b3e23cba747baa",
      "22747b6e6f399df4158639534253d8716dd8b6df",
      "b8e32dbabcf48c6d338a02b9940363841019eda1",
      "80aee2603c8ad248c9a3e038710fce7095eea5bb",
      "7ae0075fcf6c603d39a9ae37bd860bdd6dc887e2",
      "88339e243ff705805f4bd7ce1c4c94e63b46dcd1",
      "3f70a400168adbc0a6a381dc8b967637e90e2ad2",
      "986db057ea8c7d4ef28d24e63a15632af9071469",
      "bfcc3da60a903c655d548ab4972d8411f93bf8d7",
      "ec9573846151cb5be7a47421c150a98624e464ac",
      "173bf06a8824d3ff9cc24b03748d0c22fbcb65ee",
      "55b06196ffd94d4d636c4b773f5dfe052528b5a9",
      "382bde7e166027a1cb763c5ab72096ca376d70c5",
      "10c5946bb123f8110fa92ea444c1b7b0e51087b9",
      "6b2843ffe2bb2e6dad2fbdd745e7a2a3422b5773",
      "a133ac2d716ca4024f8fe4ed69fc9458ba6016ca",
      "13770275486d1b6b2b0e1e2e0561e59ce287f928",
      "34982b71d9f6a39d0f2e4ef0f177da200aadb4df",
      "9eb0127a229021d61227233414eb794d3a2c6dd2",
      "9165d64b5090939efa0354157183ab6b65dcfc96"
    ],
    "message_list": [
      "Added posibility to submit account and quality of service infomration to the SLURM, PBS and SGE scheduler",
      "Merge branch 'develop' into develop",
      "Merge branch 'develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge branch 'develop' of github.com:espenfl/aiida_core into develop",
      "Merged with upstream develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge branch 'develop' into develop",
      "Removed check for qos and account for squeue",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge branch 'develop' into develop",
      "Merged with develop",
      "Merge branch 'develop' of github.com:espenfl/aiida_core into develop",
      "Updated to comply with transfer to Python 3",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Removed superfluous empty line",
      "Updated setters for queue_name, qos and account for the slurm scheduler plugin to not set None if input is None"
    ]
  },
  {
    "pr_number": 4246,
    "commits_list": [
      "af1db75e3394d3d46c3f6e7f2a69796e044207ad"
    ],
    "message_list": [
      "do not parse None value to setup command"
    ]
  },
  {
    "pr_number": 5393,
    "commits_list": [
      "20b4518b4a4e80d44d657c2a6908b5ebdfe12af2",
      "de7d0e242f4a6357357b8aff0385854882336b44",
      "4da53dc058f5c221407c6f414264831a8f004279",
      "8c8cc167f88862cc08dfa8cca4c1e0bd0ff1a9c8"
    ],
    "message_list": [
      "fix: enable sqlachemy query caching in QB\n\nThe QueryBuilder declared a custom `jsonb_typeof` FunctionElement,\nfor which sqlalchemy disables query caching by default.",
      "simplify code",
      "reintrodyce type: ignore\n\nAlthough locally mypy 0.930 complains:\n\naiida/storage/psql_dos/orm/querybuilder/main.py:336: error: Unused \"type: ignore\" comment\nFound 1 error in 1 file (checked 1 source file)",
      "update test data"
    ]
  },
  {
    "pr_number": 4245,
    "commits_list": [
      "b3630311b13397efc035f165275054e186d4eee5"
    ],
    "message_list": [
      "Add the `verdi profile caching` command\n\nThis command will try to load the caching configuration from the caching\nconfiguration file, for any given profile. This is useful for users to\nmake sure that the file is correctly named and contains correct syntax,\nbefore actually starting to run new calculations.\n\nTo test this, a new pytest fixture is created that creates a completely\nnew and independent configuration folder, along with fixtures to create\nprofiles to add to the config and caching configuration files. Similar\ncode already exists for normal unittests, but this can be removed once\nthose have been refactored to pytests."
    ]
  },
  {
    "pr_number": 5801,
    "commits_list": [
      "3b579ae426f8b41c8a2a62713799d0250bd1f4a1",
      "86265aea148990de8c18d1979f5327e018def879",
      "48cf6092dcb57a26c44466e858b4f56a08a4e86e",
      "06d53a6621678b0899c465cb5fd27cc22394afd6",
      "d46d77be6817b3f5fe54ecb063d212a24b490898"
    ],
    "message_list": [
      "`ProcessBuilder`: Fix bug in `_recursive_merge`\n\nThe `_recursive_merge` method could raise a `KeyError` for a process\nbuilder that contains a dynamic port namespace. In this case it would be\npossible to merge in a value that contains a nested dictionay which\nwould try to access directly `dictionary[key]`, where `dictionary` is\nthe namespace of the builder, and so hit a `KeyError` since this nested\nnamespace didn't exist yet in the dynamical namespace.\n\nThe solution is to explicitly check if the `key` already exists in the\nbuilder's namespace, and if not, we assig the entire `value` to it, as\nthere is no nothing to recurse into.",
      "`ProcessBuilder`: Move `_prune` method to standalone utility\n\nThe method is a generic utility to operate on mappings and is not\nspecific to the `ProcessBuilder`. It needs to be used elsewhere soon so\ntherefore it is moved to a standalone function in a new utility module\n`aiida.engine.processes.utils`.",
      "`Port`: Add the `is_metadata` keyword\n\nIn the engine redesign from `v0.x` to `v1.0` the process interface\nneeded a way to distinguish inputs that are lined up to the process node\nas nodes themselves and those that are stored directly on the process\nnode, for example as an attribute. To this end the `non_db` keyword was\nadded to the `InputPort`. This keyword was used for the `metadata` input\nnamespace, to designate that these inputs would not be `Data` instances.\n\nThe name is quite unfortunate, however, since the inputs of these ports\nactually do get stored in the database, contrary to what the keyword\nsuggests. The most straightforward solution would be to rename the\nkeyword, however, for better or worse, the keyword has been adopted by\n(a limited amount of) plugin packages. A known application is to pass in\n`Group` instances which are not storable as nodes, but can be passed\nthrough a `non_db` port. Hypothetically, users may have used the port as\nwell to pass in sensitive data that should never be stored. Renaming the\nport or changing its behavior is therefore likely to break existing\ncode.\n\nInstead, the `is_metadata` keyword is added to the `InputPort` and\n`PortNamespace` through the `WithMetadata` mixin. When set to `True`\nthis keyword functions as the original intention of the `non_db` flag\nand an `is_metadata` port signals that its value will be stored in the\ndatabase but directly on the `ProcessNode` instead of being linked as a\n`Data` node. The naming makes sense, because the only use of this\nkeyword is by the `metadata` input namespace that all `Process` classes\nhave. The inputs in this namespace are stored, through custom logic in\nthe `Process` and `CalcJob` class, in the attributes of the node or\ndedicated columns of the node database model, such as the label and the\ndescription.\n\nThis addition leaves the behavior of `non_db` inputs unchanged and so\nplugin packages that use this keyword should continue to function as\nbefore.",
      "`Process`: Store JSON-serializable metadata inputs on the node\n\nThe promise of AiiDA's provenance is that all inputs to a `Process` are\nstored as nodes in the provenance graph linked to a node representing\nthe process. From the very early beginning, however, there needed to be\nexceptions of inputs to processes that were not nodes. Notable examples\nwere the various \"options\" set for calculation jobs. Even in AiiDA v0.x\nthese \"settings\" as they were called back then, were stored in the\nattributes of the node.\n\nIn the AiiDA v1.0 redesign, where all inputs of a process are defined\nthrough the process spec, these non-node inputs were implemented by\nallowing input ports to be made non-database storable, indicated by the\n`non-db` argument in the port declaration. These inputs would be passed\nto the `Process` instance and would be available during its lifetime,\nbut would not be stored in the database. Once again there are exceptions\nas certain inputs defined by `aiida-core` are stored on the node, but in\nvarious places. Notable examples are the `label` and `description` of\nthe process, and the `metadata.options` of the `CalcJob` class.\n\nThis historical decision has as a direct result in that it is difficult\nif not impossible in certain cases to reconstruct the exact input\ndictionary that was used to run a `Process` from the data stored on the\n`ProcessNode`. From a provenance point of view, this is a huge weakpoint\nand is what is being corrected here.\n\nThe input ports marked `non_db=True` on the base process classes\nprovided by `aiida-core`, `Process` and `CalcJob` were changed to use the\nnew `is_metadata` keyword instead in the previous commit, to remove the\ninconsistency between the naming and behavior. In this commit, all inputs\nthat correspond to `is_metadata` ports and are JSON-serializable are stored\nin the attributes of the process node under the key `metadata_inputs`.\nAll `is_metadata` input ports that are defined on process base classes by\n`aiida-core`, such as `Process` and `CalcJob` *are* JSON serializable.\nHowever, plugin packages can implement process classes with ports that\naccept inputs that are not JSON serializable, which is why this\nadditional condition has to be added. But all inputs defined by\n`aiida-core` should be covered.",
      "`ProcessBuilder`: Include metadata inputs in `get_builder_restart`\n\nThe `get_builder_restart` method on the `ProcessNode` base class would\nreturn a `ProcessBuilder` with the inputs set to those attached to that\nnode instance. The `CalcJobNode` would override this to add the metadata\noptions as well. This would be ok for `CalcJobNode`s, but if a restart\nbuilder was created from a `WorkChainNode` that calls a `CalcJobNode` it\nwould have also received options, but those would not be restored. One\ncould think that when calling `get_restart_builder` on a `WorkChainNode`\nthat we can just go down the callstack, find all the `CalcJobNode`s and\nset the options in the respective input namespaces. But this would not\nexactly reproduce the original inputs, as the options that a calculation\njob has received could have been a modified version of the original\noptions passed to the workchain, changed in the logic of the workchain.\n\nInstead, now that the exact metadata inputs for each process are stored\nin the attribute of the node, added in the previous commit, it is this\ndictionary that is used to restore the exact original inputs. It not\nonly addresses the problem of incorrect `CalcJob` options, but it also\nrestores any other metadata inputs such as the `label` and `description`."
    ]
  },
  {
    "pr_number": 4637,
    "commits_list": [
      "2b7045e2941166d60606250ca41696b67a2ca8d3"
    ],
    "message_list": [
      "`RemoteData`: deprecate automatic opening of a transport\n\nThe methods of the `RemoteData` class that require a transport now, for\nconvenience, automatically open that transport for the caller. However,\nin a high-throughput context this is not scalable as each call opening\nits own transport will incur too many opened connections to the target\nmachine.\n\nInstead, a single transport should be opened that is then passed to the\nmethod calls. All affected methods now have an argument `transport` that\nallows to pass a transport in. If it is not passed a deprecation warning\nis emitted as this usage will raise in the future as the argument will\nbe made required.\n\nNote that there is one backwards-incompatible change. The `is_empty`\nproperty had to be changed into a method in order for it to take the\ntransport as argument. It is not clear whether there is a scheme in\nwhich a property can be transformed into a method in a backwards\ncompatible way through a deprecation pathway."
    ]
  },
  {
    "pr_number": 3044,
    "commits_list": [
      "b811db8d955c86326958a0a58c50bd97c36f0201",
      "e58e0f4aac46f97318e8d9f8424ba6b0d1322b54",
      "72ad6fb6f78526a76de45c649c185b8c634e6a49",
      "81df59c630232d652cb657dc5af701299ee2af2e",
      "8ba33337fc164050b1f2aa8037ccd290a48b3d9e",
      "023a406046958cb6b3d11d0b26e9a301550e3b89",
      "68f7ffb24621c5c315172c594f7b38b35ea48713",
      "bf3651ac882eaed1e9c5ea193f9675cc0b33aa48",
      "8026ce40e9c414f76458f16d1977a2aa70941ad2",
      "14c7b06619cb73d28fd74473b81bbe56f68b6746",
      "e65d1cfacbb5bba1f68737c73f2d8979be0e1fea",
      "62148ddbd78ab080a1b113c061654cc26c1e49c4",
      "f835ff47abb11b0ddf027fdbff1503c395f502c5",
      "5517d90187e85acee1f9cf73d91facac13ea82bd"
    ],
    "message_list": [
      "Update documentation on computer setup #3014\n\nUpdate to reflect the changes on the latest AiiDA 1.0.\n\nThree problems that I see and did not touch yet (commands are not there anymore, but I'm not sure if the functionality works):\n- 'verdi computer disable COMPUTERNAME --only-for-user': the flag --only-for-user does not exist anymore\n(is the functionality somewhere else?)\n- The whole section about not bombarding: not sure if up-to-date with the current policy (isn't bombarding handled now?)\n- Computer.get('localhost') is not defined anymore in verdi shell",
      "fixes documentation for computer #3014\n\nSmall additional changes",
      "Docs: minor changes to computer setup (#3054)\n\n* Docs: minor changes to computer setup",
      "Minor text changes and address comments in #3044\n\nRemoved user-only and states that do not exist anymore. Left computer enable/disable without any specific comment",
      "Update docs/source/get_started/computers.rst\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update docs/source/get_started/computers.rst\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update docs/source/get_started/computers.rst\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update docs/source/get_started/computers.rst\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Merge branch 'fix_3014' of github.com:aiidateam/aiida_core into fix_3014",
      "Apply suggestions from code review\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Merge branch 'fix_3014' of github.com:aiidateam/aiida_core into fix_3014",
      "documentation - computer setup update\n\nincluding the -config option\ncleaning of the file\nminor fixes",
      "Apply suggestions from code review\r\n\r\nthank you for your suggestions!\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Merge branch 'develop' into fix_3014"
    ]
  },
  {
    "pr_number": 4896,
    "commits_list": [
      "a998bf45eec70926a76403c73bf12975bcc9ff15"
    ],
    "message_list": [
      "Config: warn when the configuration directory is created\n\nThe configuration directory is automatically created when it is loaded\nand it doesn't exist yet. This is useful for new installations, however,\nit can lead to problems when users accidentally make a mistake when\nsetting an explicit path with the `AIIDA_PATH` environment variable. If\na type is present in the path, instead of the intended existing config\nbeing used, a new empty one is created.\n\nWe add a warning to when the base configuration directory is created.\nNote that this will also be emitted if the `AIIDA_PATH` variable is not\nset, but even if the directory is created for the default setup this may\nbe useful information for the user.\n\nThe `aiida.manage.configuration.settings` module is also migrated to use\nthe `pathlib` module for file system path manipulations."
    ]
  },
  {
    "pr_number": 5779,
    "commits_list": [
      "bb674877ab0cf125ab1d742be62ca77263d32f12",
      "818d9ef8fae069f76634cd6821728d668b02aab0"
    ],
    "message_list": [
      "`WorkChain`: Protect public methods from being subclassed\n\nThe paradigm of the `WorkChain` requires a user in an implementation to\ndefine the workflow logic through classmethods of the `WorkChain`\nsubclass. While this gives great flexibility and choice to the user,\nthere is a risk that a user inadvertently chooses a method name that\nalready exists on the `WorkChain` base class. Usually the `super` is not\ncalled in this scenario and so the functionality is broken.\n\nThe typical example is where the user uses the `run` method as a step in\nthe outline of the `WorkChain`. The work chain will still run, however,\nonly that one step in the outline is called. Since the logic to continue\nto the next step in the outline is defined in `WorkChain.run`, which is\noverridden and now no longer called, the rest of the work chain is\nskipped without any warning or error message, leaving the user\nscratching their head as to what happened.\n\nHere we protect this and other public methods on the `WorkChain` class\nto prevent them from being overridden in subclasses. This is\naccomplished by adding the `Protect` class as a metaclass. Since the\n`WorkChain` already has the metaclass `plumpy.ProcessStateMachineMeta`,\nwhich it inherits from its `Process` base class, and all metaclasses\nneed to share the same base, `Protect` also subclasses the\n`ProcessStateMachineMeta` class.\n\nThe `Protect` class provides the `final` classmethod which can be used\nto decorate a method in the `WorkChain` class that should be protected.\nIf a subclass implements it, as soon as the class is imported, a\n`RuntimeError` is raised mentioning that the method cannot be\noverridden.\n\nThe test `test_report_dbloghandler` had to be fixed because it actually\nsuffered from the very problem that is being fixed. It used the `run`\nmethod to setup the test, but since the `check` was never being called,\nthe test always passed, even though the code `self._backend` in the\n`check` is incorrect.",
      "Merge branch 'main' into fix/5467/protect-workchain-methods"
    ]
  },
  {
    "pr_number": 4217,
    "commits_list": [
      "abe375edec29ef45be34623ccc2088f1c2ac3d3e",
      "c2371fc213dfad4d6c9841f7d3f4fa4f6a62f031"
    ],
    "message_list": [
      "docs: use new page rank feature\n\nReadTheDocs just introduced a way to control the search rank of\ndocumentation pages, allowing us to push hits in the autogenerated API\ndocs further down in the list of search results.",
      "further reduce search rank"
    ]
  },
  {
    "pr_number": 5073,
    "commits_list": [
      "8a4c554cb23b526cf443c7f1bccac8b03d4ac951",
      "7efd1a27d1ba20c1adf3a9b07c499066e960ff2a"
    ],
    "message_list": [
      "Entry points: add the `core.` prefix to core entry points\n\nThe AiiDA plugin system allows the functionality provided by `aiida-core`\nto be extended through entry points. These are defined in specific entry\npoint groups with a specific name. For these names to be uniquely\nresolvable, each name needs to be unique within an entry point group.\nThat is why the convention is that each plugin package prefixes their\nentry points with the package name, effectively namespacing all entry\npoints. Not only does this prevent entry point name clashes between\ndifferent packages, it also makes it explicit what package any\nparticular entry point comes from.\n\nUptil now, however, `aiida-core` was not respecting its own community\nguideline and failed to prefix its entry points with `core.`. Not only\ndoes this make it less obvious that these entry points are defined by\n`aiida-core`, which may become more problematic as the ecosystem keeps\ngrowing, but it also effectively blocks certain namespaces for potential\nexternal plugin packages. For example, the namespace `array` is\neffectively hijacked in the `aiida.data` group, since our `ArrayData`\nplugin and its subclasses use it. This would make it difficult for a\npotential `aiida-array` package to choose the name for its entry points.\n\nTherefore, we opt to update `aiida-core`'s entry point names and\nproperly prefix them with `core.`. The following entry point groups were\nupdated:\n\n * `aiida.calculations`\n * `aiida.cmdline.computer.configure`\n * `aiida.cmdline.data`\n * `aiida.data`\n * `aiida.parsers`\n * `aiida.schedulers`\n * `aiida.tools.data.orbitals\n * `aiida.tools.dbimporters`\n * `aiida.transports`\n * `aiida.workflows`\n\nThe missing entry point groups simply didn't have any entry points\nregistered by `aiida-core`. The exception is `aiida.node`, but this\nentry point group is not actually extensible by plugin packages and is\nmerely there for internal use. As such, the entry points are also never\nused by external packages. The classes are imported directly from the\n`aiida.orm` package.",
      "CLI: use factories in `PluginParamType` whenever possible\n\nThe `get_entry_point_from_string` method of the `PluginParamType`\nparameter type, tries to determine the entry point group and name from\nthe value passed to the parameter and when matched, attempts to load the\ncorresponding entry point. It was doing so by directly calling the\n`get_entry_point` method from the `aiida.plugins` module.\n\nSince the plugins that ship with `aiida-core` were recently updated to\nhave them properly prefixed with `core.`, the old unprefixed entry point\nnames are now deprecated. When used through the factories, the legacy\nentry point names are automatically detected and converted to the new\none, with a deprecation warning being printed. However, the command line\ndidn't have this functionality, since the `PluginParamType` was not\ngoing through the factories.\n\nHere, for the entry point groups that have a factory, the plugin param\ntype is updated to use the factories, hence also automatically profiting\nfrom the deprecation pathway, allowing users to keep using the old entry\npoint names for a while.\n\nThe factories had to be modified slightly in order to make this work.\nSince the `PluginParamType` has the argument `load` which determines\nwhether the matched entry point should be loaded or not, it should be\nable to pass this through to the factories, since this was always\nloading the entry point by default. For this reason, the factories now\nalso have the `load` keyword argument. When set to false, the entry\npoint itself is returned, instead of the resource that it points to. It\nis set to `True` by default to maintain backwards compatibility."
    ]
  },
  {
    "pr_number": 4204,
    "commits_list": [
      "90d3f8b131d0a6541385cb7e9e5a422d0b3bc025",
      "43104e3272a97bd78018013e9487eadb7809aeb5",
      "6f08bc9b9b714f8cdcbcdac8f0ddfa373effe5cf",
      "fe9323eb1ff81616600555eb9db8c86010ed3c5d",
      "f4c632b00f71b695569df632867372584582a0a0"
    ],
    "message_list": [
      "Limit `find_packages` in setup.py to aiida (sub-)packages.\n\nThe setuptools.find_packages function finds all packages in the\ncurrent directory. This means that `docs`, `tests`, and `utils`\nare currently installed when running\n```bash\npython setup.py sdist\npip install dist/aiida-core-<version>.tar.gz\n```\n\nTo avoid this, we use the `include` keyword to only find\n`aiida` and its sub-packages.",
      "Reduce files included in MANIFEST.in.\n\nRemove the following files from the distribution, by removing\nthem from MANIFEST.in:\n- `*.aiida` export files, used for testing\n- `*.png` files from the `docs` directory\n- files in `utils`, with the exception of `fasentrypoints.py`\n  which is used in the install process.",
      "Un-vendor the `fastentrypoints` dependency.\n\nThe `fastentrypoints` dependency is used to make console scripts\nfaster, in the case where they are **not** installed as wheels.\nIf they are installed as wheels, the console scripts are faster\nby default.\nBy specifying `fastentrypoints` as a build requirement in\n`pyproject.toml`, we can ensure that it can be imported whenever\nthe package is installed through `pip`.\nThis incurs a slight overhead of having to install\n`fastentrypoints` even when the package is installed as a wheel,\nbut we no longer have to vendor `fastentrypoints`.",
      "Use link to aiida.net for logo in README.\n\nUse an absolute link to aiida.net for the logo in the README,\ninstead of a relative link. This is needed to make the image\nwork on PyPI.\nRemove the now-unused logo file docs/source/logo_aiida_main.png.",
      "Remove `fastentrypoints` from `open_source_licenses.txt`.\n\nSince we no longer vendor the `fastentrypoints` code, it can be\nremoved from the `open_source_licenses.txt` file."
    ]
  },
  {
    "pr_number": 3285,
    "commits_list": [
      "0ea8db9b7fff13f9914c6066788f154f1ecce69f"
    ],
    "message_list": [
      "Allow to construct `SinglefileData` nodes from binary files\n\nThe `SinglefileData.set_file` method was opening the target file in\nnon-binary mode with default utf-8 encoding with no way for the user to\noverride it through the constructor. This would fail when a binary file\nwas passed. Since the purpose of this operation is just to copy the file\nwholesale to the repository this should simply be done in binary mode to\ncopy the bytes over."
    ]
  },
  {
    "pr_number": 3424,
    "commits_list": [
      "b30a3e9ce4fec1e2d39fee6f9ecd6d0305699335"
    ],
    "message_list": [
      "Update version requirements of dependencies\n\nA new CLI script is added `utils/update_dependencies` that helps with\nthe procedure that is necessary to update the dependency version\nrequirements. It is still a manual process but the two CLI commands\nshould already help quite a bit in the time it takes."
    ]
  },
  {
    "pr_number": 1726,
    "commits_list": [
      "80d599107a33ef7acade9d0820a11cf57865dbcd",
      "1531b22f2e4f5a3c5a56ec95a6703492636b0f67",
      "60007f77f92681064ccd238d9d58b16479632eca",
      "030e7163225aaa484a573b58cb08a07c9385a07f"
    ],
    "message_list": [
      "travis: explicitly install postgresql-server-dev-9.5\n\nBased on https://github.com/travis-ci/travis-ci/issues/9011 it seems we\nhave to specify explicitly the dependency on the postgresql-dev package\nby now.",
      "Revert \"skip pgtest related testcases until #1722 is fixed (#1724)\"\n\nThis reverts commit 777c16c14858593bee0a4e2a1027eaa89833a2e2 and fixes #1723",
      "travis: add proper postgresql bin-dir to path\n\nfixes #1722",
      "travis: fix ssh-keygen for private instances"
    ]
  },
  {
    "pr_number": 3788,
    "commits_list": [
      "a6b5905fcb370bbf828f36c2418a661551958c76",
      "d7c3c7f77a1a3e56aa37ede0576c0deefdf6a934",
      "77f7e0fb648cca41bfe43ba3f3d200b8b9f793d6"
    ],
    "message_list": [
      "verdi computer/code duplicate now reuses prepend_text and append_text\n\nThis required adding a new ContextualDefaultOption class\nto get a default callable that also receives the context.\nMoreover, a but was fixed in the function that prompts\nfor the prepend/append_text, as if the file was not touched/modified,\ninstead of reusing the text provided in input, an empty text was used instead.",
      "Merge branch 'develop' into fix_3747_duplicate_with_prepend_text",
      "Modify tests to check append/prepend text duplication\n\nSets prepend/append text when setting up the computer/code for the non-interactive tests.\nThe correct duplication was already tested by an assertion, but the prepend and append texts were set to the default value of an empty string."
    ]
  },
  {
    "pr_number": 3249,
    "commits_list": [
      "51642e8f3571a8368148911bebc5945af4fc2cab"
    ],
    "message_list": [
      "Remove `qe-tools` dependency\n\nEven though this dependency is optional through the `atomic_tools` extra,\nit only adds functionality to deal with Quantum ESPRESSO specific input\nfiles, which should therefore be added in the `aiida-quantumespresso`\nplugin and not in `aiida-core`."
    ]
  },
  {
    "pr_number": 4625,
    "commits_list": [
      "e7e93dec0861e5cdbcaba21b6204880454c28c11",
      "1a6198166eddf7dbdbaff97c91b3d69072dbed30",
      "85c44caf51e3345d6edfdcefd173c1fba271f883"
    ],
    "message_list": [
      "fix #3978: NodeLinkManager support attributes retrieving",
      "`NodeLinksManager`: add support for nested namespaces\n\nThe `NodeLinksManager` is used by the ``inputs`` and ``outputs``\nattributes of the ``ProcessNode`` class to allow users to quickly access\ninput and output nodes by their link label. Nested namespaces in link\nlabels are converted to double underscores before it is stored in the\ndatabase because it can only store a flat string. This is an\nimplementation detail, however, and the user should not have to know\nabout it and should be able to use the nested namespace. However, up\ntill now, one had to pass the link label as stored in the database, i.e.\n\n    node.inputs.nested__sub__namespace\n\nAfter this commit, it is now possible to use the more intuitive\n\n    node.inputs.nested.sub.namespace\n\nFor backwards compatibility, the old flat link is still supported but\nwill emit a deprecation warning.",
      "Merge branch 'develop' into fix/3978"
    ]
  },
  {
    "pr_number": 1578,
    "commits_list": [
      "a1df47f6417fb4b27aec3f2773eef73fc5fba1a8",
      "9d2ca9be34452523617f579edba7b5aa9a0c9fe8",
      "91c4c80d70bbb37da449cf56fc654eda97e20e62",
      "15e505693c8e46ca7825f02d56be2989a250a357"
    ],
    "message_list": [
      "Implement a generic IdentifierParamType for verdi commands\n\nA very common pattern for any verdi command will be an option or an argument\nthat takes on or multiple values that is supposed to identify a specific orm\nentity. In AiiDA there are three types of identifier:\n\n * ID\n * UUID\n * STRING\n\nMost entities have both an ID as well as a UUID that identifies them. Some\nhave an additional string based identifier, such as a label or a name.\n\nHere we define the class IdentifierParamType, which can be used to create\nreusable arguments and options that will load ORM instances based on the\npassed values, by deducing whether they are meant to repesent an ID, UUID\nor STRING. Note that since the type is lost and all values will be strings,\none has to guess the type. The value will be attempted to be interpreted\nas an ID, UUID and then STRING, in that order.\n\nThis approach can lead to ambiguities in some situations, where if an\nintended (partial) UUID can be interpreted as a valid ID. Or when a valid\nSTRING also happens to be a valid ID. However, as long as the users do\nnot use labels that are valid integers and use the full UUID in the worst\ncase, chances for these clashes should be highly unlikely.",
      "Add a mechanism to always solve identifier type ambiguity\n\nAll ORM entities can be identified with an ID, UUID or optionally a\nSTRING like identifier. Any potential ambiguity between an ID and UUID,\nfor example when a partial UUID is also a valid ID, can always be solved\nby including at least one dash in the UUID. The ambiguity when a STRING\nidentifier is also a valid ID or UUID is a little more tricky. We add the\nconvention here that if the last character of the identifier is an\nexclamation point, it will always be interpreted as a STRING type like\nidentifier. Consider three entities with the following ID, UUID and STRING:\n\n    1  b6420d91-7577 'some_name'\n    2  d9c4c081-f6bb '1'\n    3  0cf82610-3ae8 'b6420d91-7577'\n\nTrying to select either the second or third entity by their STRING identifier\nwill always result in the first entity being matched, given that their STRING\nidentifiers also match the ID and UUID, respectively of the first entity,\nTo break this one simply needs to use the identifier:\n\n\tIdentifierParam.convert('1!')\n    IdentifierParam.convert('b6420d91-7577!')\n\nThe added exclamation mark will break the ambiguity by forcing the method to\ninterpret the identifier as a STRING type.\n\nThis behavior is implemented in the `convert` method of the IdentifierParam\nparameter type and tests have been added that test the functionality.",
      "Refactoring of IdentifierParamTypes and OrmEntityLoader classes\n\nThis was necessary to allow to refactor the older load_node and load_group\nfunctions to use the same implementation as the OrmEntityLoader classes. These\nchanges allow load_node and load_group to leverage the OrmEntityLoader class to\ndo the heavy lifting while keeping the same function signature and functionality.\nThe old query builder utility in `aiida.orm.utils` that these functions where\nrelying on, which was reimplemented in a more general way in the OrmEntityLoader,\nwas removed. The BaseTranslator of the REST API was relying on this function but\nhas been reworked to be able to use the OrmEntityLoader as well.",
      "Allow base orm class to be overridden for OrmEntityLoader\n\nThe subclasses of the OrmEntityLoader have to define a base orm class\nfor which any entity that is to be matched and loaded has to be a\nsub class of. However, sometimes a user may want to narrow the query\nset even more and make the requirement on the base class even more\nspecific. To allow this, the get_query_builder and load_entity methods\nnow have the optional sub_classes argument, which takes a tuple of ORM\nclasses, each of which has to be a sub class of the base orm class.\n\nWhen valid, the query will be limited to entities which are a sub class\nof these specific orm sub classes.\n\nThis now also allows the IdentifierParamType and its subclasses to, upon\nconstruction, define a set of entry points, whose classes will be passed\nas this sub_classes tuple when a value needs to be converted. The reason\nto taking entry point strings and not the actual ORM classes is to prevent\nto have to load the database environment upon import of the verdi commands\nin order to keep the autocompletion and interface quick and snappy."
    ]
  },
  {
    "pr_number": 3616,
    "commits_list": [
      "0d1d47e1e9e4df7428b76499ba3246f5ac5769f3",
      "547965c6797dc629f82f260ce0f8893ea930efe2"
    ],
    "message_list": [
      "Reduce the number of public methods of `Computer` front-end class\n\nThe `aiida.orm.computers.Computer` front-end class had a mixture of\nproperties and methods to get and set various properties. Which one was\navailable depended on how that property was stored in the backend. Those\nthat are stored as database columns were exposed through property getter\nand setters, whereas properties that are part of the `metadata` JSONB\ncolumn had individual methods.\n\nBesides being inconsistent, having explicit methods for each property,\nof which there are already quite a few and which might increase in the\nfuture, is inconvenient. It also makes it more difficult to\nprogrammatically get all the properties that \"define\" a computer. By\ndefining all properties in a mapping class attribute, one can define all\nsetters and getters dynamically.",
      "Add `--format` option to `verdi computer show`\n\nThis allows to, in addition to the default table display, dump the\ncontents of a computer in yaml format which can in turn be used for the\n`--config` option of `verdi computer setup`. This makes the sharing of a\nparticular computer that has been setup very easy. Sharing yaml config\nfiles is more apt than AiiDA export archives which was the only possible\nway of sharing `Computer` instances to date."
    ]
  },
  {
    "pr_number": 5874,
    "commits_list": [
      "dbdf218076786e49dc2b10d9886a675d84578f60",
      "78c6e67184943ec5e10551c745c8087db31ebd39",
      "732e3305156c69483413447ec4aab7fc9806fad9",
      "dfbe7819c9c82c72f6298cecb03f5ed19b16b298",
      "d24aeb4aaafe06aaec92f3254862a2feab33467d",
      "2057e62f619fb7acc9a83ed93ade027bfaedf84a",
      "b7cd31e93725bf56d6322afbacd1c18802e89426",
      "8ce494c12b0601e1249269d393147fa8242639c3",
      "b543f4c0e378816b5fd4c4d4ca49201009adcd02",
      "294eb25c8a20c905311503a126fb31844030ba20"
    ],
    "message_list": [
      "fix: gracefully exit if no profile exists",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "Merge branch 'main' into no-profile-fix",
      "fix: add a decorator for CLI commands that require a profile to be loaded",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "test: add a test for require_profile_loaded decorator",
      "Merge branch 'no-profile-fix' of https://github.com/zahid47/aiida-core into no-profile-fix",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "Apply suggestions from code review",
      "Merge branch 'main' into no-profile-fix"
    ]
  },
  {
    "pr_number": 3893,
    "commits_list": [
      "6d956c904742cfaca6f5bead02eab18d0328f316"
    ],
    "message_list": [
      "Implement functionality to immigrate already completed `CalcJobs`\n\nWhen people start using AiiDA they typically already have many\ncalculation jobs completed without the use of AiiDA and they wish to\nimport these somehow, such that they can be included in the provenance\ngraph along with the future calculations they will run through AiiDA.\n\nThis concept was originally implemented for the `PwCalculation` in the\n`aiida-quantumespresso` plugin and worked, but the approach required a\nseparate `CalcJob` implementation for each existing `CalcJob` class that\none might want to import.\n\nHere we implement a generic mechanism directly in `aiida-core` that will\nallow any `CalcJob` implementation to \"immigrate\" already completed\njobs. The calculation job is launched just as one would launch a\nnormal one through AiiDA, except one additional input is passed: a\n`RemoteData` instance under the name `remote_folder` that contains the\noutput files of the completed calculation. The naming is chosen on\npurpose to be the same as the `RemoteData` that is normally created by\nthe engine during a normal calculation job run.\n\nWhen the engine detects this input, instead of going through the normal\nsequence of transport tasks, it simply performs the presubmit and then\ngoes straight to the \"retrieve\" step. Here the engine will retrieve the\nfiles from the provided `RemoteData` as if they had just been produced\nduring an actual run. In this way, the process is executed almost\nexactly in the same way as a normal run, except the job itself is not\nactually executed."
    ]
  },
  {
    "pr_number": 4553,
    "commits_list": [
      "1e3c775d183334d3022795c9ba48859d7335c881",
      "3d9bee05b72947228e236e81f3343645da1ceb63",
      "884e8167e95be5d3c18741f5defef45d2ef7950e",
      "2457fe86f17ee0fbdc9282588c56d5646a9db38a"
    ],
    "message_list": [
      "\ud83e\uddea Improve mypy type checking",
      "Merge remote-tracking branch 'upstream/develop' into improve-mypy",
      "fix/improve typing",
      "move flags to config"
    ]
  },
  {
    "pr_number": 2357,
    "commits_list": [
      "8249eddb069173f514fb55696018229a50cd6b65"
    ],
    "message_list": [
      "Reorganization of some top level modules\n\nThere are a few top-level modules whose purpose was not fully clear and\npartially as well as partially overlapping, which caused the content to\nbe quite unorganized and scattered. Here we reorganize these modules\nafter having clearly defined their purpose:\n\n * aiida.common: for generic utility functions and data structures\n * aiida.manage: for code that manages an AiiDA instance\n * aiida.tools: for code that interfaces with third-party libraries\n\nThe module `aiida.control` has been removed. Code that interacted with\ninternal components have been moved into `cmdline` or `manage` where\napplicable. The `aiida.control.postgres` has been moved to the\n`aiida.manage.external` module which is reserved for code that have to\ninteract with system wide components, such as the postgres database.\n\nThe module `aiida.utils` has been removed. Generic functionality has\nbeen placed in `aiida.common` and more specific functionality has been\nplaced in the appropriate sub folders."
    ]
  },
  {
    "pr_number": 4772,
    "commits_list": [
      "4cab1d61db5e9ceaceff9960a655011c8fa4219f",
      "0ed9eb185fb5e2f92b7ccd00e3b307602bee8f70",
      "821ca9795a989126fa44bc6c406312bef8b4a3aa",
      "7ea1ab03e782ae2de9c8847e3e3b2d219e721891",
      "2ff0024cb91fe9d37e2d3dfb619699dd761b0671",
      "8f11ec578b9e45dd686d91e7491fd47b74c5cbde",
      "d5dfe1b95f8110359c6025e8d992122c53a376ff",
      "3344edc6cc1fdd98ab8720860af73a1ec1569f4e",
      "05423978f281242985f1545f41e97d81ffc22bb0",
      "0542c3b8703ef5c7772e895dfdf77a09d5c50e76",
      "8e2811be199073903786be75c4a498ab5aa16cb2",
      "a71529100cfac870dc8dcdb3c5e3f3e99c5dfdab",
      "0a6b7adf4ee5d97b0f9d64fb4f20960d5c173227",
      "c70d2f3cb75ba0a381456e219f1801b9c7d7e43c",
      "c1fd448af744765c3e100fbfa4d2fd71035008ac",
      "1d14b6ad0c28035776535c25c53352a5ae9149cf"
    ],
    "message_list": [
      "\ud83d\udc4c IMPROVE: add type checking for aiida/orm/nodes/process",
      "final fixes",
      "Merge branch 'develop' into type-check-orm",
      "fix cyclic imports",
      "Update nitpick-exceptions",
      "Update test_calculation_node.py",
      "Merge branch 'develop' into type-check-orm",
      "simplify `CalcJobNode.get_authinfo`",
      "Merge remote-tracking branch 'upstream/develop' into type-check-orm",
      "Update environment.yml",
      "Revert \"Update environment.yml\"\n\nThis reverts commit 0542c3b8703ef5c7772e895dfdf77a09d5c50e76.",
      "fix pre-commit",
      "Merge branch 'develop' into type-check-orm",
      "Merge branch 'develop' into type-check-orm",
      "Merge branch 'develop' into type-check-orm",
      "fix pre-commit"
    ]
  },
  {
    "pr_number": 697,
    "commits_list": [
      "d830a31e7b3b18a2d8ca45b968e0da6f37090d39",
      "5b5613d894a0aaa519988c9dd588e1faa77a97dd",
      "8d9e89dca63f21a878632d85582194a6ac619ab6",
      "83658ba930252581db4b0c947d32ce419f493fc2",
      "e9bec8e50247678ca7422b5bf2d65fd6bfe99205",
      "224553b8781354a33757a484f56ab60ca970aedb",
      "c824ec725b0a6dc591423c4ef550f24b9fbf4d91",
      "88cdbcdf9bfaf438e24b360653d4a0b6be0a4add",
      "6d4371abd1343f8d8e41ec8300307e899dd6448d",
      "e4a5a71930420ad5460481fbf48f2902e282962f",
      "0d323a663e26d893b9b1b71d6016d7855a589080",
      "a2fb5ad08dfb35ce616bacb83b2debca6932e649",
      "58a0cc0d0ae30e83bc59e6f895061fa57b689dcc",
      "01719299deebec7cd0f46f86677de17303132fd9",
      "cfe07c6374401474ac176f512eea9d2125be918f",
      "c12b88f7ffcbc4bdebfe2ed4c057805ea47c4110"
    ],
    "message_list": [
      "add coveragerc and add generated folder to ignore",
      "WIP: move postgres management command into package",
      "refactor db management methods into module\n\n* in new `control` package because it is not 'common' and has side\n  effects\n* add pre-commit hook to ensure code quality of the control `control`\n  subpackage stays on an acceptable level",
      "add pre-commit (control only) to travis as stage",
      "retry making build stages work",
      "travis: add pre-commit linters to install phase",
      "travis: fix pre-commit requirements list",
      "postgres: complete unittests, use in profile cmd\n\n* + make yapf use google style (as documented)\n* + add tests to devel\n* + make sure quicksetup works",
      "add pgtest to testing requirements",
      "Update documentation (in-code & manual)",
      "add pre-commit requirements to extras",
      "fix class reference in docstring",
      "docs: update control, add pre-commit to developers",
      "change postgres port in example",
      "docs: fix broken refs.",
      "Add descriptive docstrs to aiida.common, aiida.control"
    ]
  },
  {
    "pr_number": 4966,
    "commits_list": [
      "4563c58699e899f32088a23efa1391556110618e"
    ],
    "message_list": [
      "example additional printing"
    ]
  },
  {
    "pr_number": 2097,
    "commits_list": [
      "d5dee97f9c24cf52d6e6eccce6d91097ff4b7547",
      "78fe54cb309c8dc279233e0a4755e4154ebb64b7",
      "28403830343a9fbf268c0997f8130ecd6475fe09",
      "075c7fd309227b99e6dcaddb323835fc18e96b86",
      "0c73a0987d6e2117759d2115646e45a4cbcffe60",
      "c835be39819be79c8e0e647308ec2c5940a74f08",
      "7cd2d0197d42e16812b167c1e5f5fb5458bc3729",
      "71b8313533080d95bf4ee2f350006f209fa2a778",
      "e3402de66c21f41e137df20f92c78908a9fea191",
      "b9cb9507d5e44fd0690fde3c2e1686f8c020ddec",
      "f9298ce8a390235f12b99d8dfd6f02beffd2df7e",
      "9954d30917606b57bc09205fd126d3ea2215c196",
      "73cd213ddb981492a2ca89974b69b4091e5da07e",
      "802105cdb377a0c7008078ad61505fbbdd12c3ef",
      "0bdcf7fadf6969e372a5ca7d4abfd63c4ff6a1a1",
      "00ee74d39f70ed0412bdb04841fc66c7b5e436dc",
      "5498e266cf03eb30b133a19c8b4317b3e2e8c6a7",
      "2a3862470f7df6a4bdf3fead33c9af577dfab8d2"
    ],
    "message_list": [
      "Added suppert for queries against Materials Project",
      "Merge branch 'develop' into mp_import_support",
      "Merge branch 'develop' into mp_import_support",
      "Removed pagesize and structure methods and corrected spelling",
      "Merge branch 'develop' into mp_import_support",
      "Merge branch 'mp_import_support' of github.com:espenfl/aiida_core into mp_import_support",
      "Moved license entry to DbEntry and replaced collection with properties",
      "Added a test and moved MPRester inside setup_db",
      "Changed naming scheme",
      "Merge branch 'develop' into mp_import_support",
      "Merge branch 'develop' into mp_import_support",
      "Merge branch 'mp_import_support' of github.com:espenfl/aiida_core into mp_import_support",
      "Added aiida.tools.dbimporters.plugins to the allowed test suite",
      "Merge branch 'develop' into mp_import_support",
      "Added files",
      "Simplified code and added a way to disable the test",
      "Merge branch 'develop' into mp_import_support",
      "Merge branch 'develop' into mp_import_support"
    ]
  },
  {
    "pr_number": 4375,
    "commits_list": [
      "7e49a19f58e30e47126f170420d1721d6985369a",
      "41758b5517cca1aeecff7245039d0fc891ace1a4",
      "79ecc0bcc3cb54838872c24ce696870002588772",
      "76afcdf9443b5cea2b5965efe5421be46d83e2b0",
      "253901d5e47b397467244a4f4275a319cc7ec56e",
      "8eb2c3f8df544e3d52a71670e01c09d8de7b5846"
    ],
    "message_list": [
      "Use -e install for tox runs\n\nImproves startup time for test runs",
      "Create docker-rabbitmq.yml",
      "Merge branch 'develop' into update-tox",
      "Update pyproject.toml\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "Merge branch 'develop' into update-tox",
      "Merge branch 'develop' into update-tox"
    ]
  },
  {
    "pr_number": 2721,
    "commits_list": [
      "5c61790debe7d416371a91a5ec5a1ec605af5606",
      "209dc2be10be2708e42c887e1c32b05145e12336",
      "3376db022110775013c02107695eabe06c1ea34a"
    ],
    "message_list": [
      "fix process control in direct scheduler\n\nThe direct scheduler was using `ps` for checking whether a command is\nrunning. However, `ps` does not show processes without a controlling\nterminal - for this you need to add the -x option:\n\n-x      When displaying processes matched by other options, include pro-\n        cesses which do not have a controlling terminal.  This is the\n        opposite of the -X option.  If both -X and -x are specified in\n        the same command, then ps will use the one which was specified\n        last.",
      "extended comments",
      "Merge branch 'develop' into issue_2649_direct_scheduler"
    ]
  },
  {
    "pr_number": 2801,
    "commits_list": [
      "d2979133cba01cc4ccd8822d23563e894be9545b"
    ],
    "message_list": [
      "Remove unused dependencies\n\n * `chainmap`: top-level dependency, unused\n * `python-mimeparse`: top-level dependency, unused\n   (introduced by cepellotti in 2015 as part of dependency freeze)\n * `meld3`: top-level dependency, unused\n   (introduced by cepellotti in 2015 as part of dependency freeze)\n * `ecdsa`: top-level dependency, unused\n   (introduced by cepellotti in 2015)\n * `click-plugins`: top-level dependency, unused\n * `anyjson`: top-level dependency, unused\n * `ujson`: top-level dependency, unused"
    ]
  },
  {
    "pr_number": 4967,
    "commits_list": [
      "37fecb8314d8ca8d330afae1bff83b689c3c5d15"
    ],
    "message_list": [
      "Engine: only call `get_detailed_job_info` if there is a job id\n\nIn the retrieve task of `CalcJobs` the `get_detailed_job_info` was\ncalled always. This would lead to problems if the node did not have an\nassociated job id. Normally this doesn't happen because without a job id\nthe engine would not even have been able to confirm that the job was\nready for retrieval, however, this can happen in artificial sitations\nwhere the whole calcjob process is mocked and for example an already\ncompleted job is passed through the system.\n\nWhen there is no job id, the `get_detailed_job_info` method should not\nbe called because it requires the job id to get any information. Without\nit, the method would except and since it is called within the\nexponential backoff mechanism, the job would get stuck in the paused\nstate."
    ]
  },
  {
    "pr_number": 3111,
    "commits_list": [
      "dc25441163c78f4c236f90f61ca6844ee896c19c",
      "cd85549b77b7a7959a5ae37eae367d17d1a19b63",
      "e0b25faaa353862abbbbba61c311b8deb8da909b",
      "c0636b8806a6e14c97c993ecddc72f2d17a25187",
      "4d0dd59e657502e748e3fe2174976473c75af4c3",
      "f328a7b8c3e9a63ec7a7dd0bbc55a857bc16071d",
      "2f55ed31af6a895557a449af101ff248d596ffd5",
      "5621beea77cf0d37f0c76ef940e9ffd3b7216fe3",
      "5ec61ec7d09f7e007d7469d52a16ece2ea707808",
      "ccd1888fc56d4fa0d3dfd057ab80e2076dc33bef",
      "9d5f81942952a5428c8d9d793dea47edb9cd3f13",
      "86c23277b4535c44f28d441f2e624b32e3829b62",
      "ce063c3612c097241f8210d0f7ef62732669355c",
      "843d213aa5b9f70a62ca83559699919fefbafaeb",
      "2c9e475f437f02fcaa364f46af24c41f46078b45",
      "e3818cf2b0051596ada2cc09c04c4c38825cd698",
      "d4a232de2edc28f41e9bc0e64c8a16a78a72c6ad"
    ],
    "message_list": [
      "update caching docs\n\n * reorder caching documentation, starting with how to enable it\n * add sketch of how caching works for calculations\n * move some technical details to developer documentation\n * add note on how to investigate why a calculation isn't being reused\n * general restructuring along similar topics",
      "fix reference",
      "Merge branch 'develop' into issue_3026_caching_docs",
      "start addressing comments\n\nstart addressing comments by @sphuber and @CasperWA",
      "address further comments",
      "Merge branch 'develop' into issue_3026_caching_docs",
      "address further suggestions by @sphuber",
      "suggestion by @sphuber",
      "Merge branch 'develop' into issue_3026_caching_docs",
      "Merge branch 'develop' into issue_3026_caching_docs",
      "more suggestions by @sphuber",
      "Merge branch 'issue_3026_caching_docs' of github.com:ltalirz/aiida_core into issue_3026_caching_docs",
      "Merge branch 'develop' into issue_3026_caching_docs",
      "remove mentions of caching for data nodes",
      "more fixes wrt caching vs hashing",
      "update context manager example\n\n + more fine-tuning of wording",
      "fix limitation wrt python code changes"
    ]
  },
  {
    "pr_number": 4080,
    "commits_list": [
      "a7a6e83a8dbca0d817cc3e9ec480d895b5183218",
      "ad9f82587bf7aeb554b5251ac500a5cb953b628b",
      "b502de0ed165c8d901cca57a5914d81d5977ce5e",
      "2e833f180ac8f1311ab76b7eb045ca7357ad3d63"
    ],
    "message_list": [
      "Docs: Add how-to on finding/querying data",
      "Docs: slim down finding data how-to",
      "Apply reviewer suggestions",
      "Apply (final?) reviewer suggestions"
    ]
  },
  {
    "pr_number": 4068,
    "commits_list": [
      "e3b9fa07321560416c4e1154d7883b465b3fd486"
    ],
    "message_list": [
      "Docs: add \"How to - Configuring your installation\"\n\nThis section describes how multiple instances of AiiDA can be isolated\non the same machine, how tab-completion can be activated and how the\n`verdi config` command can be used to configure profile options.\n\nExcept for the `verdi config` section, which is completely new, the\nother content has been adapted from existing documentation."
    ]
  },
  {
    "pr_number": 3471,
    "commits_list": [
      "00ebeb684535b19fba080a2efd3621f77cf2a8f4",
      "3fc2f37eb60555d9dedc09769e3228f1671ff6fe",
      "45df1b085498867c04545d348949269af92ec3b1"
    ],
    "message_list": [
      "Create CODE_OF_CONDUCT.md",
      "Merge branch 'develop' into add-cc-2",
      "fix email address"
    ]
  },
  {
    "pr_number": 5758,
    "commits_list": [
      "60054e165a40c34c8e0b300ec25b360df24dcc5e"
    ],
    "message_list": [
      "Fixtures: Modularize fixtures creating AiiDA test instance and profile\n\nThe `aiida_profile` is a fixture that will automatically load an\nexisting test profile, if specified by the `AIIDA_TEST_PROFILE` environment\nvariable, or otherwise create a temporary and isolated test profile from\nscratch. While very useful to make it easy to start writing tests\nagainst an AiiDA instance, it hard-coded most if not all of the\nconfiguration.\n\nFor example, the storage backend was hardcoded to the `core.psql_dos`\nstorage backend. This used to be fine since this was the only storage\nbackend available, but since `aiida-core==2.1` they are pluginnable and\nso it becomes necessary to be able to configure them.\n\nThe `aiida_profile` still works the same as in that it automatically\nprovides a fully loaded AiiDA test instance, and it still loads the\n`core.psql_dos` storage by default, but it does so through the new\n`aiida_profile_factory` fixture. This fixture can be reused in plugin\npackages to achieve the same effect, but they can override any setting\nof the profile configuration by providing those through the\n`custom_configuration` argument.\n\nThese new fixtures make the `aiida.manage.tests.main` module obsolete.\nIt was added when the tests were still run with the `unittest` module\ninstead of `pytest`. The module is deprecated as all of its\nfunctionality is completely provided by the new, more modular and more\nsuccinct pytest fixtures."
    ]
  },
  {
    "pr_number": 4190,
    "commits_list": [
      "1dfc4fc5dd3f744acaf3c117412312ad05c2d98b",
      "302cb43ccafd2f272e3d554a94d42108c6a8d234"
    ],
    "message_list": [
      "Update the pre-commit configuration\n\n * Drop `prospector`: it was only being used to run `pylint` but in that\n   case it is much simpler to just run it directly as `prospector` has\n   often given issues with incompatible dependencies\n * Add more hooks from `pre-commit` itself:\n   - end-of-file-fixer    automatically add new lines to end of files\n   - fix-encoding-pragma  automatically add the utf-8 encoding pragma\n   - mixed-line-ending    automatically deal with line break characters\n * Run `pylint` and `yapf` directly from the remote repository\n   Since `pylint` still needs to be run with `system` such that it can\n   see all dependencies and not throw false import errors, we still need\n   to keep it in the `pre-commit` extras requirements.\n * Renamed extra requires categories to match other projects\n   dev_precommit -> pre-commit\n   testing -> tests",
      "Apply the pre-commit hooks with new configuration"
    ]
  },
  {
    "pr_number": 4334,
    "commits_list": [
      "856fc06339aa3c4e0b0f8d4d6bd8b28624675986",
      "f9f6c9ca4ddd4f216c5a0e3e784976a78e8c5ca6",
      "bed201442561796ec8d891262462a14499d2a5ac",
      "31e981b98506633186518fa91c641b5cc8447316",
      "150ce93f229eaa4bf21238e5c57ec271fb3c3e3f"
    ],
    "message_list": [
      "`ArithmeticAddParser`: attach output before checking for negative value (#4267)\n\nFor the recent documentation revamp, the `ArithmeticAddCalculation` and\n`ArithmeticAddParser` were simplified significantly, by getting rid off\nas much as the unnecessary code, because they are being literally\nincluded as example for the basic how-to create a code plugin.\n\nA part that was removed was the `settings` input node that allowed to\nchange the behavior of the parser and allow negative sums instead of it\nreturning an exit code. This, however, in turn cause the Reverse Polish\nNotation tests on Jenkins to fail. Since these tests are not required to\npass, the changes were merged without a fix.\n\nIn the scope of the RPN tests, negative sums are fine, which anyway is\njust a mechanism to introduce some kind of failure mode for demonstration\npurposes. To fix this, without making the logic of the parser more\ncomplex, we simply change the order of attaching the output node and\nperforming the final check. Since the RPN workchains only check if the\noutput node is there and do not care about the exit status of the\ncalculation, they will happily continue and the code of the parser keeps\nthe same complexity.",
      "`Runner`: close loop when runner stops if runner created it (#4307)\n\nIf the loop is not closed, the file handles it managed during its\r\nlifetime might not be properly cleaned up, leading to file leaks.\r\nTherefore, if the loop is created by the `Runner` upon construction, it\r\nshould also close it when the runner closes. It cannot be done for loops\r\npassed into the constructor, because they migth actually still be in\r\nuse by other parts of the code.",
      "Release `v1.3.1`",
      "Merge pull request #4333 from aiidateam/release/1.3.1\n\nRelease `v1.3.1`",
      "Merge remote-tracking branch 'origin/master' into develop"
    ]
  },
  {
    "pr_number": 4158,
    "commits_list": [
      "56262182593a1043424517b4e96ce51f79287ab6"
    ],
    "message_list": [
      "Build(deps): Bump django from 2.2.11 to 2.2.13 in /requirements\n\nBumps [django](https://github.com/django/django) from 2.2.11 to 2.2.13.\n- [Release notes](https://github.com/django/django/releases)\n- [Commits](https://github.com/django/django/compare/2.2.11...2.2.13)\n\nSigned-off-by: dependabot[bot] <support@github.com>"
    ]
  },
  {
    "pr_number": 1214,
    "commits_list": [
      "d4986ea958797b6560ea8fd7219c6d5ca5f9bbd8",
      "7aef0521f18324148c8d9d4dfffe1299d7eeae14",
      "242c1ae00719e34c27ed32b687da84969828118e",
      "24464b11176b7a472b510692c7a9fb03b035aff8",
      "bdcf9281dd71f84b6cdc54628fd3568caddbb81e",
      "c33a733e90f341e265ccdc13152f47f8e6b9fa03",
      "d4c6ef229775d0159897b13e9ff63ce6f875c055",
      "106c7f8e661c6a20ce617573b6ac5d84d2a5f33b",
      "874f6ae2c60350091db5e438fba75c8d3d5e3577",
      "ac90745f927ca9e9bb70d6a9c0a3f9029ccbf833",
      "d056f6fa48cc220eb5fde619722581ad11ce6904",
      "729b985d1b5c28f3758eccb9654c44456592887c",
      "98e7db3155766ad61f9d3ce22fe1131a61702d96",
      "3a7ba4f457b392edaa8cd0f00bae2cb7b1b60e97",
      "a271a2f60e402905beb725f3435f8ac15edf000b",
      "938ec07e960473eb6807b029a40dc3a3afb584b6",
      "b479bdd3d940f68222d0258776c79bb8b4f3f563",
      "b81a255104534d393df78ece72eb2d6708005647",
      "e9100d3cd67b5a128b72bc55b6c768eea035bf40",
      "7ec17d18d0ab322483616a4062b44e3452c0aafa",
      "a41f40624b56a8abffeab07cbb1349255602b2d0",
      "448851bf27ad06ed3004e18f00bafba531e1a837",
      "3c38225b7d81e295e86d1578d81dd66432d2c5a4",
      "ee81e69cb94bb6dd71364acfa378574963c41cc9",
      "d706bf6ff24c270440188ddfa2e2a5a99bd6989d",
      "a0c19b05df6e43010852ad545659fa42289376ed",
      "8d908f1ea59c132f0123024f0d52667178bb4fe3",
      "bb2c57de72ae6968480720e49513d10aa695cd46",
      "728a5e0c529762d694672d2aefe5ea0587dc489b",
      "89dae8e37c7a13428bd35c8d476cbe4d8ffffa91"
    ],
    "message_list": [
      "Add prototype implementation of input serialization",
      "Fix incorrect function name.",
      "Merge branch 'workflows' of github.com:aiidateam/aiida_core into feature_serialize_input",
      "Use YAML instead of JSON in aiida.work.rmq",
      "Update plumpy version",
      "Fix syntax in WithSerializeFct __init__",
      "Add tests for serializing a class to a string.",
      "Merge branch 'workflows' of github.com:aiidateam/aiida_core into feature_serialize_input",
      "Implement serialization directly in the PortNamespace",
      "Move serialization back to Process __init__",
      "Serialize inputs also in runners.py",
      "Fix _del_checkpoin signature",
      "Merge branch 'workflows' of github.com:aiidateam/aiida_core into feature_serialize_input",
      "Fix work_chain test",
      "Add builder expansion and input serialization to 'submit' free-function",
      "Chane output name to 'output' in daemon test",
      "Fix expected result for serializing daemon test",
      "Remove dead code in abort test",
      "Add test for serialization with builder",
      "Merge branch 'fix_builder_serialize' into feature_serialize_input",
      "Add serialization in builder __setattr__",
      "Merge branch 'workflows' of github.com:aiidateam/aiida_core into feature_serialize_input",
      "Merge branch 'workflows' of github.com:aiidateam/aiida_core into feature_serialize_input",
      "Add documentation for 'serialize_fct' feature",
      "Merge branch 'workflows' of github.com:aiidateam/aiida_core into feature_serialize_input",
      "Merge branch 'workflows' into feature_serialize_input",
      "Merge branch 'workflows' into feature_serialize_input",
      "Merge branch 'workflows' of github.com:aiidateam/aiida_core into feature_serialize_input",
      "Merge branch 'feature_serialize_input' of github.com:greschd/aiida_core into feature_serialize_input",
      "Merge branch 'workflows' of github.com:aiidateam/aiida_core into feature_serialize_input"
    ]
  },
  {
    "pr_number": 3758,
    "commits_list": [
      "799a9753f26fd34ddded9a984857c7423ce816cc",
      "15d99d2705d574c3bcaff86a25c698564dd49c7d",
      "aed473a8fc5d6535c925a759d2c7f4c5b8924df3",
      "5a27fe2ba8639e4f49b595af45d74c80350a70af",
      "3a570054e6ceb45636f9810487fbc5f01ac9e3bb"
    ],
    "message_list": [
      "select python version for conda CI test\n\nIt turns out that when creating a conda environment from an\n`environment.yml` file, it is not possible to specify or override the\npython version of the environment from the command line.\n\nI.e. it turns out that\n\n   conda env create -f environment.yml -n test-environment python=3.7\n\nactually sets up a python 3.6 environment.\n\nUntil this functionality becomes available, I suggest we explicitly set\nthe python version it in the `environment.yml` file.",
      "try removing setuptools workaround",
      "switch back to python 3.6",
      "go back to python 3.7\n\nafter release of plumpy version for aiida 3.7",
      "Merge branch 'develop' into fix-conda-env"
    ]
  },
  {
    "pr_number": 4978,
    "commits_list": [
      "bddd1a9422ba50d97f0e1b0abe787eaccd2309e3",
      "41b7df69a1bb0a2712764e0502e4591dbd3d9736"
    ],
    "message_list": [
      "build(deps): bump pillow from 8.1.1 to 8.2.0 in /requirements\n\nBumps [pillow](https://github.com/python-pillow/Pillow) from 8.1.1 to 8.2.0.\n- [Release notes](https://github.com/python-pillow/Pillow/releases)\n- [Changelog](https://github.com/python-pillow/Pillow/blob/master/CHANGES.rst)\n- [Commits](https://github.com/python-pillow/Pillow/compare/8.1.1...8.2.0)\n\n---\nupdated-dependencies:\n- dependency-name: pillow\n  dependency-type: direct:production\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>",
      "Merge branch 'develop' into dependabot/pip/requirements/pillow-8.2.0"
    ]
  },
  {
    "pr_number": 5698,
    "commits_list": [
      "3f8af62fb62c0f54e526c66d178ba07e6224628a",
      "277afa8d8799d68d3f23875af92c4eff0c18c7b8",
      "1ca9683bd2a01c7156335ddd093e95d41e2c37e0"
    ],
    "message_list": [
      "Bump wrapt version to 1.14 fix conda install issue for py310",
      "update peter-evans/commit-comment version",
      "permission"
    ]
  },
  {
    "pr_number": 3953,
    "commits_list": [
      "172bff340ff0f4f5fb648bd038e2744e28b22273"
    ],
    "message_list": [
      "restapi: silence deprecation warning\n\nThe deprecation warning of the `--hookup` option was triggered also\nduring regular use since the REST API CLI was setting it to a default\nvalue.\nNow setting it to `None` in order to avoid the deprecation warning."
    ]
  },
  {
    "pr_number": 2225,
    "commits_list": [
      "84a57568b5893a2e9a6e98a290f79919ffc1f11a",
      "f5406dd13a833aa80fbd0a5deeca25c0cef7c48a"
    ],
    "message_list": [
      "Move the `Comment` class to the new backend interface",
      "Add support for `Comment` ORM class to `QueryBuilder`\n\nThis allows to make the methods of `Node` backend independent and\nas a result makes the implementation a lot simpler. The command line\ninterface is also adapted and simplified."
    ]
  },
  {
    "pr_number": 2949,
    "commits_list": [
      "01b2eba9f853410e9b8ee609302f58c4ccd889ec"
    ],
    "message_list": [
      "Improve the efficiency of `DbLog` migration for Django\n\nThe old implementation was using Django's ORM to get the count and rows\nof certain rows in the `DbLog` table. However, this was generating\nextremely inefficient queries that made the migration run for days on\nbig databases. A similar problem was encountered with the original\nSqlAlchemy implementation. That was solved in an earlier commit\n`80a0aa9117f399a5385a30ba0ce3e2afc7016a4e`, which created a more\nefficient query. We now replace the Django ORM solution with the same\ncustom SQL query as used for SqlAlchemy. This should significantly\nimprove the efficiency of this migration."
    ]
  },
  {
    "pr_number": 5085,
    "commits_list": [
      "4e4f744be99e08233c5f73955a84ab7c7440e82d",
      "77e82a38a018b465e8ff75cad5a54e180b501db0",
      "bd3d21425f930d45895dad6bb7859a5eeda8463f",
      "2b9725d48a4235eef95c53c40ffe3dc045744ed7",
      "ec81d0b40f9d50c8c332b8c5f52bac061b56918f",
      "634031113bea86f46a512a4639b96046d166056d",
      "35fe868fc4b4c2c5adf105dd782360415c0f2b2f"
    ],
    "message_list": [
      "Logging: add the `report` method and `REPORT` level to `logging` module\n\nThe built-in `logging` module comes with a number of pre-set logging\nlevels, such as `DEBUG` and `INFO`. AiiDA adds the `REPORT` level, which\nsits in between `INFO` and `WARNING` and is designed to add some extra\ngranularity of control for messages. The report level is intended to be\nused for messages that are purely informational, and so not a warning,\nyet they should probably be shown by default. Traditionally, `INFO` log\nmessages logged by module code are often a bit too verbose to always\nbe showing by default. That is why AiiDA sets the default loglevel to\n`REPORT` allowing code, such as `WorkChain`s to emit informational\nmessages that are shown by default, without also forcing all `INFO`\nmessages to be shown, which are typically too verbose.\n\nTo make the `REPORT` level behave like a built-in level, we hot-fix the\n`logging.Logger` class to also have a `report` method, such that it can\nbe called as one would call `debug()` or `info()`.",
      "CLI: add `-v/--verbosity` option to all `verdi` commands\n\nThe custom class used for the `verdi` command, `MostSimilarCommandGroup`\nis renamed to `VerdiCommandGroup`. The reason for this is that it adds\nnew functionality, in addition to the existing functionality of\nsuggesting command names in case the command is not found, to\nautomatically add a `-v/--verbosity` option to all commands. This option\nprovides a choice to set the log level for the AiiDA logger. This allows\na user to control the verbosity of output that is emitted through the\nlogging system in a way that is consistent across all `verdi` commands.\n\nSince through this method, the verbosity option is only added to sub\ncommands, the top-level commands `verdi` itself won't inherit from this\nmechanism. Instead, the option is added manually and explicitly. Note\nthat in this case the callback of the verbosity, which sets the log\nlevel on the profile, does not work, since for the top-level command the\nverbosity option is parsed before the profile is set on the context.\nThat is why the processing of the verbosity is done explicitly in the\ncommand itself.\n\nBy making the `-v` option available for all commands, the user can pass\nit at only level of the sub command stack. There is one side-effect\nhowever. Since `verdi` itself already had the `-v/--version` option,\nwhich clashes with the `-v/--verbosity`, the `-v` shorthand for the\nversion option had to be removed. For consistency the `-h` shorthand for\nthe help flag, which is the only other option that is also available\nacross all commands, is also removed.\n\nNote that this change also fixes a bug that only made the command name\nsuggestion functionality available at the top level but not for the sub\ncommands. By overriding the ``group`` method to use the same class as\nthe top-level `verdi` command, all subcommands should recursively have\nthe same functionality.\n\nThere is one subtlety where if the `cls` is set explicitly by one of the\nsubcommands, that should be respected. It can't simply override it with\n`VerdiCommandGroup` because the subclass may have overridden the\n`get_command` as well with custom functionality. This puts the onus on\nthe developer of subcommands with custom classes to make sure it\ninherits from `VerdiCommandGroup` if it is going to overwrite the\n`get_command` method and make sure it calls the super.\n\nFinally, the ``run_cli_command`` fixture, designed to help test CLI\ncommands, needed to be updated to manually apply the ``VERBOSITY``\noption. In testing, the sub command is direclty fetched from the module\nwhere it is defined, instead of getting it through the ``click`` API\nstarting from the base ``verdi`` command. This means the functionality\nthat applies the ``VERBOSITY`` option is not hit and so it has to be\ncalled manually.",
      "CLI: pipe `cmdline.utils.echo` functions through logger\n\nThe utility functions in the `aiida.cmdline.utils.echo` module are\ndesigned to be used in CLI commands to print output to stdout or stderr\nstreams. This is now changed to instead log the message to the\n`aiida.cmdline` logger. This logger was added in the previous commit.\nThe reason is that the verbosity of this logger can be controlled\nthrough the `--verbosity` option that was added to all commands.\n\nTo match the available log levels, the `echo_report` method was added,\nwhich, as the name suggests, logs messages at the `REPORT` level. The\n`echo` method, which was used to simply print a message without a\nparticular prefix, logs also at the `REPORT` level, since that is the\ndefault log level in AiiDA. This makes sense since when developers used\n`echo` they probably want it to show by default, but it is not a warning\nand so should not be shown when the log level is set to `WARNING`.\n\nThe `echo_highlight` method has been removed as it was essentially a\nthin wrapper around functionality that is already provided by `echo`.\nInstead of using the different `color` argument, one can use `echo`\nusing the `fg` argument which is more consistent with the interface from\n`click`'s methods which are wrapped.\n\nAll remaining direct calls of `click.echo` and `click.secho` in the code\nhave been replaced by the relevant `cmdline.utils.echo` command to make\nsure that they are properly piped through the logging system and respect\nthe `--verbosity` option. Note that a few instances remain, for example,\nin the `verdi completioncommand` command, but this is intentional as\nthey should not be influenced by the `--verbosity` option.",
      "CLI: replace `echo_info` by `echo_report`\n\nIn the previous commit, the `echo` utility functions were changed to go\nthrough the logger system, instead of directly using `click.echo` to\nprint to stdout and stderr. Since the default log level is `REPORT` in\nAiiDA, messages logged with `echo_info` will no longer be shown by\ndefault, unless the log level is set to `INFO` or `DEBUG`. To return to\nthe previous behavior, all `echo_info` instances are replaced by the new\n`echo_report`, unless the existing message really corresponds to info\nthat is a bit verbose and can be hidden by default.\n\nCalls to `info` for the `MIGRATE_LOGGER`, `EXPORT_LOGGER` and the\n`DELETE_LOGGER` are changed to `report`. The current implementation\nusing these loggers assumed that the default log level would be `info`\nand so these messages should be shown by default. This choice was made\nsince this functionality is mostly called through the command line. With\nthe current change where the default is now `report` these messages\nwould no longer show up by default.",
      "CLI: remove `VERBOSE` option and replace with `VERBOSITY`\n\nThe `VERBOSE` option is obsolete since all commands now automatically\nhave the `VERBOSITY` option exposed. The `VERBOSE` option was mostly\nused to configure a logger (most notably the `DELETE_LOGGER`) but this\nis no longer necessary. Since those loggers are children of the main\n`AIIDA_LOGGER`, whose level is controlled through the `VERBOSITY`\noption, the child logger will inherit its value.\n\nSome commands used the `VERBOSE` option more as a boolean switch. To\nreproduce this behavior, a utiltiy function `is_verbose` is added to the\n`cmdline` module, which will return `True` if the loglevel of the\n`CMDLINE_LOGGER` is equal or smaller than the `INFO` level and `False`\notherwise.\n\nThe `verdi archive` commands used an ad-hoc implementation of the\nverbosity option. This has been removed and they now also rely on the\nlogging level that is set on the `AIIDA_LOGGER` through the global\n`VERBOSITY` option.",
      "CLI: ensure all loggers have a custom formatter\n\nSo far, the utility functions of the `aiida.cmdline.utils.echo` module\nhave been rerouted through the `CMDLINE_LOGGER` which properly formats\nthe log message with just the message itself. However, the loggers of\nany other modules will inherit the formatter of the parent `AIIDA_LOGGER`\nwhich will prefix the message with verbose content such as the\ntimestamp, name of the logger and more, which is not what we want for\noutput generated by CLI commands.\n\nThe solution is to define a custom formatter, `CliFormatter`, which is\nadded to the new module `aiida.cmdline.utils.log`, and which is\nconfigured by default for the `CMDLINE_LOGGER`. The `configure_logging`\nfunction is updated to now take a keyword argument `cli`, which when set\nto `True`, will configure this new formatter for all loggers. By calling\nthis method with `cli=True` in the callback of the `VERBOSITY` option,\nall loggers should be correctly configured to only print the message\nwith the log level prefixed, when invoked through the CLI.\n\nThere is one peculiarity where the `load_backend_if_not_loaded`\ndecorator utility also needs to call the `configure_logging` function.\nThe reason is that this function loads the database backend, which\nindirectly calls the `configure_logging` to add the database log\nhandler, but this does not pass `cli=True`. This means that the CLI\nspecific configuration is undone.",
      "Logging: remove `override_log_formatter*` utilities\n\nThe `override_log_formatter` and `override_log_formatter_context`\nutilities of the `aiida.common.log` module were used in certain CLI\ncommands to temporarily change the formatter of the logger. This was\ndone because they used the logging system as the way to output messages\nto the terminal, but the default logger prefixed messages with extensive\ninformation such as timestamps and the name of the logger, where only\nthe message was required.\n\nThese utilities are no longer needed since this custom formatting for\nthe CLI is now done ensured by configuring the `CliFormatter` custom\nformatter which takes care of only including the log level as prefix."
    ]
  },
  {
    "pr_number": 2744,
    "commits_list": [
      "76d722883794023238d313c1fe789121bff9a241"
    ],
    "message_list": [
      "Capture `KeyboardInterrupt` in `Runner.run` and kill processes\n\nCurrently, if a user runs a `Process` in a local interpreter and then\ninterrupts the interpreter the process will be lost. However, the node\nwill reflect the last active state. This can be confusing to new users,\nwho will still for example see a \"process\" in the `Waiting` state.\n\nTo prevent this situation, the `Runner._run` method is modified and\nattaches listeners to the `SIGINT` and `SIGTERM` signals that when\nemitted will trigger a clean function that will log the interrupt and\ncall `process.kill` on the process that is currently in the scope.\nThis should ensure that if a user runs a process in a local interpreter\nand interrupts it, the process will be properly killed, which will cause\nthe node to properly reflect what has happened.\n\nThe current solution works for a single process being run, but has\nissues for nested processes. Sometimes only the current process is\nkilled and the others in the chain are unaffected before the interpreter\nshuts down."
    ]
  },
  {
    "pr_number": 5301,
    "commits_list": [
      "64b3dd2f770ba5e3eb417947abde15bd4bb5002a",
      "6e95d15bb11670d4afd4d25ffec654270d51374b",
      "3b0fae3f41dab3e7929545b455113b890bb1ca3d",
      "ae592f1b2a851429d815fa2d07276a5b7cf89257"
    ],
    "message_list": [
      "Dependencies: drop support for NumPy 1.18\n\nAccording to AEP 003, which states that we follow NEP 029, support for\nNumPy 1.18 can be dropped as of December 22 2021.",
      "Docs: replace link for exit code with more stable one\n\nThe link used to provide more background information on the concept of\nexit codes has been experiencing downtime often recently, causing the\nbuilds to fail in the `docs-build` workflow.",
      "Dependencies: explicitly compile `pymatgen` with compatible `numpy`\n\nWith the release of `numpy==1.22.0` on December 31 2022, our CI builds\nstarted failing. The reason being that `pymatgen` would get built with a\nversion of `numpy` that is incompatible with the version of `numpy` that\nwould be installed in the environment in which the tests are run.\n\nThe reason for this is that `pymatgen` does not specify an explicit\nversion of `numpy` in its build requirements and so will pick the latest\nversion. But `numpy` only guarantees foreward compatibility with its ABI\nand so if compiled with a certain version, the resulting binary won't be\ncompatible with older `numpy` versions.\n\nIn this case explicitly, `pymatgen` was getting built with `numpy==1.22`\nbut then the tests were run with `1.21.4` which would cause the\nexception.\n\nThe solution is to ensure `pymatgen` is compiled with a version of\n`numpy` that is ABI-compatible with the version that will be used in the\nactual test run. The way to do this is to first explicitly install a\nversion of `numpy` that we need and then install `pymatgen` using the\n`--no-build-isolation` flag to force `pip` to not install build\nrequirements but take them from the existing environment. This is also\nwhy we need to install the `wheel` package explicitly, because this is\nalso a build requirement of `pymatgen` that no longer will be installed\nautomatically by `pip` due to `--no-build-isolation`.\n\nFinally, there was a necessary change for the dependencies of Python 3.10\nwhere we update the version of `pymatgen` because older versions could\nnot be built since the building used CPython code that has been removed\nin Python 3.10.",
      "CI: fix failing pre-commit due to `mypy` exception\n\nThe pre-commit step was failing due to an exception in `mypy`. This was\ncaused by a bug in the typing of `sqlalchemy`. Upping the requirement in\nthe `pre-commit` extra to `sqlalchemy[mypy]~=1.4.29` makes sure that we\ninstall a version that fixes the problem. The `mypy` extra will install\nthe `sqlalchemy2-stubs` package, so that explicit requirement is removed.\nNote that these new versions require `mypy==0.930` so we update that\nversion.\n\nThe update in `mypy` comes with some improvements that allow us to\nremove some previous ignore statements and others have to be added or\nupdated.\n\nIn addition, we ensure that we install pinned versions in the workflow\nby installing from the requirements file, and then just adding the\nadditional dependencies from the `pre-commit` extra."
    ]
  },
  {
    "pr_number": 4255,
    "commits_list": [
      "cb53015995eef6cb788b33ae13b2aed3b80c862f"
    ],
    "message_list": [
      "Add support for \"peer\" authentication with PostgreSQL\n\nPostgreSQL allows \"peer\" authentication to connect to the database. This\nis signaled by a `hostname` that is set to `None` through `pgsu`. In this\ncase, the `hostname` part of the connection string of the SQLAlchemy\nengine should be left empty. If it is `None` it is converted to an empty\nstring otherwise it would be converted to the string literal \"None\"."
    ]
  },
  {
    "pr_number": 5391,
    "commits_list": [
      "714f624630b66c2d04ff3673815443738db5a78e",
      "031b5681f20f30a35918ec391c9446eeeafb5b95"
    ],
    "message_list": [
      "Dependencies: remove `simplejson`\n\nThis library was used as a drop-in replacement for the `json` module of\nthe standard library to provide consistency of JSON (de)serializing\nbetween Python 2 and Python 3. Since we have long since dropped support\nfor Python 2, we can now simply use the standard library.\n\nThe module `aiida.common.json` provided an interface to `simplejson` and\nwas used throughout `aiida-core` instead of `json`. This module is now\ndeprecated and `aiida-core` just uses `json` directly.\n\nThere are two significant changes that needed to be taken into account:\n\n * `simplejson` provided automatic serialization for `decimal.Decimal`\n   but `json` does not. The support for serializing these types to the\n   database is maintained by performing the serialization manually in\n   the `aiida.orm.implementation.utils.clean_value` method. All instances\n   of `decimal.Decimal` are serialized as `numbers.Real`, e.g. floats,\n   so the behavior should remain the same.\n\n * The `aiida.common.json` wrapper functions `dump` and `load` accepted\n   file objects in text and binary mode. However, the `json` analogues\n   only support text files. The wrapper functions therefore had to be\n   adapted to decode and encode, respectively, the contents of the file\n   handle before passing it to the `json` function.\n\n * The code that calls `json.dump` passing a handle from a temporary file\n   generated with `tempfile.NamedTemporaryFile` had to be updated to\n   wrap the handle in `codecs.getwriter('utf-8')` since the default mode\n   for `NamedTemporaryFile` is `w+b` and `json.dump` requires a text\n   file handle.\n\nFinally, the `aiida.common.json` module is deprecated and will emit a\ndeprecation warning when it is imported.",
      "Merge branch 'develop' into fix/5374/drop-simplejson"
    ]
  },
  {
    "pr_number": 4774,
    "commits_list": [
      "3eca50f67d0c02657b47e7aa1079c16f9f668210",
      "1ca0b32a3d2794eb020f8766960b7563c36e6c5b"
    ],
    "message_list": [
      "pytest: reset test manager after test run\n\nTests written in pytest did not reset the test manager, resulting in the\nglobal singleton manager instance remaining after the end of the test.\n\nThis was already taken care of in the teardown method of the unit test\nclass AiidaTestCase.",
      "disable pytest system tests"
    ]
  },
  {
    "pr_number": 2744,
    "commits_list": [
      "76d722883794023238d313c1fe789121bff9a241"
    ],
    "message_list": [
      "Capture `KeyboardInterrupt` in `Runner.run` and kill processes\n\nCurrently, if a user runs a `Process` in a local interpreter and then\ninterrupts the interpreter the process will be lost. However, the node\nwill reflect the last active state. This can be confusing to new users,\nwho will still for example see a \"process\" in the `Waiting` state.\n\nTo prevent this situation, the `Runner._run` method is modified and\nattaches listeners to the `SIGINT` and `SIGTERM` signals that when\nemitted will trigger a clean function that will log the interrupt and\ncall `process.kill` on the process that is currently in the scope.\nThis should ensure that if a user runs a process in a local interpreter\nand interrupts it, the process will be properly killed, which will cause\nthe node to properly reflect what has happened.\n\nThe current solution works for a single process being run, but has\nissues for nested processes. Sometimes only the current process is\nkilled and the others in the chain are unaffected before the interpreter\nshuts down."
    ]
  },
  {
    "pr_number": 4242,
    "commits_list": [
      "299ec7c7e3896cb366f10d6e27dcfc0130761103",
      "650f4772947c04c94beb742d863727cb10549287",
      "1d1bda226ccb4f33434029496c3ef5a5bc858474",
      "418503f51b1a3292e392037399ba099ebbd01d96",
      "7ac192bac7ace14ac71e4433258632f76a08a936",
      "62a158dcf418cb8a9c1f9e1e61eedb561257cc8b",
      "4d4d2d118b644b8ea4ca4eadd75c0b8f8ce7edb5",
      "e03e27caa4eb959ed4ba3d9fd36603250bc3c1ba",
      "5dee703f3ef4961b4c821c15d791e4af4b6fe4ca"
    ],
    "message_list": [
      "move \"Running on supercomputers\"",
      "apply @sphuber review change",
      "reference fix",
      "Add sussestion by @csadorf\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Merge branch 'develop' into docs-move-run-supercomp",
      "Apply suggestions from code review",
      "Merge branch 'develop' into docs-move-run-supercomp",
      "Apply suggestions from code review\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "fix indentation"
    ]
  },
  {
    "pr_number": 5760,
    "commits_list": [
      "7193699584a5e9b1da397723b552246878a7653d",
      "198e9d20d1ad2eee57715325c18d8a457184f5f1"
    ],
    "message_list": [
      "`StorageBackend`: Add the `initialise` method\n\nThe current interface of the `StorageBackend` provides two method to\ncontrol the initialisation and lifetime of a storage:\n\n * `migrate`\n * `_clear`\n\nThe `migrate` method will initialise a completely new storage or migrate\nan already initialised backend to the latest schema version, whereas the\n`_clear` method will clear all data from the storage. In reality, the\ncurrent implementation of `_clear` will actually completely destroy the\ncontainer and recreate it from scratch, which causes a new repository\nUUID to be generated.\n\nConceptually, but also practically, it is beneficial to restructure this\ndesign slightly. When using storage for a testing profile, it may be\nnecessary to completely reset a storage backend and reinitialise it.\n\nTherefore the `initialise` method is added to the `StorageBackend`\nabstract base class. This method replaces the `migrate` method as the\nmethod to be called when a new storage backend is created and needs to\nbe initialised. It should make sure that at the end of the initialisation\nthe storage is at the latest schema version, i.e., by calling `migrate`\ninternally.\n\nThe method takes an argument `reset`, which is `False` by default, but\nwhen set to `True`, the storage backend should be completely reset before\nreinitialising. Typically this entails that the storage is completely\ndestroyed and reinitialised in a fresh state.\n\nThis is used by the `TemporaryProfileManager` test utility class when\na new temporary profile is created. Although the tests in `aiida-core`\nessentially always use the `PsqlDosBackend` and so the Postgres database\nwill always have the correct initialisation, storage backends are now\npluginable and in plugins we may want to switch between storage backends\nwithin a test suite, in which case the storage backends needs to be fully\nreinitialised.",
      "CI: Add `workflow_dispatch` trigger to `nightly.yml`\n\nThis allows the workflow to be triggered manually. This is useful if a\nPR is created that doesn't trigger the normal conditions, for example\nthe files marked in the `pull_request` trigger are not modified, but we\nstill want to verify that the changes pass the tests."
    ]
  },
  {
    "pr_number": 2766,
    "commits_list": [
      "c45cd4edb2c5e7fcde81a6da669188e171e6573c",
      "89949c8777e97ad4c33522c9cc392df1fdd0e88b",
      "2ea332b1f145bb7f12025185eeeb87f181105f29",
      "60ee9201d51216d9d57585850416a83dbeba9263",
      "f9d7afd27dcea44a802d0263ca85ae317100908b",
      "00596b39e566e1da8153523f856210d3c7715fba",
      "900b38947ea2555c82930fcc4a8a54d9595309ba",
      "95e173058f028fc733442a1ba5d27836adb0738f",
      "60906f7fa458d23c2713871d33699c79116da130"
    ],
    "message_list": [
      "Change the mechanism of loading database backend environment\n\nSince the introduction of the `Backend` class and the abstraction of the\nORM through this interface, it is in principle no longer necessary to\nload the correct implementation statically, by importing the\nimplementation once when the profile is loaded. Instead of loading the\nbackend as soon as the profile is loaded, this is deferred to the\nmanager who will lazily load the backend that is configured for the\ncurrently loaded profile. This means that even when the backend is\nloaded, one can simply load another profile, which will reset the\nmanager and the backend, and when necessary the backend of the newly\nloaded profile will be loaded.\n\nNote that dynamic switching of backends can potentially introduce\nproblems when certain objects that are specific to a profile, are not\ncorrectly reset when another profile is loaded. This is currently not\nexhaustively tested and should be added soon, so until that is done\nchanging a loaded profile is forbidden as soon as the database backend\nhas been loaded. Until the backend is loaded, the profile can be\narbitrarily switched, which is necessary for the `verdi setup` command,\namong others, to function properly.",
      "Determine test mode based on profile instead of global variable\n\nInstead of relying on the global variable `TEST_MODE` that was being set\nin the `aiida.settings` module when tests were being run, instead we\nshould rely on whether the currently loaded profile is marked as a test\nprofile or not.",
      "Move `USE_TZ` to `aiida.manage.configuration.settings`\n\nThis allows to remove `aiida.settings` and means there is now only one\nsingle place to configure instance wide settings.",
      "Allow to specify a password in constructor of `aiida.orm.User`\n\nThe backend models provide for a password to be set, and the CLI setup\ncommands even ask for one, but the front end user class did not provide\na way to pass that password to the backend.",
      "Create transparent interface for the `Config` and `Profile` class\n\nThe `Config` and `Profile` classes are utility classes that represent\nthe configuration of an AiiDA instance, which is currently the\n`config.json` file and the profile configurations that it contains.\n\nTo provide a clear interface for developers to set and retrieve certain\nattributes, we abstract the structure in which their content is stored\nin the configuration file on disk, such that they don't need to know\nabout the data representation. An additional advantage is that if the\ndata structure for storage on disk has to change, the class can map\nthese changes and the interface can remain the same.\n\nThe downside is that these classes can no longer be used for the\nmigrations but they will have to act on the raw data structure as they\nwould have been written to disk at the time of the migration.",
      "Restore non-interactive functionality of `verdi quicksetup`\n\nThe presence of the `non-interactive` option not only triggers the\ncommand to automatically try to guess the connection method to Postgres\nbut it will also influence the other interactive options to simply\nreturn the default if it was not explicitly passed. In interactive mode\nthe options will always trigger the prompt, even if there is a default.",
      "Set user information as default configuration upon first setup\n\nThe configuration provides several global options that can be used to\nstore the user information that should be used by default:\n\n * user.email\n * user.first_name\n * user.last_name\n * user.institution\n\nThe setup commands `verdi setup` and `verdi quicksetup` have contextual\ndefaults that will prepopulate the corresponding required fields if\nthese options are defined in the configuration. To enable users only\nhaving to input this information just once, after setting up a new\nprofile, if these options have not yet been set, they are populated\nwith the user details provided in the first setup.",
      "Create backup of config.json if it contains unsupported keys\n\nThe backup is useful because the unsupported keys will simply be dropped\nand the current configuration file will be overwritten.",
      "Implement workaround to fix docs build failing on python 3\n\nThe documentation builds started failing after the mechanism for loading\nprofiles and the database backend was changed significantly. Since then\nSphinx started failing with an exception that the attributes `run` and\n`runctx` are not defined in the `profile` module. What it should be\ntargeting is the `cProfile` method, but somehow it ends up targeting the\n`aiida.backends.profile` module. Simply adding the attributes there\nfixes the problem, even though we have absolutely no idea why. Given\nthat the module is deprecated anyway and will soon be removed at which\npoint the problem should no longer exist."
    ]
  },
  {
    "pr_number": 5613,
    "commits_list": [
      "0e12e2a15c8eaf6098419ff81ad18f306f5018c0",
      "9bd3ab1dc991282823336a1aa473062250d9b750",
      "4cb6ee93a24ad307e8ab5aa0a76ea4cf76bab133"
    ],
    "message_list": [
      "Adding a sanitizer to the ArrayData when they are going to be served as JSON via the REST-API. Addresses #5559",
      "Adding a test for the download of ArrayData via the API to ensure that they are valid JSON.\n\nAdding a note in the documentation explaining that invalid JSON values will be replaced by None which will be rendered as nulls.",
      "Merge branch 'main' into dev"
    ]
  },
  {
    "pr_number": 993,
    "commits_list": [
      "6b07940b20cc5413e15170a55901f11085eeb2e2",
      "78c2bd24c9e8302027b7ab20256af9bfa621a371",
      "d403d89e436a7968c215bd37977d9a2b98898369",
      "52757e0455fdb4b0923606e7848bf76f0b3ac086",
      "baa7963915b32bfdc3bae6846b85dcf4a5d87cb4",
      "7cfd00a329e4150acb8736409cd89bc95dcee42a",
      "7d742b6cae65eb43cf02002a21da0d67740c0afc",
      "7d4e4c9683ff59c88529e1eff91849adfc3e70bb",
      "cd0772b3b44b4917e3e3d9ebc85538b0fb0e09bb",
      "dbe67549e7ef2893011031dab2e8a33a490506b5"
    ],
    "message_list": [
      "add `verdi data plug` command group for plugin data commands.",
      "fix broken unused import",
      "change plugin command to `verdi data`, get rid of plug",
      "add new click-plugins requirement",
      "add cli command plugin tutorial",
      "minor improvements to cli plugin tutorial",
      "@with_dbenv decorator: signature from decorated fn",
      "add missing requirement 'click-spinner'.",
      "cli plugin tutorial: 'pk' argument should be int",
      "doc: rename & move tutorials, deprecate old cli plugins"
    ]
  },
  {
    "pr_number": 2413,
    "commits_list": [
      "dfa7cccbc35e7d257c3d691615d320b0c83f8055"
    ],
    "message_list": [
      "Add support for the export/import of `Comment` entities\n\nTo make the export of `Comment` entities possible, the `QueryBuilder` had to\nbe extended to support retrieving comments for a given `Node`. The\nfollowing changes were applied:\n\n * Added support to join user to comment (`with_user`)\n * Added support to join comment to user (`with_comment`)\n\nThe `verdi export create` has a new flag to include or exclude the\nexport of comments for nodes that are to be exported, defaulting to include:\n\n `--include-comments/--exclude-comments`\n\nDocumentation has been updated to include new `QueryBuilder` join args\nin table and the `metadata.json` example has been updated in documentation\nto include correct Comment info."
    ]
  },
  {
    "pr_number": 3841,
    "commits_list": [
      "2e8e32e22c2ee36b5efb2fcff20b7ec5ac95e397",
      "726a580b4c22744670b5772b5252ca8f6d3d718e",
      "c27b43148bd8479114e10f0cecfb7b2c2ec68cc7",
      "5a099c368bb20b852a1108a2d81951bb2ba8297d",
      "20164a95043d562936d8ee5a1aabab038b1ba0ea"
    ],
    "message_list": [
      "Configure slash commands to trigger CI.",
      "Disable CI tests to reduce CI load during debugging.",
      "Fix syntax.",
      "Add reaction.",
      "Test example command."
    ]
  },
  {
    "pr_number": 1190,
    "commits_list": [
      "ba77ea10b7ab8b481ed2eb5d77de149c34075a0f",
      "dce15c220d588d2d47218750b4a75d28ff84e420",
      "65a7e41e1bd23177133d2369b6c8cdaaffeff3b1",
      "544e6410dce3a905de426e7b316475daf974888e",
      "a1809044e91e0ee4aa4a1972fea91ff063267098",
      "e8aaa89ee51031aca6bc5a9d792076caa35132e5",
      "0375d47e3a38a1fb5a8dcdaf05e833ec0878399e",
      "73c6b7a7635455fa66e13cb50f9b6b47ba729865",
      "bf1db4cc070e1a980ad1668a687beba10a2c5132",
      "bb3aab63ba239eccc04ed2bd3a112037ec7fb3bc",
      "dc20e374d169d887d2de456e083639ba4d829991",
      "c24f49cc120f6c5193141bd03a654fc6b1c24a78"
    ],
    "message_list": [
      "add cif.py to linter\n\nformatting + fixing minor issues",
      "cifdata: add scan_type attribute\n\nExample:\nfrom aiida.orm.data.cif import CifData\n\na = CifData('/path/to/file.cif', scan_type='flex')\n\nAlso add test to check that results are identical.",
      "cifdata: add parse_policy\n\nparse_policy decides whether the CIF file is parsed at creation\ntime of CifData or whether it is parsed only when needed.\n\nExample:\n\nfrom aiida.orm.data.cif import CifData\na = CifData(file='/path/to/file.cif', parse_policy='lazy')\n\n(default is 'eager', which leaves the code unchanged)",
      "cifdata: revamp code for new options and add test",
      "Merge branch 'release_v0.11.1' into issue_1186_flexible_cif_class",
      "set_file resets formulae and spacegroup numbers\n\nadded more tests...\none is currently failing. need to think about how to conveniently\npopulate formulae and spacegroups",
      "cifdata: update formulae upon get_formulae\n\nsame for spacegroup_names",
      "Merge branch 'issue_1186_flexible_cif_class' of github.com:ltalirz/aiida_core into issue_1186_flexible_cif_class",
      "remove side-effects of get_formulae again\n\nsame for get_spacegroup_numbers\n\nInstead, add a parse() method that will parse the CIF file, if desired.",
      "cifdata: more tests and fixes",
      "Merge branch 'release_v0.11.1' into issue_1186_flexible_cif_class",
      "cif: put main_file_name argument back\n\nMany of the `prepare_...` functions have a `main_file_name` argument.\nIt's completely unused in CifData, but I'm putting it back for the\nmoment in order to avoid breaking other people's code."
    ]
  },
  {
    "pr_number": 4678,
    "commits_list": [
      "10a6d08cfef907a1368150e69728086c8f48e869"
    ],
    "message_list": [
      "Fix: pre-store hash for -0. and 0. is now the same"
    ]
  },
  {
    "pr_number": 3777,
    "commits_list": [
      "404195afeab4656e61b83a086735b7ad99ad411e",
      "a1fd54c7f7189dcffae36fecaee923565ddc9750",
      "4bf3a298103bb17290148480c5860211ab13634f",
      "5e5c8a7f996683b5484006cb49cd081085261a2b",
      "d9d6182f1a7ca252384af22a60bb793676900f10",
      "569e119ecd6d81a8613621f600483c2eac6c17b7",
      "52eb781c5c6c910ade9b5125a33f34817f451d5e",
      "a25bc4cc1f2420714ecf97ef90a64ca702e5ad86",
      "4a91a90bb56912af2a2b34af8fbdf62d20d3e089",
      "ce6dd367092918c8504d6ea27b66071be6b4d750",
      "bf8e6ef8de24a6bd5d8ef05fb9dbbaae5e0f7b40",
      "693a561f1d3d98d543de818b5308408c0a1ea93f"
    ],
    "message_list": [
      "Fix bugs in _store_from_cache and repository.erase\n\nThe _store_from_cache needed to erase the content of its sandbox\nfolder before copying over the content of the cache source.\nCurrently, the existing contents are mixed with with the contents\nto be copied in.\nIn fixing this, we discovered an issue in repository.erase, where\nan unstored node would try erasing the (inexistent) folder in the\npermanent repository, instead of the SandboxFolder.",
      "Add regression test for bug in store_from_cache\n\nThe regression test creates a Data node with a file in a directory\nin the repository. After storing, it creates a clone, and stores\nit when caching is enabled. Finally, the hashes of the original\nand clone are compared.\nChecked that this test fails when the store_from_cache fix is not\npresent.",
      "Check that caching is used in the test",
      "Merge branch 'develop' into fix_store_from_cache",
      "Use 'aiida_profile' fixture in test.",
      "Add unit tests for repository erasing",
      "Call 'reset_db' before test_store_from_cache",
      "Add missing assertRaises to repository test",
      "Tweak docstring of repository test",
      "Merge branch 'develop' into fix_store_from_cache",
      "Add comment explaining repository erase, use new clear_db fixture",
      "Merge branch 'develop' into fix_store_from_cache"
    ]
  },
  {
    "pr_number": 2798,
    "commits_list": [
      "facdd3e328fb5900ed758a30ab13106b1b6aa56e",
      "641f1ce79739cd0e73d8458df1b6c7c3ac1d0a5c"
    ],
    "message_list": [
      "Fix bug in Django migration `0023_calc_job_option_attribute_keys`\n\nThe migration changed the keys of certain node attributes but failed to\nproperly join the matched attributes on the matched nodes. This could\ncause attributes with the target keys to be affected even if the\ncorresponding node would not have matched the filter. The chances of\nthis having occurred are minimal to non-existent, as it would require\nnon calculation job nodes with these particular keys to have existed, so\nwe simply fix the migration and leave it at that.",
      "Fix bug in SqlAlchemy migration `7ca08c391c49_calc_job_option_attribute_keys`\n\nThe migration incorrectly converted the dictionary attributes with the\nnew keys `resources` and `environment_variables` to text, because the\nwrong operator `->>` was used. This operator gets the JSONB value as\ntext, whereas `->` keeps the type, which is the one that should have\nbeen used. This was missed by the tests, because the tests undid the\ntype conversion using `ast`. Collossal fail..."
    ]
  },
  {
    "pr_number": 5380,
    "commits_list": [
      "d8bd352b8a4e006e476c8d268643b703d664589a",
      "0edbff3f0e81dac67f35a399f54430ad380d02b6"
    ],
    "message_list": [
      "Add metadata for WCI\n\nAdd metadata for workflows community initiative, see https://workflows.community/systems",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci"
    ]
  },
  {
    "pr_number": 5625,
    "commits_list": [
      "a63f8ef835c29083a62d98157a851ac6fff0a4ae"
    ],
    "message_list": [
      "API: Add function to start the daemon\n\nSo far, the daemon could only be started through `verdi` and there was\nno way to do it from the Python API. Here we add the `start_daemon`\nmethod to the `aiida.engine.daemon.client.DaemonClient` class which will\nstart the daemon when called.\n\nTo start the daemon, the function will actually still invoke the `verdi`\ncommand through a subprocess. The reason is that starting the daemon\nwill put the calling process in the background, which is not the desired\nbehavior when calling it from the API. The actual code that launches the\ncircus daemon is moved from the `verdi` command to the `_start_daemon`\nmethod of the `DaemonClient`. In this way, the daemon functionality is\nmore self-contained. By making it a protected method, we signal that\nusers of the Python API should probably not use it, but use the public\n`start_daemon` instead.\n\nThis implementation may seem to have some circularity, as `start_daemon`\nwill call a `verdi` command, which in turn will call the `_start_daemon`\nmethod of the `DaemonClient`. The reason for this is that the `verdi`\ncommand ensures that the correct profile is loaded before starting the\ndaemon. We could make a separate CLI end point independent of `verdi`\nthat just serves to load a profile and start the daemon, but that seems\nunnecessarily complicated at this point.\n\nBesides the added function, which was the main goal, the code is also\nrefactored considerably. The implementation of the command line command\n`verdi daemon start-circus` is now moved to the `_start_circus` method\nof the `DaemonClient` class. The `verdi devel run_daemon` command is\nmoved to `verdi daemon worker`, which makes more sense as a location.\nThis command launches a single daemon worker, which is nothing more than\nan AiiDA process that runs a `Runner` instance in blocking mode. The\n`verdi daemon start` will run a circus daemon that manages instances of\nthese daemon workers. To better match the nomenclature, the module\n`aiida.engine.daemon.runner` was renamed to `worker`."
    ]
  },
  {
    "pr_number": 4503,
    "commits_list": [
      "cb1835d0ab347f91a23566f1b975c3ec83b212f8",
      "cbf37255d4d45c24a2bf3c78c2b0781a194c8916",
      "8b83e7d0bb8c79f28733046f7265aba2bd0d3800",
      "22da3ae2316da3355a4c1598eaa30ceff45254c1",
      "6950a685803d2bec7ddc39c1954ff3696d3f6f57"
    ],
    "message_list": [
      "add docker-compose for local testing",
      "Update test_importexport.py",
      "Apply suggestions from code review",
      "Merge branch 'develop' into docker-compose",
      "Merge branch 'develop' into docker-compose"
    ]
  },
  {
    "pr_number": 4168,
    "commits_list": [
      "f58b4c79d045c59446bb31431fbe34c5beb22d65"
    ],
    "message_list": [
      "Raise when calling `Node.objects.delete` for node with incoming links\n\nThis guard was already in place for a node that has outgoing links but\nif a node had only incoming links, the method would happily delete it.\nThis means that one could delete output nodes for example as long as\nthey had not been used as inputs. Note also that this would leave the\nlink in place as the deletion is not cascaded automatically.\n\nDeletion through the objects collection currently cannot deal with\nautomated cascading rules on links and nodes. These rules are\nimplemented in the ORM itself through the graph traversal rules for\ndeletion. Nodes that are not isolated should therefore only be deleted\nthrough the utility function that makes sure to leave the resulting\nprovenance graph in a coherent state."
    ]
  },
  {
    "pr_number": 4984,
    "commits_list": [
      "d9f4940b81e59f1f8fec8503c6653b2a2a120dd5"
    ],
    "message_list": [
      "`ProcessBuilder`: ensure instances do not interfere\n\nThere was a bug in the `ProcessBuilderNamespace` constructor where it\nconverts the ports of the port namespace into dynamic properties and\nassigns them to the class. The problem here is that the property is\nadded to the class instead of the instance. This meant that if you have\ntwo classes that happen to have ports with overlapping names, when\nconstructing the builder for the second, the properties of the first\nbuilder that have port names.\n\nThe simple solution of simply add the dynamic properties to the instance\ninstead of the class won't work, because properties are descriptors and\ncan only be assigned to a class. The workaround is to create a subclass\non the fly and attach the properties to that new class."
    ]
  },
  {
    "pr_number": 5169,
    "commits_list": [
      "3a77c5fa54f73babee56671bd21f4620448be611",
      "f285cb9e98330284588fe4596638d4222a573492",
      "4c0ef56b2c9964e4b5f14a46a6810d1b958d5409",
      "3dd6d564146d848732b721e9090097b0ea1cd2f7",
      "570b909fd145a528777dde40c72e7c8c552ff8fa",
      "da6fd431304cb72eac9c2f98961909018b3222bc",
      "cf101317b8d6728f773b6e302572691c1c4f9ab2"
    ],
    "message_list": [
      "\u267b\ufe0f REFATOR: Delegate `RepositoryBackend` control to the `Backend`",
      "Update nitpick-exceptions",
      "Fix tests",
      "Merge branch 'develop' into move-repo-to-backend",
      "Merge remote-tracking branch 'upstream/develop' into move-repo-to-backend",
      "Merge branch 'develop' into move-repo-to-backend",
      "add back redundant methods"
    ]
  },
  {
    "pr_number": 4878,
    "commits_list": [
      "5c15aca2fc910cabc87040fdcbd2882c452a252b"
    ],
    "message_list": [
      "Fix `aiida.cmdline.utils.decorators.load_backend_if_not_loaded`\n\nThe function incorrectly used the `PROFILE` global variable to determine\nwhether the database backend environment had been loaded. This variable\nis set as soon as a profile is loaded, however, this does not\nautomatically mean that the database environment is also loaded. These\ntwo actions are separate on purpose such that a profile can be loaded\nwithout having to load the backend, since that is an expensive operation\nand is not always necessary.\n\nThis bug meant that sometimes the backend would actually not be loaded\neven though the `with_dbenv` decorator was correctly used. This affects,\namong other things, all CLI commands that rely on the backend being\nloaded but having no code in its execution path that will automatically\nload the backend, such as `verdi archive import`. Any commands that deal\nwith the ORM still work, since loading any ORM entity will automatically\nload the backend of the current profile if not already done. Since the\nimport functionality circumvents the ORM, it didn't have this failsafe\noperation.\n\nFinally, the bug was not noticed by the unittests because they run in an\nenvironment where the database backend is loaded anyway for the test\nprofile effectively hiding the bug of the `load_backend_if_not_loaded`\nmethod."
    ]
  },
  {
    "pr_number": 5427,
    "commits_list": [
      "ad915f31ab6c8f9fcf4684bc520f9a9669117c8c"
    ],
    "message_list": [
      "feat: make aiida ipython magics available\n\nAiiDA comes with an `%aiida` IPython magic that loads the default\nprofile (and imports some modules).\nHowever, in order to take advantage of this, so far, users had to\nmanually copy a python module into their IPython startup folder.\n\nThis change makes it possible to load the ipython magics as follows:\n\n%load_ext aiida\n%aiida"
    ]
  },
  {
    "pr_number": 5510,
    "commits_list": [
      "82c45bb4e467b0e3b75d390f4ed353eb149bffb5",
      "aa1e324803c4a14f18b0347fda91b19c1a2682d4",
      "1ec894ac971d04812b2790371ac824bc50241ddf",
      "4f79e003cb2972f0972a0e96b281e1155a9c8c09",
      "0f3dc5c65ca0dbd032e8d1cb0d34e6f7c107e417",
      "abed94f922c2cc6630fde0f550731412d728a56c",
      "2d3cf681c2f71e59b3894449210461e94c3e199b"
    ],
    "message_list": [
      "Add the `InstalledCode` and `PortableCode` data plugins\n\nHistorically, the `Code` data plugin came in two flavors: codes that\nrepresent an executable on an associated `Computer`, and codes that were\nincluded in the repository of the node instance itself. For the latter,\nwhen used in a `CalcJob` its files would be uploaded to the remote\nworking directory.\n\nThe use of a single class for these two different types of entities has\ncaused quite a bit of confusion, not only for end-users, but also for\ndevelopers as it leads to convoluted code.\n\nHere, we add two new data plugins, `InstalledCode` and `PortableCode` that\nare designed to implement the exact same behavior, but do so in a more\nuser-friendly and maintainable way by having them as separate classes.\nThe both subclass from `AbstractCode` which provides the functionality\nthat is shared between them.\n\nFor backwards compatibility, the original `Code` implementation is kept,\nbut it is moved to the `aiida.orm.nodes.data.code.legacy` module. The\nidea is that this class is deprecated and users are encouraged to start\nusing the new classes. In a future version, the `Code` class will be\ndecommissioned and an automatic migration will be included to migrate\nits existing entries to a `InstalledCode` or `PortableCode` instance.",
      "Add CLI options for all code implementations\n\nThe `AbstractCode` class implements the `get_cli_options` method which\nwill return an `OrderedDict` of the dictionary returned by the\n`_get_cli_options`. This dict contains a spec definition of the CLI\noptions a command should have to construct an instance of the relevant\ndata plugin class.\n\nThe `InstalleCode` and `PortableCode` add their specific options to this\nby overriding the `_get_cli_options` method. With this approach, the\n`verdi` CLI could dynamically add commands that allow to create their\ninstances by discovering them through their entry points and building up\nthe command options by the spec returned by `get_cli_options`.\n\nEach entry in the dictionary returned by `get_cli_options` will\nessentially be passed to the `click.option` decorator, so its keys\nshould match the spec of that method.",
      "CLI: Add the `DynamicEntryPointCommandGroup` command group\n\nThis custom implementation of `click.Group` will allow a CLI command to\ndynamically generate its subcommands. The subcommands will be based on the\ninstalled entry points for a specific entry point group. Optionally, the\nentry points can be filtered using a regex pattern.\n\nThis approach was already in use to dynamically provide the commands to\nconfigure computers with the various transport type plugins. The\nfunctionality is generalized and added as a single class to the\n`aiida.cmdline.groups.dynamic` module.\n\nThe code base already had one other implementation of `Group`, namely the\n`VerdiCommandGroup`. For consistency, this is now also moved from\n`aiida.cmdline.commands.cmd_verdi` to `aiida.cmdline.groups.verdi`.",
      "CLI: Add the `verdi code create` command group\n\nThis is the successor of `verdi code setup`. The difference being that\nwhere `verdi code setup` hardcoded the options and was designed to setup\nboth remote and local codes, `verdi code create` is a command group\nthat dynamically loads subcommands for each installed entry point that\nis a subclass of the `AbstractCode` data plugin.\n\nThe advantage of the new approach is that it is much clearer for both\nusers as well as developers. The code and user interface for the code\nsetup command was getting quite complicated because a single command was\nbeing used for two different purposes. By decoupling them, the interface\nis much more intuitive and the code is much more easy to maintain.",
      "Remove use of legacy `Code`\n\nThe `Code` plugin is used a lot and so cannot just be removed. Instead,\nwe keep it for now and have it subclass the new `AbstractCode`. This\nway, the class already has the interface that it will have once the\ndeprecated interface is removed. This will allow users to start updating\ntheir use of it in a backwards-compatible manner.\n\nThe codebase is updated to replace the use of the deprecated interface\nwith that of the new interface. No new instances of `Code` are created,\nonly instances of the new `RemoteCode` and `LocalCode`.\n\nWhen the time comes to drop the legacy `Code` class, all that is needed\nis a data migration that will update existing instances to either an\ninstance of `InstalledCode` or `PortableCode` depending on the attributes\nthat the instance defines.\n\nThe legacy methods and properties of `Code` plugin are deprecated.",
      "Deprecate: `verdi code setup` and `CodeBuilder`\n\nThe CLI command is replaced with `verdi code create`. The `CodeBuilder`\nis no longer necessary because code instances can now easily be created\nthrough the constructor.",
      "Docs: add documentation on new `InstalledCode` and `PortableCode`\n\nIt also provides some historical context as to why it replaces the old\n`Code` and when that will be removed."
    ]
  },
  {
    "pr_number": 1961,
    "commits_list": [
      "28d6d3845c6b89cd054bd630dab17c5971b3b59a"
    ],
    "message_list": [
      "Implement the `get_options` method for `JobCalculation`\n\nWhat is currently defined as the `options` for a `JobCalculation`, are specific\nattributes that were also stored as node attributes through `set_` methods. These\nwere called setters and are all defined explicitly, along with the corresponding\ngetters. With the introduction of the `Process` paradigm, these could no longer be\ncalled manually by the user on the calculation node, and so the `options` dictionary\nwas invented, to allow the user to specify these attributes.\n\nHowever, after a calculation was completed, there was no easy way to retrieve this\ndictionary with the collection of these attributes. Here we implement this method\nthe `get_options` method which by default will only return those attributes that\nwere explicitly set, but which can be overridden to provide the default values for\nthose that were not explicitly set.\n\nThe set of available options, along with their valid types, whether they are required\ndocstring and more, are now defined as a dictionary member of the `AbstractJobCalculation`\nclass. The `JobProcess` now uses this to dynamically build up the `options` port namespace\nin its inputs specification, just like the rest of the input ports are build dynamically\nbased on the use methods."
    ]
  },
  {
    "pr_number": 1929,
    "commits_list": [
      "c7ed481199f8b91cd91552973e7f75b7e3909262",
      "597430fa358d95396a33d5dee59879f12050e72f"
    ],
    "message_list": [
      "Added minimum scheduler poll interval\n\nNow the computer has a property that can be set to regulate the interval\nwith which the scheduler command to get jobs can be called.  This acts\nas a minimum and may in practice be longer if the transport minimum open\ninterval is longer.\n\nWith these changes all jobs in a single runner on the same computer also\nhave their requests to get the jobs list batched and therefore there\nshould be a significant decrease in the number of calls asking the\nscheduler to list the currently running jobs.",
      "Added functionality to query for scheduler jobs by id\n\nBy default the `JobsList` will get the status of running jobs by requesting\nthe status for the user, associated with the auth info, however, some\nschedulers do not support this functionality. Instead, one has to query for\na list of job ids. In this case the `JobsList` will simply get the job id list\nfrom the internal mapping it keeps."
    ]
  },
  {
    "pr_number": 5740,
    "commits_list": [
      "803d16a48941f557a7d558bb6740dc07b26c0715",
      "fc05e03c2cac67477fc4a0c6e86e440e87f56f92",
      "1440a60d2003b5df085a9d2b8f3db3f95a05823e",
      "7b78f96b42898b77b1db609d29db7c5ebab5a3d6",
      "19bd3b3576c4b09b4a3d3f3e1132115395fba567",
      "86b97ac1271930d5d542011642df96f531021137",
      "43c2ba1ba49f2729af6bdea5a336d77812df392a",
      "c0cd3c6d0f4e87af34d2518122564ea7c7ac1c19"
    ],
    "message_list": [
      "\ud83d\udc1b FIX: Import archive into large DB\n\nAs detailed in https://www.sqlite.org/limits.html, SQLITE_MAX_VARIABLE_NUMBER puts a limit on how many variables can be used in a single SQL query.\nThis size can be easily reached, if filtering by nodes in a large database.\nTherefore, this commit changes the filtering of UUIDs to be on the client side,\nthen batches queries for the full nodes by a fixed number.",
      "Merge branch 'main' into fix-archive-import",
      "Update aiida/tools/archive/imports.py\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "Merge branch 'main' into fix-archive-import",
      "qparams to query_params",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci",
      "Update imports.py",
      "Update imports.py"
    ]
  },
  {
    "pr_number": 3456,
    "commits_list": [
      "62448681d93fb5f84e2a4a745d79fed29e828218"
    ],
    "message_list": [
      "Add type checks to all plugin factories\n\nThe various entry point factories, based on the base class\n`aiida.plugins.factories.BaseFactory` would simply return the resource\nthat the given entry point corresponds to if it could be loaded. There\nwere no additional checks if the type of the loaded entry point\ncorresponds to what is expected for that particular entry point group.\nIt was possible, for example, to add `CalcJob` implementations and\ncalculation functions to the `aiida.workflows` entry point group, as well\nas `WorkChain` and work functions to `aiida.calculations`. The respective\nfactories would load these entry points without complaining. However, to\nkeep consistency it is better to raise if an entry point is loaded from\nthe incorrect entry point group.\n\nThe functionality to check that all registered plugins can be loaded\nthrough their respective factories has been abstracted to a utility in\n`aiida.plugins.entry_point.validate_registered_entry_points`. It is also\nexposed on the command line through `verdi devel validate-plugins`."
    ]
  },
  {
    "pr_number": 4848,
    "commits_list": [
      "daf0ae763e68bcad7f16d5adf4338aeafc0389e4",
      "cc3fe75b8f2b24365fc3958326eadcaffef7a424",
      "bd34362f1f70d8a44a727113b31d2a029df4774d",
      "7e49cf0f4d9c4f0319a02574911c8c1fe79e73e8",
      "6f65535ccaf187e53312ba294a5ecefb1bbc19f9",
      "520311a7708bfd3630ecb08dd0cac6cadc5ce0a6",
      "798775dc459776f8710342cfe63400ea6ae720e8"
    ],
    "message_list": [
      "removed the deprecated imp with importlib",
      "modified approach for loading a package",
      "Update aiida/restapi/translator/nodes/node.py\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/restapi/translator/nodes/node.py\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/restapi/translator/nodes/node.py\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>",
      "Merge branch 'develop' into various_deprecation_warnings",
      "Merge branch 'develop' into various_deprecation_warnings"
    ]
  },
  {
    "pr_number": 3763,
    "commits_list": [
      "b70c47c9aee9dc50927bb75fcdc66162eaec78ec",
      "2d10d2b586cbf76e2a8999a16b9f35a8cec3b6c2",
      "581ccc751f32b9e621a3a628db61aaf57cbd0f87",
      "1ff600e6bd3ad5098dc57e0742ff2b5a78261ebc",
      "575d7323ce609e94055dccf3648bf2b0adea712a",
      "94e991574014b2b6944f1f8ecf2789db6ad1a4ae"
    ],
    "message_list": [
      "set password for postgresql database",
      "Merge branch 'develop' into issue_3762_postgresql_ci",
      "add verbosity for failing tests",
      "Merge branch 'issue_3762_postgresql_ci' of github.com:ltalirz/aiida_core into issue_3762_postgresql_ci",
      "put back local postgres server executables\n\nthey are necessary for creating temporary postgres clusters",
      "go back to running all tests"
    ]
  },
  {
    "pr_number": 4886,
    "commits_list": [
      "5704b30298d03200c8fe665e3be15adfb55d63c2",
      "36700c5e6c8a7f3dc08ee8ca52e76a3856a958ac",
      "88505569fdfbb403c7c72d5296e26e8b195ebec9",
      "90d5e80be92184b6520b026a64994d4ed30df17b"
    ],
    "message_list": [
      "Docs: Fix typo's in \"Internal Architecture - Repositories\" section",
      "Restructure internals repository section",
      "Fix ref to requirements",
      "Merge branch 'develop' into mbercx-patch-1"
    ]
  },
  {
    "pr_number": 4961,
    "commits_list": [
      "2c380a1d13e16eb01068a77f8d5da084ced88f9a"
    ],
    "message_list": [
      "BaseRestartWorkChain deal with namespaced outputs\n\n`BaseRestartWorkChain` does not return `output_namespaces` of\n`_process_class` as describe in #4623. It happend because in the\n`results` method of `BaseRestartWorkChain`, only output keys go through by\n`node.get_outgoing` are checked and returned by the parent WorkChain.\n\nThe exposed_outputs adapt to output the whole nested namespace by\n`exposed_outputs`. The `out_many` method is not used here to make a\npost-check for ports to remain the original exit code check and report."
    ]
  },
  {
    "pr_number": 3581,
    "commits_list": [
      "71479b869fed83daef9b551eb85c852e13fa8967",
      "659d99e5a5c90a1d5f578ebb0399f67f20000f76",
      "b1f911317eff0ef709b1c73daef775ba2e22d33c",
      "b14aef7446f2002163fcc6288ff7c36dc191b17b"
    ],
    "message_list": [
      "Improve the backup mechanism of the configuration file\n\nThe `Config` class was creating a new backup each time `store` was\ncalled, which also happens if nothing really changed to the configuration\ncontent. With the recently introduced change that backup files are now\nmade unique through a timestamp, this led to a lot of backups being\ncreated that were essentially clones.\n\nTo improve this, the `Config.store` method now only writes the file to\ndisk if the contents in memory have changed. This is done by comparing\nthe checksum of the on disk file and that memory contents as written to\na temporary file. If the checksums differ a backup is created of the\nexisting file and the new contents written to the temporary file are\ncopied to the actual configuration file location. This latter new\napproach also guarantees that the backup is always created before\noverwriting the original one.",
      "Address PR comments by @ltalirz",
      "Move responsibility of store-when-new to `Config.from_file`",
      "Directly write config file in `Config.from_file` if it doesn't exist"
    ]
  },
  {
    "pr_number": 2520,
    "commits_list": [
      "52d906fa0c78a8009f2416295f5068f9a0c5fe3d",
      "626f6f0c80d978abbc558d2c234d33359f150f91",
      "689a5b6026cd5f3296bd50723fc1e5eca8edeb67",
      "1dd4d11f8b9adf58084f19460623fccfbb137645",
      "0080d48c60b70cfd71787f15d8fe5b43d9393fab",
      "74e32244b6e3df881182bc827cb8f15dfc24b247",
      "4ccc95bd1441b139fe91767dba5234472f2c046f",
      "f65933a50f35ddfdc7c42f4e61da7f52f2c73ad5",
      "737c8985b62da27cab03149407f9ee9e2e11eabd",
      "95e80cd0d79daaa2afbde8f64a37920b041710e7",
      "a8971e89662f9fb2f2b683005c14398163be12ce"
    ],
    "message_list": [
      "update python dependencies",
      "fix typo",
      "fix for click 7.0",
      "want tornado <5",
      "more adaptations to click 7",
      "fixes for ase 3.17",
      "new package release in the meanwhile...",
      "update pre-commit dependencies",
      "downgrade prospector\n\nprospector 1.1.6.2 complains about the not matching pylint-django\ndependencies, while letting it install the pylint-django dependencies\nmakes pylint crash",
      "Merge branch 'provenance_redesign' into update-dependencies",
      "Merge branch 'provenance_redesign' into update-dependencies"
    ]
  },
  {
    "pr_number": 3362,
    "commits_list": [
      "37e1bcdde578140b52caaee2964b77185ad9f8ff"
    ],
    "message_list": [
      "Enable auto-completion for the `ComputerParamType`\n\nThis will allow CLI commands that expose an option or argument that\ntakes a `Computer` to auto-complete partial identifiers. Note that since\nthis will require loading the database environment, the completion\nitself may take a second."
    ]
  },
  {
    "pr_number": 3742,
    "commits_list": [
      "546fe7045fa02428a0101b5e0f0aa30c1010e45a",
      "4c37ddd3a82d451c6a18a0004eeb23d9b95faf49",
      "134043c8d76b0379a30ffca1409469b0e98addd3",
      "7dac77baad6a89b83870e6baa8059342323a5dea",
      "4801e5e0d1026d3b6d3e05d8c472d4c8f6e585dd"
    ],
    "message_list": [
      "Fixed documentation of 'getfile' method for RemoteData",
      "Update aiida/orm/nodes/data/remote.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Proposed changes",
      "move back to returning nothing",
      "Merge branch 'develop' into develop"
    ]
  },
  {
    "pr_number": 4114,
    "commits_list": [
      "2f63a589e0c852a669bf1b4ad1ed974e837b48b5",
      "44fe9afbb2ec16471c77103b69492f1e2c4d8ffa",
      "8bd8275e1e071f75a177e82a03bc39317c821648",
      "ccce438734f4a31cb1030f0ee34f14f2f34c7936",
      "6668e6d25fc378a3f08c621010b042afbbd7561d",
      "9bb519fad94a83e52dd5c466cfc4da066df6685b",
      "5d1aee933c9fb7af09e2536aa5c6ebe2cb1afd38",
      "6136673cd9297d46b18695e7c8a3b7fc2a5ea5a9",
      "5e21dcd07968d7c768f01f236443ee0637831083",
      "932ad03018c996be1958e353ae57139ec2d737e8"
    ],
    "message_list": [
      "Add how-to run external codes.",
      "Add corrections from PR review",
      "Apply suggestions from code review\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Added output to bash command",
      "Apply suggestions from code review\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Removed '_node' suffix",
      "Quick-apply suggestions from code review\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>",
      "Corrections from PR review.",
      "Renamed section.",
      "Merge branch 'docs-revamp' into docs-revamp-3987"
    ]
  },
  {
    "pr_number": 4781,
    "commits_list": [
      "f322c50f90ae19c5bc20c734cacd17838d071e18",
      "395097cd8ba96b40a891dcab26dd52ee1fc36577",
      "48ec4eca2be1d5a1e79bed47da6a0b7482e38f70",
      "c9753360f5db6ca3265ee48d38e2c96d57c2ce49",
      "ccca6f612fc7dffde9c271e032052cab61e464c8"
    ],
    "message_list": [
      "add get_or_create_default_user",
      "disable exception",
      "remove user cache from test class",
      "put back backend entities",
      "delete test backend classes"
    ]
  },
  {
    "pr_number": 2113,
    "commits_list": [
      "b477569853245b48a16cc571b577068508b381cb",
      "76b15889a5416e030fd943f3b22f2ca40522c7b3",
      "095c0d31d4f5628665414a8680ffdc4618c1a1b1"
    ],
    "message_list": [
      "Upgrade SqlAlchemy dependency requirement to 1.2.12\n\nTo make the code base compatible with the new version, expressions produced\nby the querybuilder had to include casts from JSONB. The cast that needs to\nhappen depends on the type of the value, but the order of the type check\nand the cast cannot be guaranteed in the old syntax. Therefore we\nreplace the attribute queries from chains to cases, such that the type\nis checked before it is casted.",
      "Wrap `flag_modified` to unwrap `ModelWrapper` instances\n\nThe new backend wraps the database model instances in the `ModelWrapper`\nutility class, which will take care of timely flushing and updating the\nmodel instance upon changes. Across the code, model instances are passed\nto the `flag_modified` functionn of SqlAlchemy that mark a certain key\non an instance as modified. However, sometimes instead of the actual\nmodel instance being passed, the wrapped version is passed. This used to\nbe ignored for `sqlalchemy==1.0.19` but in `sqlalchemy==1.2.12` to which\nwe have updated, an `InvalidRequestError` is raised.\n\nTo overcome this, we define our own `flag_modified` function that wraps\nthe one of SqlAlchemy and will unwrap any wrapped instances that are\npassed to it.",
      "Only let the `ModelWrapper` call `flag_modified`\n\nThe `_set_metadata` method of `SqlaComputer` was calling `flag_modified`\nmanually after having set the attribute, however, since the model is\nwrapped in the `ModelWrapper`, it's `__setattr__` will take care of\ncalling `flag_modified` when it flushes upon an attribute being updated\nfor a stored entity. Calling `flag_modified` on the same key twice will\nexcept in SqlAlchemy 1.2.12 because the \"dirty\" attribute has already\nbeen removed and moved into the database model instance."
    ]
  },
  {
    "pr_number": 2691,
    "commits_list": [
      "25d28a78f6ec740155da7e2e8ab500ab54f21038"
    ],
    "message_list": [
      "Set default for `CalcJob.metadata.options.withmpi` to `False`\n\nRunning codes with MPI by accident that don't support it is worse than\naccidentally not using MPI for those that can. The safer default for\n`aiida-core` therefore is to set it to `False`. Plugins can override this\non a per calculation basis when defining the process specification."
    ]
  },
  {
    "pr_number": 4954,
    "commits_list": [
      "728cdbe76c6d78f7feaa422cb2a2486f8d524cb3",
      "b5ff789db93f0ed92d535d6e039e46580e4b5c96"
    ],
    "message_list": [
      "Disregard old test repository quirks for migration\n\nAn old test created a `.gitignore` file in the Node repository folder\n`path`, while also creating files under a `raw_input` folder.\nThis adds a logic test for this specific edge-case, migrating the\n`raw_input` folder and ignoring the `path` folder for the specific Node.\n\nFixes #4910.",
      "Extend also test for SQLAlchemy backend"
    ]
  },
  {
    "pr_number": 790,
    "commits_list": [
      "e22aa51ac2ff40d1dbd184c4c9079f51ddf2871b",
      "98063079a26033853e37cac6dbee3f971c20782e",
      "63e7c5956423249116023a40667370dea8c54702",
      "3c6690d01e7086474d6bddf974d5d6f6dde70e0c",
      "08a2c8c9d9b3eefc56745880a4d7b6f2324f3099",
      "9a0976299dc839b2a03dba377a918280d170b449",
      "825f89078599d5ca58f2d6387677d541cc1d336d"
    ],
    "message_list": [
      "run celery directly instead of via supervisord",
      "setup: drop supervisor config creation",
      "remove leftovers from supervisord",
      "simple log file rotation for daemon on start",
      "travis-data: adapt daemon test to logfile name change",
      "cmds/daemon: factor out initialization code to __init__ function",
      "Merge branch 'develop' into supervisor-removal"
    ]
  },
  {
    "pr_number": 3326,
    "commits_list": [
      "5381ecd0af7d20dc18764552b8f00abb4abdaa74",
      "dc5cc1202fc806b0b3b70d03857b8ddbbde9a59e",
      "df44053cfc3dd822b568520f0cbe51fa81c31e9a"
    ],
    "message_list": [
      "Fixing the drop in test coverage reported\n\nFixes #3324\n\nThis should set properly the coverage configuration\nfile also in the tests run via pytests, avoiding that\nsome files are reported with a relative path and some with\nan absolute path (and, as a consequence, with a low coverage\nboth for the relative and for the absolute path, resulting in\na low overall coverage).",
      "Merge branch 'develop' into fix_3324_coverage_drop",
      "Merge branch 'develop' into fix_3324_coverage_drop"
    ]
  },
  {
    "pr_number": 5715,
    "commits_list": [
      "1e1fdd9ba6eac531cdf24078c4de6f5d74aa35b0",
      "61f257a70e185e0542d079dab649b1d0ca74720c"
    ],
    "message_list": [
      "Engine: Do not let `DuplicateSubcriberError` except a `Process`\n\nIt is possible that when a daemon worker tries to continue a process,\nthat a ``kiwipy.DuplicateSubscriberError`` is raised. This happens when\nthe current worker has already subscribed itself with this process\nidentifier. The call to ``_continue`` will call ``Process.init`` which\nwill add RPC and broadcast subscribers. ``kiwipy`` and ``aiormq`` further\ndown keep track of processes that are already subscribed and if\nsubscribed again, a ``DuplicateSubscriberIdentifier`` is raised.\n\nPossible reasons for the worker receiving a process task that it already\nhas, include:\n\n  1. The user mistakenly recreates the corresponding task, thinking the\n     original task was lost.\n  2. RabbitMQ requeues the task because the daemon worker lost its\n     connection or did not respond to the heartbeat in time, and the\n     task is sent to the same worker once it regains connection.\n\nHere we assume that the existence of another subscriber indicates that\nthe process is still being run by this worker. We thus ignore the request\nto have the worker take it on again and acknowledge the current task.\n\nIf our assumption was wrong and the original task was no longer being\nworked on, the user can resubmit the task once the list of subscribers of\nthe process has been cleared. Note: In the second case we are deleting\nthe *original* task, and once the worker finishes running the process\nthere won't be a task in RabbitMQ to acknowledge anymore.\nThis, however, is silently ignored.\n\nNote: the exception is raised by ``kiwipy`` based on an internal cache it\nand ``aiormq`` keep of the current subscribers. This means that this will\nonly occur when the tasks is resent to the *same* daemon worker. If\nanother worker were to receive it, no exception would be raised as the\ncheck is client and not server based.",
      "Fixtures: Make `daemon_client` function scoped\n\nThis is more expensive than the original idea of having it session\nscoped since now the daemon is stopped after each function usage,\nhowever, not stopping the daemon can cause tests to fail if previous\ntests leave it in an inconsistent state."
    ]
  },
  {
    "pr_number": 4924,
    "commits_list": [
      "e6a944839882439db92f794bea54634273783c46"
    ],
    "message_list": [
      "\ud83d\udc4c IMPROVE: add `DaemonClient.lock()`"
    ]
  },
  {
    "pr_number": 2553,
    "commits_list": [
      "f45f8254b35914225409ee371a007f1173eed22b"
    ],
    "message_list": [
      "update authors & bump version"
    ]
  },
  {
    "pr_number": 4083,
    "commits_list": [
      "fa608e933fb31d32a88c95ab8db9e89631a92d02"
    ],
    "message_list": [
      "Fix bug in `Code.get_full_text_info`\n\nThis method was not tested and so it went unnoticed that it was still\nusing the `_get_folder_pathsubfolder` repository method that has been\nremoved in `aiida-core==1.0.0`. Direct access to the folder underlying\nthe node repository is no longer allowed and instead the API should be\nused to inspect the available objects, in this case `list_objects`."
    ]
  },
  {
    "pr_number": 5135,
    "commits_list": [
      "f317f65cf6761713afd89b009402aaa1e5b508ef"
    ],
    "message_list": [
      "WIP profile locking mechanism"
    ]
  },
  {
    "pr_number": 2723,
    "commits_list": [
      "6c4393fc68ffcc2facff9fd7069a23d44a7136d7",
      "69403cd67024d8a3a7e86f2e6b5be9c67935afe2"
    ],
    "message_list": [
      "Implement `ConfigFileOption` reusable command line option type\n\nThis option type allows to add an option to command line interfaces that\nwill allow all its other options to be set through a yaml configuration\nfile that is passed to the config option. This functionality requires\nthe added click plugin dependency `click-config-file`. The new functionality\nis implemented for `verdi quicksetup` and `verdi setup`.",
      "Add config file option to `verdi computer/code setup`\n\nThe previous commit already implemented the basic required functionality\nto read all command line parameters from a configuration file and added\nit to the `verdi setup` and `verdi quicksetup` commands. With this\ncommit it is also enabled for the setup of `verdi code/computer`."
    ]
  },
  {
    "pr_number": 5531,
    "commits_list": [
      "90993246d67fbfe6d44ef55f0b91c0887fa9cec6",
      "5f78ee901a22756fc4c296a508002d6ac7ed5311",
      "0ad46c232e25c53b5a7b63b74f83478142e8bc7b",
      "8a47feb8c1dcf3e3f4c7a3653f9fa3049e9e097a"
    ],
    "message_list": [
      "fix: raise when detecting multiple entry points\n\nDue to a slight misunderstanding of the new importlib.metadata API,\nAiiDA did no longer raise when detecting multiple entries for a given\nentry point (pair of group + name), and instead simply loaded the first\none.\n\nExample of previous (buggy) behavior:\n```\n\nIn [8]: eps.select(group='aiida.workflows',name='mcscf.abcChain')\nOut[8]:\n[EntryPoint(name='mcscf.abcChain', value='aiida_mcscf.workflows:abcChain', group='aiida.workflows'),\n EntryPoint(name='mcscf.abcChain', value='aiida_mcscf.workflows:AbcDefChain', group='aiida.workflows')]\n\nIn [9]: result=eps.select(group='aiida.workflows',name='mcscf.abcChain')\n\nIn [10]: result.names\nOut[10]: {'mcscf.abcChain'}\n\nIn [11]: len(result)\nOut[11]: 2\n\nIn [12]: len(result.names)\nOut[12]: 1\n\nIn [13]: result['mcscf.abcChain']\nOut[13]: EntryPoint(name='mcscf.abcChain', value='aiida_mcscf.workflows:abcChain', group='aiida.workflows')\n```",
      "fix: accept multiple identical entry points\n\nWhen encountering multiple entrypoints of the same name in a given\ngroup, still proceed if their values (where they are pointing) are\nidentical.\n\nThis makes it possible to e.g. install a version of AiiDA locally, over\na system-level installation of AiiDA that cannot be uninstalled (as long\nas those versions do not change the value of an entry point).",
      "Add a test for `aiida.plugins.entry_point.get_entry_point`",
      "Update aiida/plugins/entry_point.py\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>"
    ]
  },
  {
    "pr_number": 5516,
    "commits_list": [
      "dc16d1120c4e42d6bbec3d5a0f55ba287621c374"
    ],
    "message_list": [
      "Deprecate `Profile.repository_path`\n\nThe `Profile.repository_path` was historically used to give the absolute\nfilepath to the file repository on the local file system. However, this\nis a property of the storage backend and not all backends will have this.\nSo instead of being generic to all profiles, this should be a storage\nspecific setting.\n\nThe property is deprecated and the functionality is added to a new function\n`aiida.storage.psql_dos.backend.get_filepath_container`. We considered\nadding it as a property on the `PsqlDosBackend` class, which makes the\nmost sense semantically, but that made the code more complex than ideal.\nThe reason for this is because the property needs to be accessed both\nfrom the backend as well as the associated migrator. However, the migrator\ncannot have a reference to the backend since the migrator is used to\nvalidate the backend class in its constructor, which would lead to a\ncircular dependence. The alternative of implementing the property on the\nmigrator and exposing it on the backend was possible, but also slightly\nconvoluted. The root of this problem is the limitation in the current\ndesign that the migrator cannot have a reference to the storage backend\nto which it belongs, but this should be addressed in another commit.\n\nThere is a also a slight difference in the implementation of the new\nfunction. It returns the path of the disk object store container, which\nis a subfolder of the original repository path. Old invocations of\n`profile.repository_path` should therefore be replaced with\n`get_filepath_container(profile).parent`."
    ]
  },
  {
    "pr_number": 1888,
    "commits_list": [
      "d92c573b5ad13189facc373c07b3e23cba747baa",
      "22747b6e6f399df4158639534253d8716dd8b6df",
      "b8e32dbabcf48c6d338a02b9940363841019eda1",
      "80aee2603c8ad248c9a3e038710fce7095eea5bb",
      "7ae0075fcf6c603d39a9ae37bd860bdd6dc887e2",
      "88339e243ff705805f4bd7ce1c4c94e63b46dcd1",
      "3f70a400168adbc0a6a381dc8b967637e90e2ad2",
      "986db057ea8c7d4ef28d24e63a15632af9071469",
      "bfcc3da60a903c655d548ab4972d8411f93bf8d7",
      "a9f72b2e11835e9087b85b1916dee37c424f45cb",
      "88df43874fef9849c42ce7f7463184d123016451",
      "ec9573846151cb5be7a47421c150a98624e464ac",
      "173bf06a8824d3ff9cc24b03748d0c22fbcb65ee",
      "55b06196ffd94d4d636c4b773f5dfe052528b5a9",
      "6ad0fd841906edbb654ab3e0f8af1589b8405e63",
      "1bece9b00e7e84896aab74d76ead7a0064281e6f",
      "382bde7e166027a1cb763c5ab72096ca376d70c5",
      "10c5946bb123f8110fa92ea444c1b7b0e51087b9",
      "6b2843ffe2bb2e6dad2fbdd745e7a2a3422b5773",
      "9b6e77131a2d50739a1d8ed01d82506bdec24a96",
      "a925bdbed1b966c4c14ab1fc29155b2830ee274b",
      "e0b685a41e6b2a21bba7673c42429aa40eb44d93",
      "2ae9b2c4fa0375f4ac636421f82558bda09cb9e3",
      "a133ac2d716ca4024f8fe4ed69fc9458ba6016ca",
      "13770275486d1b6b2b0e1e2e0561e59ce287f928",
      "34982b71d9f6a39d0f2e4ef0f177da200aadb4df",
      "26a058886d884e645aa8602ddf0e3cb0104b32e3",
      "5123c48339151cf938c6434e50ca185f4ba95125",
      "c1d2506ba3c184595566a4de11fb25f47dbb39a9",
      "86d4d65e79a26b8f0f0c00d44621b42ad243beb7"
    ],
    "message_list": [
      "Added posibility to submit account and quality of service infomration to the SLURM, PBS and SGE scheduler",
      "Merge branch 'develop' into develop",
      "Merge branch 'develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge branch 'develop' of github.com:espenfl/aiida_core into develop",
      "Merged with upstream develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Primitive suggestions for handling escape and csi characters",
      "Merge branch 'develop' into escape_handler_suggestions",
      "Merge branch 'develop' into develop",
      "Removed check for qos and account for squeue",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge branch 'develop' into escape_handler_suggestions",
      "Merge branch 'develop' into escape_handler_suggestions",
      "Merge branch 'develop' into develop",
      "Merged with develop",
      "Merge branch 'develop' of github.com:espenfl/aiida_core into develop",
      "Merge branch 'develop' into escape_handler_suggestions",
      "Updated with bytes flag",
      "Replaced strip with replace",
      "Merge branch 'develop' into escape_handler_suggestions",
      "Updated to comply with transfer to Python 3",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge branch 'escape_handler_suggestions' of github.com:espenfl/aiida_core into escape_handler_suggestions",
      "Merge remote-tracking branch 'upstream/develop' into escape_handler_suggestions",
      "Updated to comply with new yapf version etc.",
      "Merge branch 'escape_handler_suggestions' of github.com:espenfl/aiida_core into escape_handler_suggestions"
    ]
  },
  {
    "pr_number": 3617,
    "commits_list": [
      "67613f804803ee3ea13f9bc4b50050918ade4697",
      "8a44651c14cf5d9badb0180d2ce497593005e8af",
      "00a77f002a841acff1199fb3e2f64aee1c2b0842"
    ],
    "message_list": [
      "Add test for raising an error when a broken workchain is documented.\n\nAdds a test of the sphinx extension where a workchain that raises\nan error in its 'define' method is documented. Due to plumpy PR#136,\nthe error should propagate into the sphinx build.\nOn the side of the AiiDA sphinx extension, we catch the error and\nre-raise with a more helpful error message.",
      "Merge branch 'develop' into fix_swallowed_sphinx_errors",
      "Merge branch 'develop' into fix_swallowed_sphinx_errors"
    ]
  },
  {
    "pr_number": 4144,
    "commits_list": [
      "b7bacd50a3443206ea0c0c9a26039ce22da94fc2"
    ],
    "message_list": [
      "`QueryBuilder`: fix type string filter generation for `Group` subclasses\n\nThe `aiida.orm.querybuilder.get_group_type_filter` function that is\nresponsible for generating the correct filters for the `type_string`\nwhen a `Group` or subclass of it is appended, contained a bug. To\ngenerate the correct type string, it should strip the `group.` prefix\nfrom the provided ORM classifier. This was incorrectly done with the\nstring method `lstrip`, which does not strip an entire prefix as is, but\nwill strip any characters that are matched starting from the left. This\nwould cause extra characters to be erroneously stripped if the rest of\nthe type string started with any letters in the sequence `group.`. For\nexample `group.pseudo.family` would be stripped to `seudo.family`. The\nsolution is to use the first N characters where N is the length of the\nprefix."
    ]
  },
  {
    "pr_number": 3954,
    "commits_list": [
      "9ead2b1162d7ec2d034f451ade22d482b3f58e5b",
      "3357d5fc6f3c6b93631f1e6f71d02f1b92084724",
      "9da8f68553daf1d78f5bf4db5af1542e42df705d",
      "e01e66fd5236589359612a1313ce03d9be7bb7d2",
      "11bf5c85611be0a51eeeea34bd92d177e23daa3a"
    ],
    "message_list": [
      "Parse complex chemical formulas strings\n\nThe \"parse_formula\" function can now parse more complex formulas,\nincluding elements with partial occupancies and groups of elements enclosed\nin parentheses. Spaces separators are no more needed.\nElements are counted and a dictionary is returned.\nThis can be useful to parse the formulas included in cif files, or those\ngenerated by pymatgen.\n\ne.g.  C[NH2]3NO3  -->  {'C': 1, 'N': 4, 'H': 6, 'O': 3}",
      "Minor beauty edits to cif.parse_formula\n\nApply suggestions by @CasperWA",
      "Add cif.parse_formula test\n\nTest the chemical formula parser against many cumbersome cases.",
      "Add custom formula tags to `CifData.get_formulae`\n\nIt is possible to specify 'custom_tags' that will try to be read by the\n'CifData.get_formulae' method.",
      "Merge branch 'develop' into parse_formula"
    ]
  },
  {
    "pr_number": 4579,
    "commits_list": [
      "acd7edc55f7f239cc710e0284de1987648d89230",
      "c1830da5396031339a844f7fe8790d9b789bdc36"
    ],
    "message_list": [
      "CI: revert apt source list removal\n\nThis work around was added some time ago because this source for the\n`apt` package manager was causing the install of system dependencies to\nfail.",
      "CI: Add workflow to run tests against various RabbitMQ versions\n\nThe main test workflow runs against a single version of RabbitMQ but\nexperience has shown that the code can break for different versions of\nthe RabbitMQ server. Here we add a new CI workflow that runs various\nunit tests through pytest that simulate the typical interaction with the\nRabbitMQ server in normal AiiDA operation. The difference is that these\nare tested against the currently available versions of RabbitMQ.\n\nThe current setup, still only tests part of the functionality that AiiDA\nuses, for example, the default credentials and virtual host are used.\nConnections over TLS are also not tested. These options would require\nthe RabbitMQ service that is running in a docker container to be\nconfigured differently. It is not clear how these various options can be\nparametrized in concert with the actual unit tests."
    ]
  },
  {
    "pr_number": 4410,
    "commits_list": [
      "456115a2dfd6daa0810d8defbbaf694df9ac8b50"
    ],
    "message_list": [
      "Dependencies: update requirement `pytest~=6.0` and use `pyproject.toml`\n\nStarting from v6.0, `pytest` supports using the `pyproject.toml` instead\nof a `pytest.ini` to define its configuration. Given that this is\nquickly becoming the Python packaging standard and allows us to reduce\nthe number of configuration files in the top level of the repository, we\nincrease the version requirement of `pytest`.\n\nNote that we also require `pytest-rerunfailures>=9.1.1` because lower\nversions are broken in combination with `pytest==6.1`. See the following:\n\n   https://github.com/pytest-dev/pytest-rerunfailures/issues/128\n\nfor details."
    ]
  },
  {
    "pr_number": 4478,
    "commits_list": [
      "8d772d0b85ea1035a6faba4dde445e2ed7b65890",
      "3137a6e6d738515f1ef27310016217139ffcf303",
      "c8815fc2bbb50ad16f02bf32ee4e66f66a3ffea6",
      "f413f94acdaa252b9c7f4129381afeb52a830805",
      "f10d435d5a3378323e9f7a86462be16baf111e91"
    ],
    "message_list": [
      "Docs: Serve a custom 404 page",
      "Address review comments",
      "re-add dependency",
      "fix pre-commit",
      "Merge branch 'develop' into docs/404"
    ]
  },
  {
    "pr_number": 2447,
    "commits_list": [
      "78407a419279cf1efef2766826321d8c65d5c83f",
      "f4a575777fe606a44c4527504d425325c9d74ca7"
    ],
    "message_list": [
      "Fix the wrong migration for the `DbLog` table\n\nIn commit `c0fba2e38557fe1ea535d1dc24076cba99212616`, a migration was\nintroduced for the `DbLog` table in order to support the export and\nimport of log entries. However, this migration was flawed and would\nlose valuable information.\n\nThe old `DbLog` was designed to support adding log messages to various\nentities. Because these entities were not restricted to just nodes, and\ntherefore could live in different database tables, a foreign key could\nnot be used for the relationship, but instead an `objpk` and `objname`\ncolumn were used to allow multiple table references. However, for the\nimport to know whether an exported log record is already present in the\ndatabase and therefore should not be imported again, the object pk could\nnot be used as these do remain the same between databases. To this end\nthe migration added a UUID column to the `DbLog` table and dropped the\n`objpk` column. However, the data was not migrated before doing so and\nso the connection between logs and their entities was lost.\n\nAfter the faulty commit, we realized that the reason for not using the\nforeign key was that logs needed to be able to be associated with both\nnodes as well as legacy workflows, but since support for the latter had\nalready been dropped this was no longer necessary. Given that now only\nnodes can have associated logs, we can revert the `objpk` to be a proper\nforeign key and the `objname` can be dropped. Since the logs for legacy\nworkflows (and any other unexpected entities) will be dropped from the\ndatabase, they should be exported to a file for archival purposed.\n\nIn summary, the migrations perform the following logical actions:\n\n * Export existing logs for legacy worklows and other entities to file\n * Delete records from legacy workflows and other entities from `dblog`\n * Create a foreign key `dbnode_id`\n * Migrate data from `objpk` to `dbnode_id`\n * Create a new column `uuid` for `DbLog` that is unique\n * Delete the `objpk` and `ojbname` columns from `DbLog`\n * Delete `objpk` and `objname` from the `metadata` of `DbLog` records\n\nNote that the addition of the unique `uuid` column had to be done in\nthree steps. The problem is that the default value for the UUID is\ngenerated by a python function which cannot therefore be done on a\ndatabase level but the values have to be set after the creation of the\ncolumn. However, as soon as the column is created the uniqueness\nconstraint is violated since it will be empty for all the existing\nrecords. For that reason the migration is split in three steps. In the\nfirst, the column is created and set as nullable. The second step will\npopulate the records using the python function to generate the UUIDs and\nfinally, the third migration will alter the column to be unique.\n\nNote that for Django this migration could be written in a single file,\nbut for SqlAlchemy it had to be broken into several migrations. This is\nbecause of Alembic, the framework used to perform SqlAlchemy migrations,\nwhich had trouble with setting the UUIDs after creation of the column in\na single transaction. To circumvent this problem, the migration was split\ninto three separate migrations and Alembic is now instructed to use a\nsingle transaction per revision, mirroring the migration behavior of\nDjango.",
      "Add support for the export/import of `Log` entities\n\nTo make the export of `Log` entities possible, the `QueryBuilder` had to\nbe extended to support retrieving log records for a given `Node`. The\nfollowing changes were applied:\n\n * Internal `log_model_class` has been removed and replaced by `orm.Log`\n * Added support to join node to log (`with_node`)\n * Added support to join log to node (`with_log`)\n\nThe `verdi export create` has a new flag to include or exclude the\nexport of logs for nodes that are to be exported, defaulting to include:\n\n `--include-logs/--exclude-logs`\n\nDocumentation has been updated to include new `QueryBuilder` join args\nin table and the `metadata.json` example has been updated in documentation\nto include correct Log info."
    ]
  },
  {
    "pr_number": 1655,
    "commits_list": [
      "69208b663d9825b59b7e4293270a415ec666a740",
      "08b5f0799e4f965ca276237378845d4aa5a103e9",
      "3eb365cfeb5a55c110ff131aae5accf6737a9719",
      "13499bbe23ba28af678a50ead6a9dd10eb1f12e0",
      "0da966452ef19e23c377f4f12988c921c6767a3f",
      "a60b9ca0ca09d01a254c43b09e98e0864ec51100",
      "a87dbc6d924e8b4db7a91c6d5acce70e43d569ce",
      "8588d8f1772dd3447861bb966e964bc4a767f224",
      "b6d6c14e2edf08587fa36e404e23a1613e714d8a",
      "d67294d116faac37473c6f4f7181edb579c15e61",
      "fb4a4ca872a24a98d601dea6d8146c25653c9f69",
      "e0486c7756daab079520d018389f49dbb12571e1",
      "f22ad75020b47fd3e09ace77524f933c80beb25a",
      "daaefdc0cb1edde182294a898fe07e40aaccd1e0",
      "bb56eef4652331319292d3ad4ecaab33fb5d4d65",
      "5e71ce067fb55dd22880512f7c1a642991bc5f70",
      "29e32cc36bcfddd4196ae963e315aa1383f0ba1c",
      "f96dc6fed00135e4968648a68613cb37a02f77e2",
      "7af31a45902ffa462197b8de9a666f7c96bb7c86",
      "516beec55f1f2971296df5b4101e86999df919a9",
      "deff862045566b16baca3eaeea40a2e2add91625",
      "5f2cf728bb19e593cd4d91a525c0ae2efcf26158",
      "5e0066b45870ef84196123aa109fe377c364a9d3"
    ],
    "message_list": [
      "migrate verdi code delete",
      "add hide, reveal + unit tests",
      "add get_code_label convenience function\n\nfix #1614\nCode.get_code_label is the same as Code.label, but you can get the\nfull code label (including computer name) by specifying 'full=True'",
      "Merge remote-tracking branch 'origin/verdi' into migrate_verdi_code",
      "migrate code rename, add code relabel\n\n * migrate verdi code rename to click\n * add verdi code relabel (verdi code rename deprecated)\n * add Code.relabel function\n * deprecate verdi code update",
      "harden tests",
      "migrate verdi code list to click",
      "Merge branch 'verdi' into migrate_verdi_code",
      "Merge branch 'verdi' into migrate_verdi_code",
      "add code.py to pre-commit\n\n + fix 'code show' and add test",
      "migrate old tests for code list",
      "Merge branch 'verdi' into migrate_verdi_code",
      "Merge branch 'verdi' into migrate_verdi_code",
      "use deprecated_command decorator\n\n + fix bug introduced in merge",
      "add immediate check for absolute path\n\nCheck for absolute path was only done at the very end.",
      "fix doc warning",
      "remove some commented lines\n\nleaving in a few comments for verdi code duplicate",
      "check Code._is_hidden() instead of HIDDEN_KEY",
      "remove protection from Code.hide and Code.reveal\n\nfix #1660\n\n * Code._hide => Code.hide\n * Code._reveal => Code.reveal\n * Code._is_hidden => Code.is_hidden",
      "fix leftover _hide",
      "replace Code.get_label() by Code.full_label\n\nreplacing function with parameter full by property.",
      "adjust help strings for local/remote codes\n\nfix #127\n\nAdjust help strings similar to suggestion in\nhttps://github.com/aiidateam/aiida_core/issues/127#issuecomment-352452878",
      "change order of prompts\n\nask for input plugin before asking for anything related to\nthe computer where the code is stored."
    ]
  },
  {
    "pr_number": 5209,
    "commits_list": [
      "f945b52104d29ed5881ab81c70f4f91b67b384d1",
      "6d1795e66d6051c318996b35d3541d57e689240e",
      "1a1363c1437ace43d1bd6612aeb86991d8e4e847"
    ],
    "message_list": [
      "`Transport`: store `machine` keyword under `hostname` attribute\n\nA `Transport` is created for a `Computer`, but to keep the transport\nclass independent of the `Computer` ORM class, it is not passed as an\nargument. This means however that, given a transport instance, it was\nnot possible to know for which `Computer` it was constructed, which is\nsometimes necessary for consistency checks.\n\nWhen the transport is constructed the hostname of the `Computer` is\npassed as the `machine` keyword argument, and normally the hostname\nshould identify the computer uniquely. Note that there is still the\nquestion of for which user the transport is configured, but that is\ncurrently not taken into account.",
      "`RemoteData`: allow passing transport to `_clean` and set extra\n\nAllowing to pass an already open `Transport` to the `_clean` method,\nprevents having to open the same transport multiple times when cleaning\nmany `RemoteData` nodes on the same computer. The downside is that it is\npossible for a user to pass a transport that is configured for a\ncomputer that is not the one of the `RemoteData`. A check is added to\nprevent this, but it only checks the hostname as it cannot check the\nuser for which the transport is configured.\n\nIn addition to this efficiency update, if the clean operation is\nsuccessful, an extra with the key `KEY_EXTRA_CLEANED` is set to `True`.\nThis is useful to identify `RemoteData` nodes that have been cleaned.\n\nCo-Authored-By: Sebastiaan Huber <mail@sphuber.net>",
      "`verdi calcjob cleanworkdir`: fix filter bug and add filter option\n\nThe command did not apply the `AND` clause when both the `-o` and the\n`-p` options were provided simultaneously and only the former would be\napplied. This is now corrected.\n\nOn top of that, a new option `-E/--exit-status` is added that allows to\nfilter on a specific exit status of the calculation jobs."
    ]
  },
  {
    "pr_number": 4074,
    "commits_list": [
      "3cfc03012643e7adb89aae2f44b8a640addbe0dd",
      "ecf579e98546b433f0e2919c59107977569d6d1d",
      "ef43dca8a53d474e692d794b24c20b6465584f5e",
      "ccf24876a20cfddd20c3a31b551dcb81265fb134"
    ],
    "message_list": [
      "Moving in the new documentation scaffolding structure the following sections:\n\nhow-to:tune performance (fixes #4002)\nhow-to:update installation (fixes #4003)\nhow-to:backup (fixes #4004)\n\nI'm doing these in a single commit since they are closely related (especially for what concerns links)",
      "Addressing review comments\n\nSome reorganisation of content and some rewording\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Adding section on how to use passphrase-protected SSH keys\n\nCo-authored-by: Nicola Spallanzani <nicola.spallanzani@max-centre.eu>",
      "Apply suggestions from second round of review\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>"
    ]
  },
  {
    "pr_number": 3720,
    "commits_list": [
      "cb4f9f1228e393636c11b82ad3605768340f9131"
    ],
    "message_list": [
      "Add `provenance_exclude_list` attribute to `CalcInfo` data structure\n\nThis new attribute takes a flat list of relative filepaths, which\ncorrespond to files in the `folder` sandbox passed to the\n`prepare_for_submission` call of the `CalcJob`, that should not be\ncopied to the repository of the `CalcJobNode`. This functionality is\nuseful to avoid the content of input files, that should be copied to the\nworking directory of the calculation, to also be stored permanently in\nthe file repository. Example use cases are for very large input files or\nfiles whose content is proprietary. Both use cases could already be\nimplemented using the `local_copy_list` but only in the case of files of\nan input node in its entirety. The syntax of the `local_copy_list` does\nnot support the exclusion of arbitrary files that are written by the\ncalculation plugin to the sandbox folder.\n\nBefore the addition of this new feature, the contents of the sandbox\nfolder were added to the repository of the calculation node simply by\nmoving the contents of the sandbox entirely to the repository. This was\nchanged to an explicit loop over the contents and only copying those\nfiles that do not appear in the `provenance_exclude_list` list.\n\nThe advantage of recursively looping over the contents of the sandbox\nfolder and *copying* the contents to the repository as long as it is not\npart of `provenance_exclude_list`, over deleting those excluded files\nfrom the sandbox before *moving* the remaining content to the\nrepository, is that in the former there is a better guarantee that the\nexcluded files do not accidentally end up in the repository due to an\nunnoticed problem in the deletion from the sandbox.\n\nThe moving method is of course a lot more efficient then copying files\none by one. However, this moving approach is only possible now that the\nrepository is still implemented on the same filesystem as the sandbox.\nOnce the new repository interface is fully implemented, where non file\nsystem repositories are also possible, moving the sandbox folder to the\nrepository will no longer be possible anyway, so it is acceptable to\nalready make this change now, since it will have to be done at some\npoint anyway."
    ]
  },
  {
    "pr_number": 4611,
    "commits_list": [
      "22efc4893eda172ec4acac5dc8e298c1e9d8328b",
      "f08cdd06f680d82197c5199bc5348c2a5c4ae883"
    ],
    "message_list": [
      "`CalcJob`: improve testing and documentation of `retrieve_list`\n\nThe documentation on the `retrieve_list` syntax and its functioning was\nincorrect. The inaccuracies are corrected and extensive examples are\nprovided that give an example file hierarchy for the remote working\ndirectory and then for a variety of definitions of the `retrieve_list`\nthe resulting file structure in the retrieved folder is depicted.",
      "Address PR review comments\n\n * Add test for non-existing filepaths in retrieve list\n * Clarify that both instructions formats can be used at the same time\n * Clarify that target element cannot be used for renaming"
    ]
  },
  {
    "pr_number": 5265,
    "commits_list": [
      "532952407244d8cfa6171a311adb183bb3473575",
      "24fc5c0709e7bb749569e470491639df87db4ccf"
    ],
    "message_list": [
      "`Process`: raise when `exposed_outputs` gets non-existing `namespace`\n\nUp till now, a call to `exposed_outputs` providing a `namespace` as\nargument that doesn't exist, i.e., no outputs were ever exposed in that\nnamespace, would simply be a no-op. Given that this would often be\nbecause the namespace was accidentally misspelled, the error would go\nunnoticed and the process would finish without error message.\n\nTo prevent this situation that is difficult to debug, the method will\nnow raise a `KeyError` if the top-level namespace does not exist.",
      "Merge branch 'develop' into fix/5261/exposed-outputs-non-existing-namespace"
    ]
  },
  {
    "pr_number": 3354,
    "commits_list": [
      "e213bd0e45f1bec79cb57dc02d77864ec3308774",
      "c9ed5fedb0e48d2e94b01d8ae0c60ee6f5a4516c",
      "904223c8192b4a3dc8690195e32b4011eb234809",
      "9573472adb9e1bc72b68f3287e2a49dc50e3c5dc",
      "ab8b6d3888f4e93a82b8861e83199a1e447a9297"
    ],
    "message_list": [
      "Multiple RETURN links import now possible",
      "Redo Link validation for import functions",
      "Fix test - sort lists before assertion",
      "Address review by @sphuber",
      "New links are continuously added to existing links"
    ]
  },
  {
    "pr_number": 4196,
    "commits_list": [
      "6c251bd3bcd6210de3813205c8b1ee9ff4aa7abe"
    ],
    "message_list": [
      "Remove all files from the pre-commit exclude list\n\nExcept for the documentation of course, which should remain excluded.\n\nAlso move the `pylint` pre-commit hook back under `local`. The idea by\nputting it under the remote repos was to profit from the separate\nvirtual environment that is created with the exact version specified in\norder to prevent clashes with requirements of other projects being\ndeveloped in the same virtual environment. However, this approach leads\nto many spurious false positive import-errors because `pylint` cannot\nfind all other third party dependencies. This can be fixed by specifying\n`language: system`, but this just forces the normal virtual environment\nto be used, rendering the whole point of using the remote repos moot."
    ]
  },
  {
    "pr_number": 4171,
    "commits_list": [
      "07946ba301d937dc8db69565548e0ca7d55f333a",
      "1481a2342062520c43094de8736f500672949093",
      "09faa6083220386baf9921c18a4ca9bcbc16a311"
    ],
    "message_list": [
      "Fix bug in `TrajectoryData.get_step_data`\n\nIf a `TrajectoryData` does not contain an array `cells` the\n`get_step_data` method should define and return `cell = None`.\nThere was a typo that caused an error in this case.",
      "Add tests for the `TrajectoryData` class\n\nThe new `tests.orm.data.test_trajectory` module tests some of the methods\nof `TrajectoryData` to extract data and structures from a particular time\nstep of the trajectory.",
      "Add parameter `custom_cell` to `TrajectoryData.get_step_structure`\n\nIf a `TrajectoryData` does not contain cells, the method `get_step_structure`\nreturns a `StructureData` with the default cell.\nThe optional parameter `custom_cell` allows one to specify the cell of\nthe structure that will be extracted from a `TrajectoryData`.\nIf omitted, the cell will be read from the trajectory, if present.\nIf not found, the `StructureData` will use the default cell, and an user\nwarning will be emitted.\nDocs updated."
    ]
  },
  {
    "pr_number": 4950,
    "commits_list": [
      "1e7589e5e013a85ea4c944b2713c1f0098e1feaa",
      "28598e0d1ac583dc7f87df1a2e25e3afcc920bc6",
      "b60ffe49de24ac8026ff1e1e6fbbc6e1c2b610e0"
    ],
    "message_list": [
      "suggest running reentry scan for unstorable Node\n\nWhen encountering a Node that isn't storable because of an entry point\nthat isn't registered, suggest to run `reentry scan`.\nWe got a user question regarding this, apparently this was not obvious.",
      "incorporate suggestions from code review",
      "Update aiida/orm/nodes/node.py\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>"
    ]
  },
  {
    "pr_number": 5410,
    "commits_list": [
      "eefa3678b528639382718717519996ef7963df74",
      "fe0d3a93f8994218d491c859cb695935d6966338",
      "952ed5aa532d8fb04fc5286c3e5b2b8762e03c6e"
    ],
    "message_list": [
      "`QueryBuilder`: add `flat` keyword to `first` method\n\nThis keyword already exists for the `all` method and it will likewise be\nuseful for `first` when only a single quantity is projected. In that\ncase, often the caller doesn't want a list as a return value but simply\nthe projected quantity. Allowing to get this directly from the method\ncall as opposed to manually dereferencing the first item from the\nreturned list often makes for cleaner code.",
      "Define `overload`ed methods for typing purposes",
      "Use `first(flat=True)` where applicable and add pylint ignore where it is being a pita"
    ]
  },
  {
    "pr_number": 5114,
    "commits_list": [
      "62040f56c23ff94403c79c99e866c3d20961eb01",
      "4c92cec146f94616f6e2394bc9671e626f8d88df"
    ],
    "message_list": [
      "`Node`: add the `copy_tree` method\n\nThis method allows to easily copy the entire repository tree, or a\nsubdirectory, of a `Node` to a place on the local file system. The\nmethod simply forwards to the `aiida.repository.Repository` class, where\nthe functionality is actually implemented using the `Repository.walk`\nmethod, recursively looping over all the files contained within the\nnode's repository.",
      "Merge branch 'develop' into feature/4928/node-copy-tree"
    ]
  },
  {
    "pr_number": 4669,
    "commits_list": [
      "4d24556fb914972e75922408b0cac7cfe785fe91",
      "744a5c1bc4f000fb237dd24cf92b3a5597336e4b",
      "a51d4a5eda5ac1c03e3864311df7e78a642cbf0a",
      "f46ab1de5e310bafb0e04773cc73665dfd8ca314",
      "e8a044003d2b185660de9c13f9d47e6e32a0dd45",
      "fa996fb43dee4395e9a4973cc0b5850380246803",
      "8651edab837e7e124f51cd8f99f9d0196703273e",
      "9c06bc38174814bd6292927c8d2c33cd13b89ff8",
      "9edd9e9f6dfca41e297d0adce660d5aa631088d3",
      "75d8ba391a082a439597ce7eafd5d5aa98a46dd2",
      "3596e143ae021a05d8cac7037b59b93e8ede4da6",
      "2a6679ed9ca98a624e8a259faf2cd4deb51d0514",
      "0091fc69f93f177a97993cc27efa19f797581e2a",
      "498dd028b5d3686ceb3db48ba76d84d02df82dda",
      "8151a83d50e27778f43ef7e3e3e15fe4a424c6ce",
      "24140ade25bc5add3fe3f34dadb5d4e609b09e30",
      "d9ef4621f499543e4078f0bd8f31bd373a48dffe",
      "481c4b66e7f973b70a161b9836eda04932d6297d",
      "1e11dab23685b010503d366955d97aa7585635db"
    ],
    "message_list": [
      "Type Checking: `aiida/engine/processes`",
      "Merge branch 'develop' into typing/processes",
      "fix tests",
      "fix docs",
      "fix test",
      "add `aiida.manage.manager`",
      "Apply suggestions from code review",
      "add aiida/engine/launch.py",
      "Add persistence and runners",
      "update",
      "Update nitpick-exceptions",
      "add transports and utils",
      "add aiida/engine/daemon",
      "Move aiida.orm imports to top of file",
      "added logger warning if killed future was None",
      "reimplement yapf: disable",
      "final lint fixes",
      "revert yapf disable",
      "Merge branch 'develop' into typing/processes"
    ]
  },
  {
    "pr_number": 2323,
    "commits_list": [
      "1be38c2b711a155f1a78ddd237974a664dc7ba40",
      "c034eb61678c4a31a3abadc658b147720b2da258"
    ],
    "message_list": [
      "Add uniqueness constraints on all UUID fields in SqlAlchemy models\n\nThis adds the uniqueness constraint on the UUID fields of the models:\n\n * Computer\n * Comment\n * Group\n * Node\n * Workflow\n\nA check is performed before the migration is applied, to see whether the\naffected tables actually contain any entries with duplicate UUIDs. In\nthis case a warning is emitted and the migration is stopped. The\ninfrastructure to deduplicate, that is present all ready for nodes, has\nnot yet been generalized to arbitrary tables.",
      "Generalize the UUID deduplication for more database tables\n\nThe original deduplication code was implemented only for the `DbNode`\ntable but has now been extended to `DbComputer` and `DbGroup`. Note that\nthis has not been done for `DbComment`, because it does not yet have an\nentity loader. If `load_comment` were to be implemented, support for\ndeduplication can be added to the `deduplicate_uuids` function."
    ]
  },
  {
    "pr_number": 3409,
    "commits_list": [
      "176442a7c3e4dfa692047eb5d4ccd3363a9b0467"
    ],
    "message_list": [
      "Suppress kiwipy log messages for verdi status check (#2914).\n\nThe kiwipy package logs the full traceback in case that the connection\nto the RabbitMQ server cannot be established. This patch suppresses this\nlog message to reduce the noise presented to the user for the status\ncheck."
    ]
  },
  {
    "pr_number": 5307,
    "commits_list": [
      "47db0bec1a58994ed470663e8b017b161de2a617"
    ],
    "message_list": [
      "Dependencies: drop support for Python 3.7\n\nFollowing AEP 003, which follows NEP 029, support for Python 3.7 is\ndropped as of December 26 2021."
    ]
  },
  {
    "pr_number": 3681,
    "commits_list": [
      "f6c05d47d844eb2d9f45ad31582381365bc72b95"
    ],
    "message_list": [
      "fix performance issue when exporting many groups\n\nWhen providing groups to `verdi export`, it was looping over all groups\nand using the `Group.nodes` iterator to retrieve the nodes contained in\nthe groups. This results (at least) in one query per group, and is\ntherefore every inefficient for large numbers of groups. The new\nimplementation replaces this by two queries, one for Data nodes and one\nfor Process nodes.\nIt also no longer constructs the ORM objects since they are unnecessary."
    ]
  },
  {
    "pr_number": 3117,
    "commits_list": [
      "e61149d971853bf59b131277fb409eaa9b97d997",
      "0e48f39eefca309cb3b36d10e2e262a04b803c95",
      "0f34868c1a552aec9f7defd45017b07c8a30c350",
      "8c8fe46b5a540ce2981d9078fcc78bbd266abb9f",
      "c71ad5482ba6a3e24f0af9afcb23ffb231a45ca8"
    ],
    "message_list": [
      "update command line documentation\n\n * hide unnecessary TOC (duplication of list)\n * reorder subsections, starting with parameters, then help strings\n   (which contain info on parameters)\n * generally make text less wordy and move technical information to\n   subsections\n * replace ID with PK",
      "put back line for verdi docs autobuild",
      "Merge branch 'develop' into issue_3020_cmdline_docs",
      "incorporate suggestions by @CasperWA",
      "further fixes by @CasperWA"
    ]
  },
  {
    "pr_number": 3537,
    "commits_list": [
      "8925b8017013dddb1f0fbae6204cea5922ca2907",
      "5e7fd7ab3888ebdfc009956a15bdfad14ff27710",
      "6b594919b0ee0b67fba973f6f4a8b9a3b68f1e05",
      "dca2107bc8b4ce1e0c030d364be78d1bfb72f3b3",
      "9ddd6a94d4df40b820faeaba6f3c03d32ced7e01"
    ],
    "message_list": [
      "Encode UTF-8 for export json files",
      "Have encoding info for ZipFolder",
      "Use bytes-stream when writing a ZipFolder\n\nTest 'b' in mode is respected, when using ZipFolder.open().",
      "Issue #3199 has been fixed, remove skip of test\n\nIssue #3199 has been fixed by #3402.",
      "Support Py3.5"
    ]
  },
  {
    "pr_number": 2690,
    "commits_list": [
      "5a92c22334ea3cf07c9384c358936104ca396261"
    ],
    "message_list": [
      "improve error message for missing parse method"
    ]
  },
  {
    "pr_number": 5233,
    "commits_list": [
      "a3fae7511373e1209f39ca0e000ff0cfa65b2ead"
    ],
    "message_list": [
      "Do not use the deprecated matplotlib config option 'text.latex.preview'.\n\nDeprecated as of version 3.3.4, see:\nhttps://matplotlib.org/3.3.4/api/api_changes.html#text-latex-preview-rcparam\n\nRequires matplotlib~=3.3,>=3.3.4.\n\nFixes #5231."
    ]
  },
  {
    "pr_number": 4915,
    "commits_list": [
      "6146a02b2fe693bf800a4fc4ba78e9d8f37e2578"
    ],
    "message_list": [
      "Fix the benchmark tests\n\nThey were still using the `silent` keyword for the import and export\nfunctions, which have recently been removed causing the benchmark tests\nto except."
    ]
  },
  {
    "pr_number": 4458,
    "commits_list": [
      "a5ab25f191d06889e25517eabc3476b1b03b6ffa",
      "82493598473ae3fdc67b00a36fc4cf4f1c4bd501",
      "e2cf8bcaa05f4fb688a47b0042e1d15863211884"
    ],
    "message_list": [
      "Docs: add \"How to share data\"\n\nThe \"how to share data\" section includes instructions both for dealing\nwith AiiDA archives (e.g. for publishing AiiDA graphs alongside your\npublication) and for using the AiiDA REST API.\n\nCo-authored-by: Casper Welzel Andersen <43357585+CasperWA@users.noreply.github.com>",
      "replace all occurences of \"export file\"\n\nWe have agreed on the terms \"AiiDA archive (file)\" and \"AiiDA archive\nformat\".\n\nCo-authored-by: Casper Welzel Andersen <43357585+CasperWA@users.noreply.github.com>",
      "Merge branch 'develop' into issue_4432_how_to_publish_data"
    ]
  },
  {
    "pr_number": 1438,
    "commits_list": [
      "90cf7f74e0386984280e2e3d1d639e64ddc91fac",
      "f93e61ed2bab1e349256864b63d0d1da9f347699",
      "e5cfda004a4844da116dc8bc8ef6be52b3bce84f"
    ],
    "message_list": [
      "document adding aiida daemon as system service",
      "update reentry",
      "typo"
    ]
  },
  {
    "pr_number": 3459,
    "commits_list": [
      "717d83a992cf433cb26a0b6bbdb9028a67bff045",
      "05bc3784c7a093ed773b0109f1af7eed3758c71f",
      "de30a4e7665f5e0082f22468851aabcb31c9e750",
      "b8d21ba9f23e5356d278d248506e75a16efa7075",
      "ca0cf57ea0b35a75b2b97662de7f99751434bd81",
      "67e1387ae04da0e632450f3f1027a92fe6c49e22",
      "5112cd8a38465ee411c87e4c92e42c77c0e1b801"
    ],
    "message_list": [
      "Modified the default behaviour of export traversal rules. Also:\n* Adapted test to new defaults.\n* Captured some warning prints during tests.\n* Fixed a section of sqlalchemy import no being properly 'silenced'.",
      "Modified the documentation to comply with new export behavior.",
      "Changed default behavior for the delete feature and adapted tests.\nNow call_calc_forward and call_work_forward will default to True.",
      "Documentation updated.\n* New delete default rules updated in workflows and calculations.\n* Cascading example now explains things from the new defaults.",
      "Addressed comments and corrections.",
      "Last changes.",
      "Merge branch 'develop' into new_defaults"
    ]
  },
  {
    "pr_number": 4415,
    "commits_list": [
      "739eb14c88e39a67e7136bb428908c534bad3f1e"
    ],
    "message_list": [
      "`CalcJob`: make sure `local_copy_list` files do not end up in node repo\n\nThe concept of the `local_copy_list` is to provide a possibility to\n`CalcJob` plugins to write files to the remote working directory but\nthat are not also copied to the calculation job's repository folder.\nHowever, due to commit 9dfad2efbe9603957a54d0123a3cec2ee48b54bd this\nguarantee is broken.\n\nThe relevant commit refactored the handling of the `local_copy_list` in\nthe `upload_calculation` method to allow the target filepaths in the\nlist to contain nested paths with subdirectories that might not yet\nnecessarily exist. The approach was to first write all files to the\nsandbox folder, where it is easier to deal with non-existing\ndirectories. To make sure that these files weren't then also copied to\nthe node's repository folder, the copied files were also added to the\n`provenance_exclude_list`. However, the logic in that part of the code\ndid not normalize filepaths, which caused files to be copied that\nshouldm't have. The reason is that the `provenance_exclude_list` could\ncontain `./path/file_a.txt`, which would be compared to the relative\npath `path/file_a.txt` which references the same file, but the strings\nare not equal.\n\nThe solution is to ensure that all paths are fully normalized before\nthey are compared. This will turn the relative path `./path/file_a.txt`\ninto `path/file_a.txt`."
    ]
  },
  {
    "pr_number": 4852,
    "commits_list": [
      "579bc1288fc25ad275ccc3f1c6c8ba4127fc40b5",
      "b1370dcad6e3e09342a71f8e4f535bb89d39ff7b"
    ],
    "message_list": [
      "Implement 'identify-outdated' command for dependency management.\n\nUtility function to analyze the package dependencies and identify\npackages where the latest release is no longer compatible with our\ndependency specification constraints.",
      "Add requirements.txt for utils/."
    ]
  },
  {
    "pr_number": 2078,
    "commits_list": [
      "69153a39866510043e18984e6828420c90d79ec3"
    ],
    "message_list": [
      "Add cmdline templates to the MANIFEST.in\n\nThese template files are necessary for the `verdi` command line to\nwork and so should therefore be included in the package, which is\nachieved by listing them in the manifest file."
    ]
  },
  {
    "pr_number": 5568,
    "commits_list": [
      "573734c504482be153786ec12f1862c79d68a66b",
      "3ee1aa7dc8de4a91bfe7bbc34ab330280fc7d24f",
      "81636ff2753777a7271f7f40cddcdbaa1f26145a"
    ],
    "message_list": [
      "feat: `verdi process status --max-depth`\n\nAiiDA workflows can get very complex with many layers, leading `verdi\nprocess status` to print hundreds of lines of text.\n\nThe `--max-depth` allows the user to \"zoom out\" and look at the bigger\npicture, while still being able to \"zoom in\" on subworkflows of interest\nby calling `verdi process status` on those.",
      "add test for max-depth=0",
      "revise code logic for Jason"
    ]
  },
  {
    "pr_number": 1668,
    "commits_list": [
      "b9c9552364b12d4d24bda2910bb2f4d2aafbbb80",
      "dc286b5a7f5089f2ae98a9ec32093f457c5f76c8",
      "8f7eeb56b76011112199b0e810b9bae733efba9d",
      "7a5554d58c2cb7b32965c545051105fafe7a277e",
      "6e713a6d54167638356ac02ad2e21cd05300afb4",
      "90c8627045c0b47692f34c8c40d1ab3d83cfbcef",
      "59673e636d05fccd7068467d61cb85ced809df48",
      "773b926dc8ada3439e5d71655761f84fd8c6eaa3",
      "4154c45394a0f7e24318d67d0df4fd62390b7c3b"
    ],
    "message_list": [
      "[wip] convert legacy workflows command, no tests",
      "verdi workflow: add tests, pass",
      "verdi workflow: add tests for legacy workflow param type",
      "fix LegacyWorkflowLoader for sqla backend",
      "fix test faillures related to the legacy workflow commands",
      "improve docstrings and replace prints with echos",
      "fix #1671 (sqla Workflow.get_report())",
      "remove unrelated files from linters, fix workflow list",
      "verdi workflow: unify pk formatting for testability"
    ]
  },
  {
    "pr_number": 4816,
    "commits_list": [
      "7a0402ab1da5db4dd7a1653630854f43f845dbcd",
      "e32bf079fc7723a7bf8eb6f779da45b8fe7fd7fe",
      "e4d7a1ef52601c94400f6159c2532e7e235a3cf7",
      "9edf6665a2e4a3b7c71ce263ed23229f6d58183c",
      "36cd45435de1163027a881ad953a1f3c719b0175",
      "96948e42977540ed433f55fc7a44c5970d4d5dfd",
      "a9c263f0f834ebd37d3e8ba40a31190bf7ddc7d3",
      "2a066a545bde30e9f5f9dc9545b3450aa20e3ebf",
      "789eaf2330f3af02a2f505ef5f672aa6cbe99f7d",
      "80aff5ababe66b75b76c2c06685447e35dff4d3a"
    ],
    "message_list": [
      "\ud83d\ude80 RELEASE: v1.6.0",
      "Add PR links",
      "Update CHANGELOG.md",
      "Merge branch 'develop' into release/v1.6.0",
      "fix header",
      "fix heading",
      "review suggestions",
      "final review suggestions",
      "Apply suggestions from code review\n\nCo-authored-by: ramirezfranciscof <ramirezfranciscof@users.noreply.github.com>",
      "Apply suggestions from code review\n\nCo-authored-by: ramirezfranciscof <ramirezfranciscof@users.noreply.github.com>"
    ]
  },
  {
    "pr_number": 4575,
    "commits_list": [
      "a1b7591488f3ee77f3381112f1d0ebb0a6c46eaa",
      "b321e0836f5302be9876bd5c20485a815a7bab50",
      "815d0de2d9bb5ed8f33ddd5f51378ff1c56e7b9d",
      "917a7239eb229e4453ec8d0068530e54bac556c7",
      "28f09cf3ba932300140767c19f00907160400959"
    ],
    "message_list": [
      "Improve `verdi node delete` performance",
      "finalise",
      "Update aiida/manage/database/delete/nodes.py",
      "Update aiida/tools/graph/graph_traversers.py",
      "Merge branch 'develop' into verdi-node-delete"
    ]
  },
  {
    "pr_number": 2706,
    "commits_list": [
      "80c5a345348ecb001d214dbdaf59e854f0086c43",
      "c671d8cc1a1806546466a59f27c8b84a1e06127e"
    ],
    "message_list": [
      "Replace `TextField` stored for storing JSON with proper `JSONField`\n\nThe following model fields were using a text field to store JSON:\n\n * `DbComputer.metadata`\n * `DbComputer.transport_params`\n * `DbAuthInfo.auth_params`\n * `DbAuthInfo.metadata`\n * `DbLog.metadata`\n\nNow that Django supports a JSON field for the Postgres JSONB field it is\nbetter to replace the old implementation such that these columns become\npropertly queryable and we are no longer responsible for proper\nserialization and deserialization",
      "Clean the `AuthInfo` frontend and backend implementations"
    ]
  },
  {
    "pr_number": 3713,
    "commits_list": [
      "5e470acacc36931e07aa5aff6140f1a1486d6b21",
      "0c8a448dbf987f708f2e76dee623c9475a142027"
    ],
    "message_list": [
      "limit batch size for bulk_create operations\n\npostgresql has a \"MaxAllocSize\" that defaults to 1 GB [1].\nIf you try to insert more than that in one go (e.g. during import of a\nlarge AiiDA export file), you encounter the error\n\n    psycopg2.errors.ProgramLimitExceeded: out of memory\n    DETAIL:  Cannot enlarge string buffer containing 0 bytes by 1257443654 more bytes.\n\nThis commit avoids this issue (for the django backend) by setting a\nbatch size for bulk_create operations (via a verdi config option).\n\n[1] https://github.com/postgres/postgres/blob/master/src/include/utils/memutils.h#L40\nmax alloc\" size",
      "Merge branch 'develop' into issue_3712_max_postgres"
    ]
  },
  {
    "pr_number": 967,
    "commits_list": [
      "56dd128a6527b6caf32055e078582e8e4c573492",
      "10bb73ade4c267723795d28990786a8cf6b5edcf"
    ],
    "message_list": [
      "add regression test for #966",
      "fix #966."
    ]
  },
  {
    "pr_number": 5744,
    "commits_list": [
      "025d91a62eb6157d9f1389f13f10a591127b2202"
    ],
    "message_list": [
      "Process functions: Add serialization for Python base type defaults\n\nAs of 06802099fad32fb113b52fca40c867674aa7ea94, process functions will\nautomatically add the `to_aiida_type` serializer for arguments such that\nPython base types are automatically converted to the corresponding AiiDA\ndata type if not already a `Data` instance.\n\nThe same was not added for defaults however, so if a user defined a\ncalcfunction with a Python base type default, an exception would be\nraised once the dynamic `FunctionProcess` would be constructed. The\ndefault would be passed to the `InputPort` constructor as is and so\nwould not validate against the type check.\n\nHere we update the dynamic process spec generation to detect if a\ndefault is provided for an argument and if so serialize it to the node\nequivalent. Note that this is done indirectly through a lambda as using\nconstructed node instances as defaults in process specs can cause issues\nfor example when the spec is exposed into another process spec, the\nports are deep copied, including the defaults of the ports and deep\ncopying of data nodes is not supported."
    ]
  },
  {
    "pr_number": 2853,
    "commits_list": [
      "16e402008bf2e7baab48e8a49574bdb4a1d18b04"
    ],
    "message_list": [
      "changes in verdi restapi command\n\n- updated verdi restapi command and its click parameters\n- removed load_profile functionality from restapi code as it is\n  already handled by verdi command\n- removed __main__ from restapi code as it is duplicating the\n  functionality provided by 'verdi restapi' command"
    ]
  },
  {
    "pr_number": 4269,
    "commits_list": [
      "3d7462b68286a71d43d4a1671fa2f93fe103b7c0"
    ],
    "message_list": [
      "Run test-install GA workflow only on main repository.\n\n * To avoid wasting of resources on forks.\n * Resolves issue #4257."
    ]
  },
  {
    "pr_number": 2838,
    "commits_list": [
      "77cf654e0edfd264ae6b3a5d52d687b69b99f264",
      "e101c2409d57239f3284e02ecdcc3a6244513e5e"
    ],
    "message_list": [
      "BandsData: fix _prepare_gnuplot() and matplotlib_header_template\n\nfixes #2756\n\n* _prepare_gnuplot():\nRemove raw_data.encode('utf-8') call that tries to convert\npreviously converted bytes object\n\n* matplotlib_header_template:\nremove `from __future__ import print_statement`",
      "fix: verdi data bands show"
    ]
  },
  {
    "pr_number": 4475,
    "commits_list": [
      "656b6130b971a714c767d3bfa3316c03341ea8f4"
    ],
    "message_list": [
      "Docs: add \"How to interact with AiiDA\" section\n\nThis section gives a high-level overview of the various methods with\nwhich can interact with AiiDA. For details it refers to relevant\nsections with more information."
    ]
  },
  {
    "pr_number": 4825,
    "commits_list": [
      "0e4b44ab4569692d0b05aa1c0c29d2e234a9144b",
      "af03861d268fb9ed39e8a6172273f832281914f1",
      "e9a32a70c07f7e471aec96fed9cb8a4ce5fc55f0",
      "cdb0329c59c4c6bde806b52c72e96c6b77039272",
      "e2c046c9c00877543283a3ed74339c610bcbf69a"
    ],
    "message_list": [
      "scheduler memory updates",
      "Merge branch 'develop' into scheduler-mem-fix",
      "warning instead of info",
      "bonus bugfix: wrong variable name used",
      "Merge branch 'develop' into scheduler-mem-fix"
    ]
  },
  {
    "pr_number": 3625,
    "commits_list": [
      "4f8b8a7464dbb4901acf487ba33db56bad9d3c28",
      "3f1f6fc4fb5e1aa5642b2ae5b623f42c15f72078",
      "5b5f6b8cfa04a964730a8c2526904e82f6ec71b5",
      "ca0f9dc74be195c2fe11e22e14a2c37ece6fb88c",
      "07d5e2f8507d45356199b13ebe2d6ffac669d56e",
      "8e60649e8fdf50dcf564208a0da6f2f67d71135e",
      "ab9df714880d367d4aa89cef1c3a6fe390a0b92d",
      "16a9bc417d50a18ff2f73e3a0a01c20cea93c207",
      "f865d48c0d141664734ff19d09f2a1afe8c5bf4b",
      "d973aaf82360b54fc92072d5e67792cbb2b29d12",
      "d86ef1e3afb4e05ced64408d66439e2915a956a3",
      "b07fc4f8a25fec814d56d73d8822150983c6abd8",
      "03a81e50a72f4470e76b9a2baa6055721c84a195",
      "fc42510333edb3aced6c5ada4e3ebacd3c7f3e73",
      "f0521148f0667daed7f660d11e64eb750b0f4c2e",
      "68fb6cca484b18fb579a914ffe9b963202f02846",
      "2c77f24d722de247c4cd76f348098b9728fa7e2d",
      "582ea90e49a1168e34bc9a2d6ea69388a1382a01",
      "3a7ef925c819be9345ab250d68b022b6198b6fef",
      "cd6aa2002d6be8457f30f1e0befeebd6d491e638",
      "35cc2c589eead8c415889da40c3e5a3fcf077d4e",
      "ca3274bcb6d2c63a11a2bfc09fe7a2b409ee6c00",
      "799d2d2a7e7a027c24b68b0cbac9c32a53320efa"
    ],
    "message_list": [
      "Remove 52 files from .pre-commit black list",
      "Delete aiida/backends/tests/cmdline/commands/test_computer.py",
      "fix pre-commit",
      "rm python_2_unicode_compatible from all files",
      "Update aiida/backends/general/abstractqueries.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "modify aiida/backends/general/abstractqueries.py",
      "Update aiida/backends/sqlalchemy/models/settings.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/sqlalchemy/models/settings.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/sqlalchemy/models/user.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/sqlalchemy/models/user.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/sqlalchemy/tests/test_generic.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/sqlalchemy/tests/test_generic.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "fix aiida/backends/tests/cmdline/commands/test_user.py",
      "Update aiida/backends/tests/engine/test_class_loader.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/tests/engine/test_persistence.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/tests/engine/test_process.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/tests/engine/test_runners.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/tests/engine/test_runners.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/tests/engine/test_utils.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/tests/orm/utils/test_loaders.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/tests/orm/utils/test_loaders.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/tests/test_base_dataclasses.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "modify aiida/backends/tests/test_dataclasses.py"
    ]
  },
  {
    "pr_number": 5082,
    "commits_list": [
      "553954da2a9663d65c6e33af168f5d66d2966c3a",
      "2feb2b47ccaeeded86eee6d41a1e07763c7d3dde",
      "6018636dba91041196903799dfbca759c0f35db3",
      "e38d636dc1ab7db42709420bf435d22bd71ad8c0"
    ],
    "message_list": [
      "MAINTAIN: Add 'check-yaml' to pre-commit hooks.",
      "Dependencies: Fix the pyzmq requirement specification for Python 3.9.",
      "CI: Implement resolve-pip-dependencies job.\n\nThis job checks whether the environments defined in the requirements/\nfiles are resolvable.",
      "CI: Run test to install-with-conda job for all supported Python versions.\n\nVerifies that aiida can be installed with conda for all supported\nPython versions."
    ]
  },
  {
    "pr_number": 4901,
    "commits_list": [
      "70c05051aa4680a222946c95b239237041c3c856"
    ],
    "message_list": [
      "`Config`: add the `delete_profile` method\n\nThe class already has `remove_profile` that merely removes the profile\nfrom the index and doesn't actually delete its associated resources. The\nnrem `delete_profile` method will by default also delete the repository\nand database that are configured for the profile, although this behavior\ncan be controlled with the `include_database` and `include_repository`\narguments.\n\nThe implementation of `verdi profile delete` is update to use this new\nfunctionality instead of the loose functions that were defined in the\n`aiida.manage.configuration.setup` module. The latter, since it was not\npart of the public API and was only used by this CLI command, has been\nremoved from the codebase.\n\nNote that the implementation for the deletion of the database is\nslightly different. Not only does `Config.delete_profile` not contain\nany echoing of information, as this should be done in the CLI command\nand not in the generic API, the logic for deleting the database user\nhas also changed. The old function `delete_database` did delete the\ndatabase user configured for the profile, as long as no other profiles\nin the config used the same. This is dangerous, however, because the\nsame database user can be used by a program outside of AiiDA and we\ndon't own those users necessarily. Now, the deletion of the database\nuser is optional and turned off by default. When it is used though, the\ndatabase user is deleted without checking the config for other profiles\nthat may also have configured it."
    ]
  },
  {
    "pr_number": 2962,
    "commits_list": [
      "5ba688a9ea7fb36a17ccf78cd093a143716356d1",
      "d1fa180adb206fd8904e46894471f2e99d572f9c"
    ],
    "message_list": [
      "Fix bug in `verdi process show` where not all called processes are shown\n\nThe utility `get_node_info` was called by `verdi process show` to format\na multiline string of the given process node, including all the inputs\nand called processes. The function used the `LinkManager.nested` to\nreconstruct a nested representation of the relevant set of neighboring\nnodes. This is useful for the inputs and outputs that can in fact be\nnested and have unique labels, however, for called links, the link\nlabels are not necessarily unique and so the dictionary returned by the\nnested method could only represent a single called process with the same\nlink label. To fix this we use a different formatter that simply takes\nthe list of neighboring nodes, which then does not care about label\nuniqueness.\n\nSince the `nested` method was silently overwriting duplicate keys, which\ncaused this bug, its behavior is adapted to raise a `KeyError` in this\nsituation.",
      "Merge branch 'develop' into fix_2868_verdi_process_show_duplicate_links"
    ]
  },
  {
    "pr_number": 2690,
    "commits_list": [
      "5a92c22334ea3cf07c9384c358936104ca396261"
    ],
    "message_list": [
      "improve error message for missing parse method"
    ]
  },
  {
    "pr_number": 3481,
    "commits_list": [
      "51216fe20ade2c51ca2ec64bad46f5a1212b6d05",
      "e7e91977d9ce290860c9611f80d4966acbdc2ae2",
      "8b2206c37c7cfc560a3c9b3235a23c7588d4ec57"
    ],
    "message_list": [
      "Accept arguments to aiida_profile",
      "Alternative approach with factory function",
      "Fixed reset_db access"
    ]
  },
  {
    "pr_number": 3269,
    "commits_list": [
      "485859ee0cb15cdfe5f5bb1e9a95b6e4ef401a28",
      "f88430f3ffc78831def68d604d586fc10b36f26a",
      "01a5c1473c08395c8c7eeb316ac89bc1cd5f3545",
      "8550043b9cd5e1b1cc18097da570dfcd1022b447",
      "3fbaa070aafb4685936f694fe1f9c70b945165f2",
      "d0c4d18e33dbd46e9e8daad4ae22bd3f9cd17734",
      "5ce411cbb1af777561b509c721a663c8267cf99d",
      "3ed7aff04dfa9d0f853c9410651ea9169fad5853",
      "55c7d14036d99aa757fb1387b260ad66608f66eb"
    ],
    "message_list": [
      "added nodes/types endpoint with testcase\n\nIt returns unique list of all the nodes types available in database.",
      "Fix eagerness of `verdi setup --help` being overruled (#3257)\n\nThe eagerness of the `--help` option for the `verdi setup` and `verdi\r\nquicksetup` commands was being overruled by the `--profile` option. This\r\nis because the latter was also made eager, such that `verdi setup --help`\r\nactually prompted for the profile name instead of showing the help.\r\n\r\nThe profile option was made eager in an attempt to solve another problem\r\nwhere some of the other options of the command have contextual defaults\r\nthat depend on the value of the profile. It therefore needs to be parsed\r\nfirst. In `click` the parse order is determined by the order in which\r\nthe options are defined on the command, regardless of the order in which\r\nthey are passed by the user. This was done correctly, but still the\r\ncontextual default of, for example the `--repository` option, was called\r\nbefore the `--profile` was parsed, if the former was passed first. This,\r\nhowever, was due to the fact that for the repository option, both the\r\n`callback` as well as the `contextual_default` were defined, where the\r\nformer was inherited from the base option for the `verdi quicksetup`\r\ncommand.\r\n\r\nThe final solution then is to remove the `callback` from the `--profile`\r\noption for `verdi setup` and only have the `contextual_default`. This\r\nwill make sure that `--profile` will be parsed before the prompt loop\r\nwill be entered and the `contextual_default` will be called.",
      "rename \"repository path\" to \"repository URI\" (#3267)",
      "clarify documentation of label/description (#3169)\n\nswitch from :py:meth: to :py:attr: in order not to show \"()\"\r\n(this confused people)",
      "Remove dead code for printing comment mode (#3254)",
      "Fixed pre-commit issues",
      "Fixed prospector issues",
      "removed /codes/types endpoint as codes now comes under data node.",
      "Merge branch 'develop' into rest_types_3266"
    ]
  },
  {
    "pr_number": 3314,
    "commits_list": [
      "70f45416a0393a694badb6586dfff11a7ed504b9",
      "d0b179c0c89ea2458ef839128242359fec17d3ca",
      "d6d13fe1d68cdeb1635cfada7cb7bb5129d6f74b",
      "3f619756b8ab0379193d7cc9373de7a71615ea78",
      "615fbb4eaed7a55ab8ceca5a5bb2099ff48a901f",
      "64f1f1dee300c07a3460f8e5feaf0305e8f7031a",
      "b41892c7b169e0d6826c3a2e43496cea2cce0bc3",
      "975f82ffec25a6df89803cb59b302a3b03718a8b",
      "039c436f474e4a0e52cd2b31cd6cf29165bad563",
      "52bcaee05e767996e755ba591b8d5bf9a39a0576",
      "9dab0155d3fcf141c1f6db5625f5d70a62b2cfae",
      "387ad07e10a615949bc1f2183cdd8126906950da",
      "f33f20ea3643b984b50e76a24111d9b11da9148b",
      "b5150f8b9601d171b2f9b49ec7a00ff95d361023",
      "55500f4894a22ed8a05cc37f9ff717c61b07da23"
    ],
    "message_list": [
      "dirty commit\n\nfirst working build of docs",
      "more fixes",
      "Merge branch 'develop' into issue_3018_calcjob_docs",
      "revert changes to test_workchain.py",
      "fix class check\n\n+ apply linter",
      "incorporate update by @espenfl\n\n + fix XML reference",
      "Merge branch 'develop' into issue_3018_calcjob_docs",
      "switch from aiida-process to aiida-calcjob",
      "Merge branch 'develop' into issue_3018_calcjob_docs",
      "revert red highlight of port names\n\n + fix pre-commit",
      "Apply suggestions from code review\n\nCo-Authored-By: Sebastiaan Huber <mail@sphuber.net>",
      "Merge branch 'develop' into issue_3018_calcjob_docs",
      "fix reference results for pytest",
      "try fixing test under python2.7\n\nit seems on travis you get 'basestring' instead of 'str'",
      "Merge branch 'develop' into issue_3018_calcjob_docs"
    ]
  },
  {
    "pr_number": 2853,
    "commits_list": [
      "16e402008bf2e7baab48e8a49574bdb4a1d18b04"
    ],
    "message_list": [
      "changes in verdi restapi command\n\n- updated verdi restapi command and its click parameters\n- removed load_profile functionality from restapi code as it is\n  already handled by verdi command\n- removed __main__ from restapi code as it is duplicating the\n  functionality provided by 'verdi restapi' command"
    ]
  },
  {
    "pr_number": 3459,
    "commits_list": [
      "717d83a992cf433cb26a0b6bbdb9028a67bff045",
      "05bc3784c7a093ed773b0109f1af7eed3758c71f",
      "de30a4e7665f5e0082f22468851aabcb31c9e750",
      "b8d21ba9f23e5356d278d248506e75a16efa7075",
      "ca0cf57ea0b35a75b2b97662de7f99751434bd81",
      "67e1387ae04da0e632450f3f1027a92fe6c49e22",
      "5112cd8a38465ee411c87e4c92e42c77c0e1b801"
    ],
    "message_list": [
      "Modified the default behaviour of export traversal rules. Also:\n* Adapted test to new defaults.\n* Captured some warning prints during tests.\n* Fixed a section of sqlalchemy import no being properly 'silenced'.",
      "Modified the documentation to comply with new export behavior.",
      "Changed default behavior for the delete feature and adapted tests.\nNow call_calc_forward and call_work_forward will default to True.",
      "Documentation updated.\n* New delete default rules updated in workflows and calculations.\n* Cascading example now explains things from the new defaults.",
      "Addressed comments and corrections.",
      "Last changes.",
      "Merge branch 'develop' into new_defaults"
    ]
  },
  {
    "pr_number": 1403,
    "commits_list": [
      "2fa3165f50cd76b9df2226a5a47c84decd631caa",
      "8cfaf847f8c356615f6fa2f11eb322b90ba6fbd3"
    ],
    "message_list": [
      "Fix the problem with socket filename size limitation for circus IPC protocol\n\nCurrently, we are using the IPC protocol as the default for the daemon to\ncommunicate with the endpoints of the circus controller. The sockets used\nto be stored in the AiiDA config folder, however, UNIX has a built in byte\nlimit of 107 on socket filenames and given that the config folder can be\nnested arbitrarily deeply, this limit was easily exceeded. The solution is\nto store them in a temporary directory. This solution should always avoid the\nlimit comfortably but adds slightly more complexity. A temporary folder is\ngenerated by the OS and is only accesible by the user who created it, so\nthe security is guaranteed here, which is what the IPC protocol was supposed\nto solve in the first place over the TCP protocol.",
      "Remove profile uuid from socket names to further reduce size\n\nAlso added a test that will check whether all endpoints do not\nexceed the limit of the OS which is exposed through the Zmq\nlibrary in the IPC_PATH_MAX_LEN constant"
    ]
  },
  {
    "pr_number": 3546,
    "commits_list": [
      "b7b792e3894a3a67f4934cfb65b3dd3a6809c953",
      "7a867b5a8a4535e3e13a4a8099a8a5a734933417",
      "d5785f836597c2398a3f5034826faf0faf9362ee",
      "b08b979fdce2f55b18c66e816645b385b2f9a544"
    ],
    "message_list": [
      "add --force option for verdi code delete\n\nverdi code delete was preventing users from deleting codes with\nassociated calculations.\nWhile it's good to warn users, preventing them from doing it entirely\nseems unnecessary, also given that they could simply use verdi node\ndelete (which most users won't realize).\n\nThus adding a --force option that pipes `verdi code delete` through to\n`verdi node delete`.",
      "Merge branch 'develop' into issue_3545_code_delete",
      "simply pipe through to verdi node delete\n\n * pipe \"verdi code delete\" through to the same function\n   used in \"verdi node delete\"\n * fix migration warning for newer DB version",
      "Merge branch 'develop' into issue_3545_code_delete"
    ]
  },
  {
    "pr_number": 4446,
    "commits_list": [
      "bb888253024e68c1ee53cfcc9ea9f5f2be591a93",
      "7bf40eb5006aae292f97a4fd3187798f04289781"
    ],
    "message_list": [
      "\ud83d\udc4c Improve archive export  code\n\nThis PR is part of a set that aims to decouple the generation of data to export, from the actual writing of the archive; as a prelude to introducing a new archive format.\n\nThis particular PR does not introduce any breaking changes, it:\n\n- splits the amazingly complex `export_tree` function into a number of separate, self-contained, functions\n- adds mypy compliant typing\n- fixes an bug whereby, in python 3.8, `logging.disable(level=` has changed to `logging.disable(lvl=`",
      "Merge branch 'develop' into export/refactor"
    ]
  },
  {
    "pr_number": 4169,
    "commits_list": [
      "d3b49e58d23845775b220ebcaefaa934425e4ce9",
      "817844d7cefdf5edffd31aea0fa25234ac7eeefb"
    ],
    "message_list": [
      "Simplify `ArithmeticAddCalculation/ArithmeticAddParser` implementation\n\nThe purpose of these basic `CalcJob` and `Parser` plugins is for easy\nintegration testing, but also to serve as an example for the\ndocumentation and tutorials on how to implement these plugin types.\n\nTo prevent code duplication and make sure that the code that is\npresented in the documentation is actually tested, we want to use this\nactual implementation there by literal-including it. This means that the\ncode has to be as simple and minimal as is possible, within reason and\nby maintaining correctness for testing purposes.\n\nThat is why we get rid of unnecessary utility methods and try to\nimplement the code as straightforward as possible. Since the code is\nsimplified it fits well into a single method anyway.",
      "Update tutorial and how-to guides to literal-include for code snippets\n\nThe basic tutorial and how-to guides on implementing `CalcJob` and\n`Parser` plugins, as well as how-to write work functions and work chains\nhave been updated to `literal-include` the implementation of the actual\nplugins that ship with `aiida-core`.\n\nThis has the advantage that the code that is shown in these crucial\nparts of the documentation is guaranteed to work with each release as\nthe plugins themselves are tested in the unit test suite. The down side,\nhowever, is that we do not always want to show everything from the\nentire implementation at any point, so we have to resort to specifying\nline numbers.\n\nThis is fine, except that it runs the risk that the line numbers run out\nof sync if the implementation is changed and automatically detecting\nthis is difficult. Nonetheless, the other solution of just duplicating\nthe code in the documentation has the same problem, and in addition the\ncode is not easily tested. Tests *could* be added, but given that\ntesting these plugins is not trivial, the required additional code and\ninfrastructure would be more of a burden than the current solution.\n\nFinally, by using the `:pyobject:`, `:start-after:` and `:end-before:`\noptions as much as possible, depending on explicit line numbers is\nalso reduced. Currently, only a snippet in `howto/codes.rst` on exit\ncodes in parsers relies on explicit line numbers. That is why the\nimplementation in `aiida.parsers.plugins.arithmetic.add` has a comment\nwith a warning that if edited, the snippet in the documentation should\nbe checked for consistency."
    ]
  },
  {
    "pr_number": 4999,
    "commits_list": [
      "70134f0a59c5da621c4370d75f6decdc00f77bf6",
      "7e8a060ed73688061eaf9da01fb58cfadde87d45",
      "07c88b55d9840c65ddd9fdb5d2269e8c6145026e",
      "af07dc3be465c3d96afba22df0f7e40f31c2b443",
      "2d5b190534de7ba46c507c8938841417cfda1fdc",
      "4648beea84b1e93cbe0eac985cad1171fd0fcb02",
      "ff3a7ab9cfe47322b6bc0e3e517761de61493eb7"
    ],
    "message_list": [
      "Make `verdi group` messages consistent",
      "Fix test that relies on messages",
      "Update __str__ for Group and verdi group messages\n\nRemove mention of user.\nUse {group} directly in verdi group click messages.",
      "Store str(group) prior to deletion for verdi msg",
      "Setup __repr__ to work with eval()\n\nAlso, update __str__ to \"Group<LABEL>\".",
      "Revert __repr__ to its original output",
      "Merge branch 'develop' into close_4998_verdi-group-messages"
    ]
  },
  {
    "pr_number": 5153,
    "commits_list": [
      "e89fc3455bb2ca60f12ecf39acd48bccdec2cfc1",
      "b85afd4cd2f39368d5a370f1a0c0504afcf6412e",
      "3e8b30f6db7ff2c10cd594d6661ae54948f2ac74"
    ],
    "message_list": [
      "Updated LSF scheduler to accept number of nodes",
      "Fix and add tests",
      "Merge branch 'main' into lsf_update"
    ]
  },
  {
    "pr_number": 4275,
    "commits_list": [
      "135caf9e7a9d0128f055e6f6125666551ff5d0d9",
      "4c5c04a93e0327bc0f4de55c781eae87ca1644e7"
    ],
    "message_list": [
      "Fix bug in `aiida.engine.daemon.execmanager.retrieve_files_from_list`\n\nThe `retrieve_files_from_list` would loop over the instructions of the\n`retrieve_list` attribute of a calculation job and for each entry define\nthe variables `remote_names` and `local_names` which contain the\nfilenames of the remote files that are to be retrieved and with what\nname locally.\n\nHowever, for the code path where the element of `retrieve_list` is a\nlist or tuple and the first element contains no wildcard characters, the\n`remote_names` variable is not defined, meaning that the value of the\nprevious iteration would be used. This was never detected because this\ncode path was not actually tested.\n\nThis bug would only affect `CalcJob`s that specified a `retrieve_list`\nthat contained an entry of the form:\n\n    ['some/path', 'some/path', 0]\n\nwhere the entry is a list and the first element does not contain a\nwildcard.",
      "Merge branch 'develop' into fix/4273/retrieve-files-from-list"
    ]
  },
  {
    "pr_number": 3713,
    "commits_list": [
      "5e470acacc36931e07aa5aff6140f1a1486d6b21",
      "0c8a448dbf987f708f2e76dee623c9475a142027"
    ],
    "message_list": [
      "limit batch size for bulk_create operations\n\npostgresql has a \"MaxAllocSize\" that defaults to 1 GB [1].\nIf you try to insert more than that in one go (e.g. during import of a\nlarge AiiDA export file), you encounter the error\n\n    psycopg2.errors.ProgramLimitExceeded: out of memory\n    DETAIL:  Cannot enlarge string buffer containing 0 bytes by 1257443654 more bytes.\n\nThis commit avoids this issue (for the django backend) by setting a\nbatch size for bulk_create operations (via a verdi config option).\n\n[1] https://github.com/postgres/postgres/blob/master/src/include/utils/memutils.h#L40\nmax alloc\" size",
      "Merge branch 'develop' into issue_3712_max_postgres"
    ]
  },
  {
    "pr_number": 624,
    "commits_list": [
      "f887cb62be59d7b01ca360a1604642298540bb74",
      "05f7c608e417950d0941cdcc38012897a0efbbaf",
      "89668d718f6b849477a5fe96907236b36f457205",
      "ee6e2bbc167cf4ffad230e502811954d6f1a4058"
    ],
    "message_list": [
      "Import JobCalculation from aiida.orm instead of aiida.orm.calculation.job",
      "De-duplicate JobCalculation class, move get_desc to AbstractJobCalculation",
      "Merge branch 'develop' of github.com:aiidateam/aiida_core into import_jobcalculation_from_orm",
      "Merge branch 'develop' into import_jobcalculation_from_orm"
    ]
  },
  {
    "pr_number": 3799,
    "commits_list": [
      "7c718030ae60f7306802132e41b3e8460226f3be",
      "ab545c8ca2c1e53dcf9518aabae2f9a9ebebfff5",
      "6ce67f68f9746a4e98fb681387c071e349d163a1",
      "141f1b471901391aad9d797b4334d50b2a6612d0",
      "adc92bd6e7e708e1028cbba077c994abb4f8fd3a"
    ],
    "message_list": [
      "extend plugin design guidelines\n\nupdate plugin design guidelines with suggestions on how to think about which information to store where.",
      "apply plugin (package) naming convention\n\nApply naming convention for AiiDA plugins (=extensions of aiida-core)\nand AiiDA plugin packages (=bundles of plugins, for distribution).\nAlso update several sections related to migration from older AiiDA\nversions & plugin development in general.",
      "incorporate suggestions by @sphuber",
      "Merge branch 'develop' into update-plugin-guidelines",
      "Apply suggestions from code review\n\nCo-Authored-By: Sebastiaan Huber <mail@sphuber.net>"
    ]
  },
  {
    "pr_number": 3951,
    "commits_list": [
      "97ff09620cea68fee9060f85979a5c94cc8b6760",
      "972683fbbf06da11897add2c8838656b128e2eb4",
      "d9cad462a1dc7db58faced8e37b6adeb4ad8dd8b"
    ],
    "message_list": [
      "reintroduce CORS headers to REST API\n\nDue to a recently introduced bug, the CORS headers sent by the REST API\ndid not pertain to the correct prefix but to a python module path.\n\nAlso, there was a leftover print statement in the HOSTNAME CLI\nparameter, introduced in another PR (shame on me!).",
      "add tests\n\n * add test for CORS header to prevent issue from reoccuring\n * add test for /server endpoint (formerly untested)\n * add configure_api method\n * deprecate dual-use of run_api method (run_api was used both to\nconfigure the app and to run it)",
      "address comments by @sphuber"
    ]
  },
  {
    "pr_number": 2048,
    "commits_list": [
      "9836a9c246ec752b96b81c3dae17ca3624a9c76c",
      "9174bca8a0177209ad3168af79ee2cb94c050ee8",
      "30b4d4e4cea1e42fb913cd3a33193a579e73a5a5",
      "bc99a0e21fe9f530846809ee0b409e498a4d2d6d",
      "90caa8926aa037c03a42b98f919a72e78536bb41",
      "ff5010683abc78f47d66a9053c2072dbc80a56d2",
      "976e770baba6130f283fe8c3c57e9b012606cbb5",
      "390deec5837884dbfceda8211c74ce11944f226e",
      "b927d42f642fab3be7cb1d26479fa6243f6e106e",
      "3316ebcc70f81ae3683a47f37b2162d15498e538",
      "553003e4b28fe675ea3bc7bf944803cde035b46f",
      "8fa5953d0c5bbf92625922e4f8213fbacfab43e9",
      "85268c54d487bd90542d475bd8054fef38557b85",
      "e5c8162bc174ee07db7f126d7c8d54dec2995aec",
      "09be62c5328a372a4e4c8a78fc7a5bdb3ac8416f"
    ],
    "message_list": [
      "fix bug in load_dbenv()\n\nFixes #1966\nThe if_dbenv_loaded() function was always returning True\nif it is was called right after load_dbenv(). The problem was in\nload_dbenv() function which was setting LOAD_DBENV_CALLED variable\nto True before actually doing something. Consequently, for the\ncase where a connection to the database was not setup the call of\nif_dbenv_loaded() was returning a wrong True result.",
      "Merge branch 'develop' of https://github.com/aiidateam/aiida_core into develop",
      "Fix pre and post execution scrip parser\n\nPreviously, lines containing '#' symbol where generating parsing errors\nas well as the lines starting with '#='. Now it should be fixed: lines\nstarting with #= are completely ignored, while presence of '#' symbols\ndoes not cause any errors. Fixes the issue #1965",
      "test_multiline: modify lines not containing '#'\n\nvim adds \"Test\" word at the end of every line not containing '#'\ncharacter.",
      "edit_pre_post(): add error handling + 2 more tests\n\n- Handle errors related to modification/multiple inclusion of the pre/post separator\n- Add two more tests for:\n\t-- ignoring the lines starting with '#=' characters\n\t-- NOT deleting the lines containing # character\n\t   (BASH comments should be kept)",
      "Catch InputValidationError\n\nCatch InputValidationError whenever ensure_scripts function is called.\n\n+ fix some typos in the output text",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "acknowledge MARVEL and MaX",
      "README.rst -> README.md",
      "Update README.md to properly show the images",
      "Change README.md to properly show the following:\n\n1) travis\r\n2) coveralls\r\n3) readthedocs\r\n4) badge.fury",
      "Make the following links appear properly:\n\n1) MARVEL\r\n2) SNSF\r\n3) MaX",
      "setup.py: README.rst -> README.md",
      "Correct badges links",
      "Update the placeholders to something meaningful"
    ]
  },
  {
    "pr_number": 5149,
    "commits_list": [
      "d5591330e1f452873df64994617ee934cb6763ce",
      "95ec9883467c3c18e5a13a65b749b552330d91ec"
    ],
    "message_list": [
      "`BaseRestartWorkChain`: clearify the log report when output ports not compatible\n\nThe `BaseRestartWorkChain` output ports required are reported to missing\nin two conditions.\n1) when the nested `CalcJob` do not correctly giving the outputs.\n2) when the base work chain does not correctly expose the output ports,\nwhich is not considered here. It causes issue https://github.com/aiidateam/aiida-quantumespresso/issues/723\nThese shold be two different scenario but we report at one log.\n Would be better to separate this two situation but that will complicate the code logic here\n(need to check the actual outputs of embed process and then raise for the first half report),\nI don't think it worth to do for this.",
      "Merge branch 'develop' into improve-restart-wc-result-report"
    ]
  },
  {
    "pr_number": 2492,
    "commits_list": [
      "f0a2e9e2564d90f7888f72ffe9fa52d44b43b910",
      "83df91d060136db6aff04792526bf141e6931333",
      "bc56dd6068f5926a19e104e00b15d4048a649614"
    ],
    "message_list": [
      "fix problem with empty labels array",
      "Fix bands plotting\n\nIf the first and/or the last kpoint label does not point to the first and/or\nthe last kpoint - the plotted band structure was cut on the left and/or right\nsides. To avoid this I add empty labels to the first and/or the last kpoints\nif they are not labeled.",
      "Add documentation on how to use the BandsData object"
    ]
  },
  {
    "pr_number": 2838,
    "commits_list": [
      "77cf654e0edfd264ae6b3a5d52d687b69b99f264",
      "e101c2409d57239f3284e02ecdcc3a6244513e5e"
    ],
    "message_list": [
      "BandsData: fix _prepare_gnuplot() and matplotlib_header_template\n\nfixes #2756\n\n* _prepare_gnuplot():\nRemove raw_data.encode('utf-8') call that tries to convert\npreviously converted bytes object\n\n* matplotlib_header_template:\nremove `from __future__ import print_statement`",
      "fix: verdi data bands show"
    ]
  },
  {
    "pr_number": 3685,
    "commits_list": [
      "0f20908b8aa36c394765c2e1364440af682ede38"
    ],
    "message_list": [
      "Consider 'AIIDA_TEST_PROFILE' in 'get_test_backend_name'.\n\nChange the logic of 'get_test_backend_name' to check (if given)\nthe backend set in the configuration for the profile specified\nin 'AIIDA_TEST_PROFILE'. If a backend is also specified in\n'AIIDA_TEST_BACKEND', it checks that the two match, raising\nValueError otherwise. If neither is specified, fall back to the\ndefault django backend."
    ]
  },
  {
    "pr_number": 4412,
    "commits_list": [
      "289d4b7dae77ae057d0e0d3efbc5efa8802ea3e6",
      "f7ff9ddd088374498f54252d63a71829977f979f",
      "e06c54cebc12a234e53b8a49722fa64ed68c37b6",
      "f47e2981e13ba03b93dd51fb93990800de40cb87"
    ],
    "message_list": [
      "REST API: list endpoints at base URL\n\nThe base URL of the REST API was returning a 404 invalid URL response\nwithout providing any guidance to new users as to how to use the API.\nWe change this to return the list of endpoints formerly available only\nunder /server/endpoints.\n\nDocumentation of where to find the list of endpoints -- which seems\nto have been entirely deleted -- is added.",
      "add test",
      "Merge branch 'develop' into issue_3916_rest_api_base_url",
      "Merge branch 'develop' into issue_3916_rest_api_base_url"
    ]
  },
  {
    "pr_number": 3625,
    "commits_list": [
      "4f8b8a7464dbb4901acf487ba33db56bad9d3c28",
      "3f1f6fc4fb5e1aa5642b2ae5b623f42c15f72078",
      "5b5f6b8cfa04a964730a8c2526904e82f6ec71b5",
      "ca0f9dc74be195c2fe11e22e14a2c37ece6fb88c",
      "07d5e2f8507d45356199b13ebe2d6ffac669d56e",
      "8e60649e8fdf50dcf564208a0da6f2f67d71135e",
      "ab9df714880d367d4aa89cef1c3a6fe390a0b92d",
      "16a9bc417d50a18ff2f73e3a0a01c20cea93c207",
      "f865d48c0d141664734ff19d09f2a1afe8c5bf4b",
      "d973aaf82360b54fc92072d5e67792cbb2b29d12",
      "d86ef1e3afb4e05ced64408d66439e2915a956a3",
      "b07fc4f8a25fec814d56d73d8822150983c6abd8",
      "03a81e50a72f4470e76b9a2baa6055721c84a195",
      "fc42510333edb3aced6c5ada4e3ebacd3c7f3e73",
      "f0521148f0667daed7f660d11e64eb750b0f4c2e",
      "68fb6cca484b18fb579a914ffe9b963202f02846",
      "2c77f24d722de247c4cd76f348098b9728fa7e2d",
      "582ea90e49a1168e34bc9a2d6ea69388a1382a01",
      "3a7ef925c819be9345ab250d68b022b6198b6fef",
      "cd6aa2002d6be8457f30f1e0befeebd6d491e638",
      "35cc2c589eead8c415889da40c3e5a3fcf077d4e",
      "ca3274bcb6d2c63a11a2bfc09fe7a2b409ee6c00",
      "799d2d2a7e7a027c24b68b0cbac9c32a53320efa"
    ],
    "message_list": [
      "Remove 52 files from .pre-commit black list",
      "Delete aiida/backends/tests/cmdline/commands/test_computer.py",
      "fix pre-commit",
      "rm python_2_unicode_compatible from all files",
      "Update aiida/backends/general/abstractqueries.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "modify aiida/backends/general/abstractqueries.py",
      "Update aiida/backends/sqlalchemy/models/settings.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/sqlalchemy/models/settings.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/sqlalchemy/models/user.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/sqlalchemy/models/user.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/sqlalchemy/tests/test_generic.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/sqlalchemy/tests/test_generic.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "fix aiida/backends/tests/cmdline/commands/test_user.py",
      "Update aiida/backends/tests/engine/test_class_loader.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/tests/engine/test_persistence.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/tests/engine/test_process.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/tests/engine/test_runners.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/tests/engine/test_runners.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/tests/engine/test_utils.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/tests/orm/utils/test_loaders.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/tests/orm/utils/test_loaders.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Update aiida/backends/tests/test_base_dataclasses.py\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "modify aiida/backends/tests/test_dataclasses.py"
    ]
  },
  {
    "pr_number": 4458,
    "commits_list": [
      "a5ab25f191d06889e25517eabc3476b1b03b6ffa",
      "82493598473ae3fdc67b00a36fc4cf4f1c4bd501",
      "e2cf8bcaa05f4fb688a47b0042e1d15863211884"
    ],
    "message_list": [
      "Docs: add \"How to share data\"\n\nThe \"how to share data\" section includes instructions both for dealing\nwith AiiDA archives (e.g. for publishing AiiDA graphs alongside your\npublication) and for using the AiiDA REST API.\n\nCo-authored-by: Casper Welzel Andersen <43357585+CasperWA@users.noreply.github.com>",
      "replace all occurences of \"export file\"\n\nWe have agreed on the terms \"AiiDA archive (file)\" and \"AiiDA archive\nformat\".\n\nCo-authored-by: Casper Welzel Andersen <43357585+CasperWA@users.noreply.github.com>",
      "Merge branch 'develop' into issue_4432_how_to_publish_data"
    ]
  },
  {
    "pr_number": 4908,
    "commits_list": [
      "8ea1c8c6dbddaf07f0ee423793a3ad4c42064c34"
    ],
    "message_list": [
      "CLI: set `localhost` as default for database hostname in `verdi setup`\n\nThe default was actually being defined on the option, however, it was\ntaken from the `pgsu.DEFAULT_DSN` dictionary, which defines the database\nhostname to be `None`. Still, 9 out of 10 times the database is on the\nlocalhost so not having this as a default is kind of annoying and\nunnecessary."
    ]
  },
  {
    "pr_number": 4657,
    "commits_list": [
      "f02e549ee08ba2cfe957c5cbc38134af71861cb1"
    ],
    "message_list": [
      "docs: clarify difference between jobinfos\n\nCalcJobNodes contain two differente job infos, the detailed_job_info and\nthe last_job_info.\n\nThe distinction between the two was not obvious, and not documented."
    ]
  },
  {
    "pr_number": 4681,
    "commits_list": [
      "095d38c540d7e43ad2de68295c4b436f48600464",
      "6dc30d5211d7875ea6e1b61dab47aa98be56f246",
      "8bd942ae6b036e820bb136b45e4ff5f511238b5a"
    ],
    "message_list": [
      "remove leftover use of Computer.name\n\nRemove leftover use of deprecated Computer.name attribute in `verdi\ncomputer list`.",
      "update click version\n\nclick 7.1 introduces additional whitespace in verdi autodocs",
      "Merge branch 'develop' into fix-computer-name-use"
    ]
  },
  {
    "pr_number": 4686,
    "commits_list": [
      "c30d4aa93a67526649790dd6aa7d1efa4de95eb7"
    ],
    "message_list": [
      "ci: update paramiko version\n\nNow that the Github Action runners switched to 20.04, the default SSH\nkey format of OpenSSH changed and is no longer supported by paramiko\n<=2.7.1."
    ]
  },
  {
    "pr_number": 1977,
    "commits_list": [
      "4c1a2407e943c7612d493d559835bb230e12012b",
      "45b1d62dba9b54503a5c0fb72a5f76eed45edece",
      "0fc4e6593bd3637fe42a6d461d9230378d0a54d9",
      "6cb8209c0166c7e9256b0fc3bc2b9447f89b11ee",
      "76d31874edb6d4c5691d1d1d12ecd3a298b4ca0a",
      "3e0b856612be793e69cf456c7a2b63af7dbd7444",
      "d98cee3584a2b428fc64acb24e99aa34588691d6",
      "9a3ed8abb7ba0b8ac67e171d69ddedbacd9d87de",
      "402f5ca922f7322127ea42fdf78d85539266d11a"
    ],
    "message_list": [
      "Making Coveralls builds parallel",
      "Merge branch 'develop' into fix_1905_parallel_coveralls",
      "Shorten line length",
      "Running coveralls in all travis runs",
      "Triggering another build",
      "Adding a missing dash to the travis YAML",
      "Adding a final notification webhook to get the report back\n\nSee discussion here:\nhttps://github.com/lemurheavy/coveralls-public/issues/1139",
      "Merge branch 'develop' into fix_1905_parallel_coveralls",
      "Merge branch 'develop' into fix_1905_parallel_coveralls"
    ]
  },
  {
    "pr_number": 3224,
    "commits_list": [
      "a76f0a360442f122f2692ff3d46c89d05eaaab66"
    ],
    "message_list": [
      "Make sure all CLI helps are consistent.\n\nFixes #3196\n\nThis makes all commands conform to PEP 257\n(in particular dot at the end of the first line)\nso that they are properly displayed by click.\n\nI have also used the occasion to put a confirmation\nprompt in `verdi rehash` as I realised it would\njust rehash without asking. I've added also a force\noption to avoid questions asked (e.g. for tests).\n\nI have noticed also that `verdi comment add` and\n`verdi comment delete` did not have some arguments\nmarked as required, but then the command would fail.\nSince not passing the argument does not make much sense,\nI have made those required.\n\nFinally, I have moved the `verdi rehash`, `verdi graph`\nand `verdi comment` inside `verdi node`, but left them\nthere with a deprecation warning to be backwards-compatible.\n\nTests have been added for the actual `verdi node xxx` commands,\nand the old ones left in there to make sure also the deprecated\ncommands still work. These tests will be removed when the\ndeprecated commands will be removed."
    ]
  },
  {
    "pr_number": 5103,
    "commits_list": [
      "a8b32659126b5f9f85927c8b44c7f49dd4b0fffe",
      "3313c10db5925881035162e7ee4f25e9a161d719"
    ],
    "message_list": [
      "\u2b06\ufe0f UPGRADE: sqlalchemy v1.4 (v1 API)",
      "Update aiida/orm/implementation/sqlalchemy/utils.py"
    ]
  },
  {
    "pr_number": 1951,
    "commits_list": [
      "a618575a8dceba3b409aa7d19ae185dee1dd043d"
    ],
    "message_list": [
      "Ported Computer and QueryMangaer to new backend interface\n\nNow computer is created using backend.computer.create(...)\n\nwhile the query manager for a particular backend can be accessed via\nbackend.query_manager\n\nThis brings things into line with User, AuthInfo and LogEntry."
    ]
  },
  {
    "pr_number": 5207,
    "commits_list": [
      "f8a67535068456f38d9026f2cb6a4df9eb2f888d"
    ],
    "message_list": [
      "`Node`: add the `is_valid_cache` setter property\n\nThis property allows to mark an individual node as invalid for use as a\ncaching source. It works by setting the `_aiida_valid_cache` extra to\neither `True` or `False`.\n\nPreviously, the unofficial way of ensuring a node was no longer used as\na cache source, was to remove the `_aiida_hash` extra, containing the\nhash of the node. This would achieve the desired effect, since without a\nhash, the node would never be found when looking for identical nodes to\ncache from. However, this not an ideal approach because the hash may be\nuseful for other purposes. In addition, if the node were to be rehashed\nat some point, which could happen by accident if all nodes of a\nparticular class got rehashed, it would all of a sudden be used in\ncaching again.\n\nThe solution is to have a way to persistently mark node instances as\ninvalid for caching. Note that the `is_valid_cache` property of the\n`Node` before was intended as a hook to allow changing the caching\nbehavior of subclasses of `Node` as a whole, and not on the instance\nlevel. This is now changed as the property has a setter that operates on\nthe instance level. Subclasses should take care to respect this base\nclass implementation if it makes sense. Since `Node` can only be\nsubclassed by `aiida-core` and not in plugins, we can easily enforce\nthis system."
    ]
  },
  {
    "pr_number": 5387,
    "commits_list": [
      "9f029569558862846859429f3716fc9419f492fa"
    ],
    "message_list": [
      "Move `aiida.cmdline.utils.common.get_database_summary` to storage backend\n\nThis utility was used in `verdi archive inspect` and `verdi storage info`\nto give information about the contents of an archive or normal storage\nbackend. Historically, a distinction used to be made between the contents\nof the database and the repository, but in v2.0 these are unified by the\nstorage backend. The information about the repository is already\nretrieved through `StorageBackend.get_info` but the contents of the\ndatabase were still built through the `get_database_summary` function.\n\nThe implementation is moved to the `get_orm_entity_overview` method of\nthe `StorageBackend` base class. This is done because the implementation\nis currently still independent of the storage backend implementaiton. So\nfor now the implementations can simply call to this method in their\n`get_info` implementation, but it leaves the option to override it with\na more performant implementation that doesn't go through the shared ORM\nof AiiDA."
    ]
  },
  {
    "pr_number": 5431,
    "commits_list": [
      "1388bd38c25e8e22c9ef3e2e74a3d21c247cee48",
      "e42ffe056efbd07b7f9f50cd2257101abbe14805"
    ],
    "message_list": [
      "Docs: improve the backup section for v2.0\n\nWith AiiDA v2.0 the repository is now provided by the `disk-objectstore`\nwhich is optimized for being backed up with `rsync`. This means the old\ncustom implementation of backing it up is no longer necessary nor\nsupported. It can now simply be backed up using `rsync`.\n\nThe instructions are updated and improved by tying the backup procedure\nto the storage backend used for the profile. This will allow to have\nother instructions for other storage backends in the future, as they\nwill most likely not be the same.",
      "Merge branch 'develop' into fix/4853/docs-backup"
    ]
  },
  {
    "pr_number": 5397,
    "commits_list": [
      "9d4de930da74e4b81d9ce3e30eec9f2e8a20db5e",
      "41470e64cc8e492dd9de2fd17785476fd483bd47",
      "f3a9f8693353d42ebbd67fc262b050b69e518d31"
    ],
    "message_list": [
      "fix: delete configured computers\n\nDeleting a configured computer resulted in an error, since the\nassociated Authinfo object was not deleted (instead, the \"dbcomputer_id\"\nwas set to null, which violated a not-null constraint).\n\nDeleting computers was already tested, but not deleting a configured\ncomputer. This is fixed as well.",
      "add test",
      "Merge branch 'develop' into issue-5396-computer-delete"
    ]
  },
  {
    "pr_number": 5087,
    "commits_list": [
      "ea111d8703481ffd4ea7c8dbd6a255ba3d28b6a0",
      "9ce29cc524bcf4bd2e8b52298dda82cd67226d41",
      "5b01efb77c821966c51ca5a0ee517cd6bf6893eb",
      "0540ffde6a8fafc9b66968bafc7fe6a8a88b1875",
      "9fba371eb6efaf6116342e3e9e40913e32398112",
      "74c229c5fc8dba6f2dcada2c38162945bae4b4dd",
      "1489e09a2bfdb7533000d020b1a81ed8e2775702",
      "a97b9d2ac064c0b2027cb9e7bb6701b3c409379b",
      "9900527ff9064eefd062669438063f4fe6d431ee",
      "f4cda2ea3b5d2a44526e8c40cb4ae117d6c5cc6d",
      "c58cf687e2010e38345920588c401d737994588e",
      "af4f54acdb1562c86a2b7f4c9b8136e2821e6f52"
    ],
    "message_list": [
      "\ud83d\udd27 MAINTAIN: Update pylint version\n\nUpdate pylint to v2.9 and remove restrictions on astroid and pylint-django.\n\n- `django-not-available` -> `django-not-configured`\n- Fix new `consider-using-dict-items`, `consider-using-enumerate`, `too-many-function-args`, `consider-using-from-import`, `consider-using-max-builtin`, `arguments-renamed`\n- extra disables required for `no-member`\n- disable `unsubscriptable-object` for cif.vaues\n\ncloses #4647",
      "enable `consider-using-with`",
      "enable `use-maxsplit-arg`",
      "enable `use-a-generator`",
      "enable `deprecated-decorator`",
      "fix abstract setters",
      "Update aiida/cmdline/params/types/path.py",
      "move   # pylint: disable=unsubscriptable-object",
      "Update data.py",
      "fix pre-commit",
      "Update lsf.py",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci"
    ]
  },
  {
    "pr_number": 4816,
    "commits_list": [
      "7a0402ab1da5db4dd7a1653630854f43f845dbcd",
      "e32bf079fc7723a7bf8eb6f779da45b8fe7fd7fe",
      "e4d7a1ef52601c94400f6159c2532e7e235a3cf7",
      "9edf6665a2e4a3b7c71ce263ed23229f6d58183c",
      "36cd45435de1163027a881ad953a1f3c719b0175",
      "96948e42977540ed433f55fc7a44c5970d4d5dfd",
      "a9c263f0f834ebd37d3e8ba40a31190bf7ddc7d3",
      "2a066a545bde30e9f5f9dc9545b3450aa20e3ebf",
      "789eaf2330f3af02a2f505ef5f672aa6cbe99f7d",
      "80aff5ababe66b75b76c2c06685447e35dff4d3a"
    ],
    "message_list": [
      "\ud83d\ude80 RELEASE: v1.6.0",
      "Add PR links",
      "Update CHANGELOG.md",
      "Merge branch 'develop' into release/v1.6.0",
      "fix header",
      "fix heading",
      "review suggestions",
      "final review suggestions",
      "Apply suggestions from code review\n\nCo-authored-by: ramirezfranciscof <ramirezfranciscof@users.noreply.github.com>",
      "Apply suggestions from code review\n\nCo-authored-by: ramirezfranciscof <ramirezfranciscof@users.noreply.github.com>"
    ]
  },
  {
    "pr_number": 4616,
    "commits_list": [
      "d489fee8c0f292a6c0f4221f5f3e3d36f7c079c0"
    ],
    "message_list": [
      "Fix wrong raise from dict nodes\n\nWhen accessing an attribute as a dict (`dictnode['key']`), the error\nraised was an AttributeError instead of a KeyError, which is rather\nunusual. This has been fixed and a test that checks the correct error\nraise has been added."
    ]
  },
  {
    "pr_number": 4469,
    "commits_list": [
      "3087bb9c17a816e0397814fac3828e1c1784c949",
      "8f0fbd4d6195b75e5cbb8d399b7a7b561e093485"
    ],
    "message_list": [
      "Docs: Move data types to Topics section",
      "Add suggestions from code review"
    ]
  },
  {
    "pr_number": 5568,
    "commits_list": [
      "573734c504482be153786ec12f1862c79d68a66b",
      "3ee1aa7dc8de4a91bfe7bbc34ab330280fc7d24f",
      "81636ff2753777a7271f7f40cddcdbaa1f26145a"
    ],
    "message_list": [
      "feat: `verdi process status --max-depth`\n\nAiiDA workflows can get very complex with many layers, leading `verdi\nprocess status` to print hundreds of lines of text.\n\nThe `--max-depth` allows the user to \"zoom out\" and look at the bigger\npicture, while still being able to \"zoom in\" on subworkflows of interest\nby calling `verdi process status` on those.",
      "add test for max-depth=0",
      "revise code logic for Jason"
    ]
  },
  {
    "pr_number": 4112,
    "commits_list": [
      "03edb0cdd70acd26215834526ac943859ff5afee",
      "86258e3a3eab6f6bd459040a7a9c5cb2729d5bfb",
      "b8e5a1b1d987539cb73ed02bfc6e2858b0cce281",
      "0c831688db21dfafaa029a592e4b51aa76098491"
    ],
    "message_list": [
      "Docs: Add HowTo on writing workflows",
      "Apply suggestions from code review\n\nCo-authored-by: Sebastiaan Huber <mail@sphuber.net>\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Apply reviewer suggestions part two",
      "Add warning to not use global submit"
    ]
  },
  {
    "pr_number": 4508,
    "commits_list": [
      "7b31fad0822df567755b8e7e6db0398970d08fd3",
      "a4b71a254ac4bf7d55f36c82af0fe6f5573b3fc9"
    ],
    "message_list": [
      "Docs: Add `verdi process status`  command to tutorial",
      "Update docs/source/intro/tutorial.rst\n\nCo-authored-by: Leopold Talirz <leopold.talirz@gmail.com>"
    ]
  },
  {
    "pr_number": 5871,
    "commits_list": [
      "3e18a394d5ae997506f830edac2dc7c5fafaebd7",
      "f6119ade23ac9e42a39c4c19e8b40dc8b72e8d59"
    ],
    "message_list": [
      "fix: limit sqlalchemy<1.4.45\n\nsqla 1.4.45 introduced a breaking change resulting in two test failures\n\nFAILED tests/orm/test_querybuilder.py::TestRepresentations::test_as_sql_literal_quote - sqlalchemy.exc.CompileError: No literal value renderer is available for literal value \"['Si']\" with datatype JSONB\nFAILED tests/orm/test_querybuilder.py::test_analyze_query - sqlalchemy.exc.CompileError: No literal value renderer is available for literal value \"['x', 1]\" with datatype JSONB",
      "fix pre-commit on CI\n\nmy local yapf did not pick up on this..."
    ]
  },
  {
    "pr_number": 4901,
    "commits_list": [
      "70c05051aa4680a222946c95b239237041c3c856"
    ],
    "message_list": [
      "`Config`: add the `delete_profile` method\n\nThe class already has `remove_profile` that merely removes the profile\nfrom the index and doesn't actually delete its associated resources. The\nnrem `delete_profile` method will by default also delete the repository\nand database that are configured for the profile, although this behavior\ncan be controlled with the `include_database` and `include_repository`\narguments.\n\nThe implementation of `verdi profile delete` is update to use this new\nfunctionality instead of the loose functions that were defined in the\n`aiida.manage.configuration.setup` module. The latter, since it was not\npart of the public API and was only used by this CLI command, has been\nremoved from the codebase.\n\nNote that the implementation for the deletion of the database is\nslightly different. Not only does `Config.delete_profile` not contain\nany echoing of information, as this should be done in the CLI command\nand not in the generic API, the logic for deleting the database user\nhas also changed. The old function `delete_database` did delete the\ndatabase user configured for the profile, as long as no other profiles\nin the config used the same. This is dangerous, however, because the\nsame database user can be used by a program outside of AiiDA and we\ndon't own those users necessarily. Now, the deletion of the database\nuser is optional and turned off by default. When it is used though, the\ndatabase user is deleted without checking the config for other profiles\nthat may also have configured it."
    ]
  },
  {
    "pr_number": 4825,
    "commits_list": [
      "0e4b44ab4569692d0b05aa1c0c29d2e234a9144b",
      "af03861d268fb9ed39e8a6172273f832281914f1",
      "e9a32a70c07f7e471aec96fed9cb8a4ce5fc55f0",
      "cdb0329c59c4c6bde806b52c72e96c6b77039272",
      "e2c046c9c00877543283a3ed74339c610bcbf69a"
    ],
    "message_list": [
      "scheduler memory updates",
      "Merge branch 'develop' into scheduler-mem-fix",
      "warning instead of info",
      "bonus bugfix: wrong variable name used",
      "Merge branch 'develop' into scheduler-mem-fix"
    ]
  },
  {
    "pr_number": 3910,
    "commits_list": [
      "08f38d04c9ff940f31ab24f2354762879dd29c7e"
    ],
    "message_list": [
      "Add the `-v/--version` option to `verdi export migrate`\n\nThe default behavior remains the same and if not specified the export\narchive will be migrated to the latest version. However, with the flag\nany other version can be chosen, as long as it constitutes a forward\nmigration as backward migrations are not supported."
    ]
  },
  {
    "pr_number": 5312,
    "commits_list": [
      "afd74c7dc5e92796357f3d90cfeab94667717ade",
      "63e28f0a2cb66a2b67441d006452e03911d969c2",
      "25c6a2511827903ea941f7ad1f800e32c6eecdd2",
      "5e22916f5c969a8eb60f2402246aa4a35859d0f6",
      "f575508ffdb47fea80e08e29cda847e575a6b396",
      "3eb9224e3a294cf1e7cbc24cc024631c25154d89",
      "5ad00437ae4bf417738ee1a42a1ef90860cd5ab8",
      "b8da4381c1b83c3da8c8c0803ee6e2d1a4127355",
      "2071695839562894f9cc5decb6974c785eb58d4b",
      "8322c98fa474e61ec11f44ce8c757f8598b6de22",
      "409599b5cfbf73de2da7bc9c003ff3eef9294c02",
      "1dccb262a5a015e3a8dc591ac18e1c34122d205d",
      "a025d01ea6c80af2169668bdddb537911adbcc72",
      "a2f57cd1cbd4188356d48c16be6bd252b3f86292"
    ],
    "message_list": [
      "\ud83d\udd27 Move to flit for package build",
      "Update .pre-commit-config.yaml",
      "update dependency checks",
      "Update requirements.txt",
      "Update dependency_management.py",
      "replace occurrences of setup.json",
      "Update release.yml",
      "Merge branch 'develop' into flit",
      "Update pyproject.toml",
      "Merge branch 'develop' into flit",
      "Update release.yml",
      "Update utils/requirements.txt\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Merge branch 'develop' into flit",
      "remove arg from check_release_tag.py"
    ]
  },
  {
    "pr_number": 3598,
    "commits_list": [
      "885e3a4eb116b99f455b6d712be0850bada626fd",
      "47364dac968510fc2a4a38e6b3d53f7306ff86ac",
      "40c133e03147cedc85a606a248c7a9cf0f0cbaac",
      "224a2db5ce8d3f437263ef427a87f01e64058a24",
      "06c75d891615d5fb36a9e0749f49a82ff1e1bffa",
      "b15aef2d1646c4fb2bf021f6558876f44c8e22f9",
      "acc1e750b27e22b86a6eb99ddd7e90c69bc8ed4d",
      "0d863392e5b6f4ac4e04c057b15b5b73c38db0cc"
    ],
    "message_list": [
      "Docs: Add WSL 2 information in pre-requisites",
      "Docs: Update prerequisites",
      "Minor updates",
      "Reintroduce Win RabbitMQ\n\nMove info about WSL 2 to \"For developers\" note.\nHint installing \"Ubuntu\" app, instead of a version specific application.",
      "Apply suggestions from code review by @ltalirz\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>",
      "Remove travis from pre-commit.\n\nRemove special installation pre-requisites for developers in WSL\nconcerning Travis.",
      "Apply suggestions from code review",
      "Update with review from @ltalirz"
    ]
  },
  {
    "pr_number": 967,
    "commits_list": [
      "56dd128a6527b6caf32055e078582e8e4c573492",
      "10bb73ade4c267723795d28990786a8cf6b5edcf"
    ],
    "message_list": [
      "add regression test for #966",
      "fix #966."
    ]
  },
  {
    "pr_number": 1035,
    "commits_list": [
      "17c7e3f232d3916cbe7e0ed50d8f2941cae29e1a",
      "87eda2f3c067bf5981ed3047edff22a09c68ed45"
    ],
    "message_list": [
      "add subcommand completion for data plugin commands",
      "add documentation to completion mechanism additions"
    ]
  },
  {
    "pr_number": 1668,
    "commits_list": [
      "b9c9552364b12d4d24bda2910bb2f4d2aafbbb80",
      "dc286b5a7f5089f2ae98a9ec32093f457c5f76c8",
      "8f7eeb56b76011112199b0e810b9bae733efba9d",
      "7a5554d58c2cb7b32965c545051105fafe7a277e",
      "6e713a6d54167638356ac02ad2e21cd05300afb4",
      "90c8627045c0b47692f34c8c40d1ab3d83cfbcef",
      "59673e636d05fccd7068467d61cb85ced809df48",
      "773b926dc8ada3439e5d71655761f84fd8c6eaa3",
      "4154c45394a0f7e24318d67d0df4fd62390b7c3b"
    ],
    "message_list": [
      "[wip] convert legacy workflows command, no tests",
      "verdi workflow: add tests, pass",
      "verdi workflow: add tests for legacy workflow param type",
      "fix LegacyWorkflowLoader for sqla backend",
      "fix test faillures related to the legacy workflow commands",
      "improve docstrings and replace prints with echos",
      "fix #1671 (sqla Workflow.get_report())",
      "remove unrelated files from linters, fix workflow list",
      "verdi workflow: unify pk formatting for testability"
    ]
  },
  {
    "pr_number": 4048,
    "commits_list": [
      "ee5b182970891ee7134126e7fa951a3f74b01625"
    ],
    "message_list": [
      "Docs: move in section \"Topics - Processes\"\n\nThe content that was envisioned for this section was originally divided\nover the \"Concepts\" and \"Working\" sections, which were itself roughly\ndivided in information on \"Processes\", \"Calculations\" and \"Workflows\".\nThe original idea was to nest the material on calculations and workflows\nwithin the global processes topic, since in the code the former are also\nsub classes of the latter. However, since there are also documentation\non processes in general, I was not able to make this hierarchy work.\n\nInstead, \"Calculations\" and \"Workflows\" are now individul topics on the\nsame level as \"Processes\". Each of these topics are roughly subdivided\nin a \"Concepts\" and \"Usage\" section, where the content from the latter\ncame from the original \"Working\" directory. The concepts explains the\nconcepts of the topic, where the usage really exposes and explains the\nAPI with all the nitty gritty details."
    ]
  },
  {
    "pr_number": 1705,
    "commits_list": [
      "46ea86716fae98997d1d8d0c7b08d8c9e5e8340d",
      "220c3ea58f0594d5a58bda52f09df22ae205e260"
    ],
    "message_list": [
      "Do not allow the copy or deepcopy of Node, except for Data\n\nWe explicitly disallow calling the functions copy and deepcopy\nfrom the python copy module on a Node instance. The reason is that\nthe behavior of this for a Calculation node, with respect to what\nshould be returned and how this would affect the graph, is not clear.\nRather, to clone a Calculation, the caching mechanism should be\nused or a new ProcessBuilder could be generated from a completed\nProcess that would recreate the necessary inputs that could then\neasily be relaunched.\n\nFor Data nodes, the behavior can be defined a bit better. Therefore\nwe allow to call deepcopy on a Data node, which will call the internal\nclone method, which will return an identical, but unstored, clone of\nthe original Data node. The clone will have no links and a newly\ngenerated UUID.",
      "Raise on __deepcopy__ for a stored Data node\n\nDeep copying an unstored node is fine however and just pipes\nthrough to the clone method."
    ]
  },
  {
    "pr_number": 1713,
    "commits_list": [
      "f61ee61991c8f70cd7e706bcbd601e2e9b854642",
      "53039b730d09367a8a88445c0269ec7fcbb5a132",
      "5ea8dfc6bb307bb7b4d488a211d6e516640a49f6",
      "de03b053a17e6a80474542293649e0c271c9ee8e",
      "9c7b717a64184e88a64e4405fe9c14c6ece28fa5",
      "910f049df9b3febefaad12f2d1f664238c7cafe8"
    ],
    "message_list": [
      "Add unique constraints on UUID column of Node database model\n\nEven though the UUID of Nodes are supposed to be unique, there was no\nexplicit constraint on the column, allowing two nodes with identical\nUUIDS to be stored without complaint from the database. However, these\nnodes can now no longer be loaded through their UUID leaving the\ndatabase in an unusable state. This commit adds the migrations for both\nbackends and adds a test to check that attempting to store a Node with\na UUID that already exists will raise an IntegrityError.",
      "Improve error message if `load_dbenv` fails in click command\n\nWhen the database environment is loaded for a click command through\nthe `with_dbenv` decorator and the database version is inconsistent\nan exception is raised for the Django backend. Here we catch this\nexception and instead print a critical error message. This has the\nbenefit that the user is not overwhelmed with the stack trace but\nsimply sees the instructions on how to perform the migration.",
      "Ensure correct profile is printed if `check_schema_version` fails\n\nThe function was calling `get_current_profile` to determine the current\nprofile name, in order to format the migrate command that the user\nshould run in order to migrate the database. However, this function\nonly returns the profile name if the database environment was loaded,\nand otherwise None. The problem is that the very reason we are trying\nto get the current profile is *because* loading the database environment\nfailed. Therefore the returned profile will necessarily always be None.\nInstead, we pass the profile explicitly from the `load_dbenv` call and\nif it is None, the default profile was used, so we invoke the helper\nfunction `get_default_profile` to get the applicable profile name.",
      "Add `verdi database integrity` to the command line interface\n\nThe reason for this addition is that due to a bug in the database\nmodels, due to the omission of a uniqueness condition on the UUID\ncolumn of the node table, nodes with duplicate UUIDs coud be created\nleaving the database in an inconsistent state. Before we can apply\na migration to add the uniqueness constraint, any databases with\nduplicate UUIDS would have to manually solve this or the migration\nwould fail.\n\nA script was written for this particular case, but more such cases\ncould arise in the future. For this reason, this new `verdi` end\npoint was created to host any database integrity checks that might\nbe added in the future.",
      "Add a node UUID uniqueness check before applying migration\n\nA migration was recently added to apply a uniqueness constraint on\nthe UUID column of the Node table. However, for databases that violate\nthis constraint, the migration will fail and this will have to be\naddressed first. In a previous commit a cli end point was added to\ninvestigate the database for this problem and automatically fix it\nto remove this integrity violation. Here we add a check before the\nmigration, to see if duplicate UUIDs exist. If that is the case, the\nmigration is interrupted and the user is instructed to run the\nrelevant verdi command to fix the problem first manually.",
      "Add unit test for duplicate node UUID migration for Django\n\nWe define a special class `TestMigrations` that can be used to test\nmigrations for the Django backend. A test that implements this class\ndefines the starting revision and the migration it will apply and the\n`setUpBeforeMigration` method, which allows to add data before the\nmigration is applied.\n\nWe apply this concept here where we migrate to the penultimate, as of\nwriting migration, insert nodes with duplicate UUIDs, apply the fix\nthat would be run manually by the user to fix this problem, before the\nactual migration is applied. Afterwards, we verify that the deduplication\nexecuted before the migration had the desired and expected effect."
    ]
  },
  {
    "pr_number": 2016,
    "commits_list": [
      "2f11b4092b89e38a2c08856f5de56b890d9b973d"
    ],
    "message_list": [
      "Make `max_wallclock_seconds` option for JobCalculation optional again\n\nThis used to always be an optional option for JobCalculations but in\nthe recent change of the option getter and setters, it was marked\nas required. Here we make the option's properties compatible with\nthe original definition."
    ]
  },
  {
    "pr_number": 3850,
    "commits_list": [
      "5babbb817068517137ad80e7d0a4bc99f20372e9"
    ],
    "message_list": [
      "Inconsequential 'test' commit to test the codecov coverage checks."
    ]
  },
  {
    "pr_number": 2078,
    "commits_list": [
      "69153a39866510043e18984e6828420c90d79ec3"
    ],
    "message_list": [
      "Add cmdline templates to the MANIFEST.in\n\nThese template files are necessary for the `verdi` command line to\nwork and so should therefore be included in the package, which is\nachieved by listing them in the manifest file."
    ]
  },
  {
    "pr_number": 4269,
    "commits_list": [
      "3d7462b68286a71d43d4a1671fa2f93fe103b7c0"
    ],
    "message_list": [
      "Run test-install GA workflow only on main repository.\n\n * To avoid wasting of resources on forks.\n * Resolves issue #4257."
    ]
  },
  {
    "pr_number": 4296,
    "commits_list": [
      "da1e846093a099798330810fa30765b7c5d08fcc",
      "8e02d5fddf05e8ff74485ac505306e9f4e6d984b"
    ],
    "message_list": [
      "Pin ase dependency to <3.20 for Python 3.5.\n\nResolves issue #4295.",
      "Bump ase requirement to 3.20 in requirements (Python > 3.5)."
    ]
  },
  {
    "pr_number": 4320,
    "commits_list": [
      "c1ea7d786d5d66f7778c72aec3827f2d280cbc5b",
      "0167ba72fc3eb71eccafc06b5e097b5295007384",
      "13191a74dbea51c1e4eb210568e25d44ac2636e6",
      "35cf4cfedd89a7afbe4dd93e00109e23ddcee1c8",
      "7c69e3461be41a8ee1e542d55a7bca1709d05a4e"
    ],
    "message_list": [
      "Enable pip '2020-resolver' feature flag for ci workflow.",
      "Run 'install-with-pip' test with and without pip 2020-resolver feature flag.",
      "Run 'install-with-pip' test with and without extras.",
      "Do not install with pip with '--editable' option.",
      "Revert \"Do not install with pip with '--editable' option.\"\n\nThis reverts commit 35cf4cfedd89a7afbe4dd93e00109e23ddcee1c8."
    ]
  },
  {
    "pr_number": 4130,
    "commits_list": [
      "980d55a8ae8192c70d2ea4c7dae20f987830d1df",
      "3c5e1e5178e38e6eddd3045e9b1ad07edec616a1"
    ],
    "message_list": [
      "add top level indexes",
      "undo unintentional change"
    ]
  },
  {
    "pr_number": 4851,
    "commits_list": [
      "96ce992f1db9e1a00b6b4480b54531cf1f8af5f3",
      "c8c2de1646c1d9fdf781e9a74e770bbc15346203",
      "54acab6231ab900554febc9b8dbd2f8b3d1ebaac"
    ],
    "message_list": [
      "Implement 'show-requirements' command for dependency_management script.\n\nUtility function to analyze the package dependencies.",
      "Constrain jupyter-client to <6.1.13.\n\nTo avoid conflict for nest-asyncio required by both plumpy (dependency)\nand  jupyter-client (sub-dependency).",
      "Constrain pytest-cov to <2.11.\n\nTo avoid conflict for coverage required by us with constrain <5 and\npytest-cov."
    ]
  },
  {
    "pr_number": 4852,
    "commits_list": [
      "579bc1288fc25ad275ccc3f1c6c8ba4127fc40b5",
      "b1370dcad6e3e09342a71f8e4f535bb89d39ff7b"
    ],
    "message_list": [
      "Implement 'identify-outdated' command for dependency management.\n\nUtility function to analyze the package dependencies and identify\npackages where the latest release is no longer compatible with our\ndependency specification constraints.",
      "Add requirements.txt for utils/."
    ]
  },
  {
    "pr_number": 5082,
    "commits_list": [
      "553954da2a9663d65c6e33af168f5d66d2966c3a",
      "2feb2b47ccaeeded86eee6d41a1e07763c7d3dde",
      "6018636dba91041196903799dfbca759c0f35db3",
      "e38d636dc1ab7db42709420bf435d22bd71ad8c0"
    ],
    "message_list": [
      "MAINTAIN: Add 'check-yaml' to pre-commit hooks.",
      "Dependencies: Fix the pyzmq requirement specification for Python 3.9.",
      "CI: Implement resolve-pip-dependencies job.\n\nThis job checks whether the environments defined in the requirements/\nfiles are resolvable.",
      "CI: Run test to install-with-conda job for all supported Python versions.\n\nVerifies that aiida can be installed with conda for all supported\nPython versions."
    ]
  },
  {
    "pr_number": 5233,
    "commits_list": [
      "a3fae7511373e1209f39ca0e000ff0cfa65b2ead"
    ],
    "message_list": [
      "Do not use the deprecated matplotlib config option 'text.latex.preview'.\n\nDeprecated as of version 3.3.4, see:\nhttps://matplotlib.org/3.3.4/api/api_changes.html#text-latex-preview-rcparam\n\nRequires matplotlib~=3.3,>=3.3.4.\n\nFixes #5231."
    ]
  },
  {
    "pr_number": 4959,
    "commits_list": [
      "076fc53c2dcc5f80c970ba373fc2d41457029a0d",
      "a3772a170f280193b09ed11b03ab51f6e3d34dd8",
      "dcfc5541bcd0f72ea78e2dc2d26a7d68383d3748",
      "3e9e086a177c1ba00ae1a92c6ce84e80bd3a0073",
      "6d21e63497896a708d058fc6415a9b9c111eae4d",
      "149daa623e2b26268a2ab7f4c6485d613c7f870e"
    ],
    "message_list": [
      "Merge pull request #1 from aiidateam/develop\n\nMerge Updates",
      "Merge branch 'aiidateam:develop' into develop",
      "Create hashing function for DatetimePrecision Object",
      "Adds tests",
      "Move DatetimePrecision to aiida.common",
      "Merge branch 'develop' into datetimeprecision_hash"
    ]
  },
  {
    "pr_number": 1738,
    "commits_list": [
      "83d401d8d4c99f6908665c44bc857cde0e7a68ca",
      "0b952cc39df0c98d51d9f943d4b02ce6a4ffa48c",
      "e66cc26025346c10a38a2232270502d9daef7a32",
      "77b5d52ec9edc7b6df8ae9769323ed1a26968a5d",
      "89673612786ecad2a3cb90ae390448eadbf03154",
      "369ea8077b42463ba681470a670fb1232e1cdc92",
      "dde2bd5d639522d73c513dac42ded04f992a46fc",
      "13f18a59fb044f353b999ac68ae0d53aef528b16",
      "9453023a640868a32abd0d0e189dfb578120a111",
      "f903a2b1b8fdfe1dab7eeb777b20328fd8ef548f",
      "dfa90ded0c88435dfbabe5696c90a1ade3ee60f7",
      "abf765635bf6fcc81d0840d85b163b94f147d669",
      "1c01a590b43a14e4404173204dd8833060a317d0",
      "041ba6de05b02a4c93c3fba45482a3e27736b232",
      "8c6f3713189d3b185373d5ed7f972371d20ff73f",
      "b0b587a7dfb5d03b2b795787eb9039049dd37d96",
      "e9a1d1d92c2093a4de39a2aa24aa314d5674271f",
      "493bab692cb46487f2bfae7d00be0abc0d3062b4",
      "815188f788006ac6accfef292de56d2081a8eefe",
      "e92bd6314bf44090fd79603f1182d8e9d3f15b73",
      "ecd544333dcd3c048c49ef1a44a0a686b0cbed88",
      "d3e23c332ae4c5fca1b1f6d99052509f248710de",
      "aaea79da4ea9e2c64348d663d335953a92b8b443",
      "e0f4a83ba9b57d874c256d0ff2e6ef9b41f54e64",
      "bb00dab186d97839efe00da819291b8fb79fe249",
      "cd0a942897bbac3646849bd45ae6a9f8908fb715",
      "9b5394033b591ef48a6b1527918e2add73f91c69",
      "b217e957b74fe8a9ba315a4ab6fb139605450708",
      "4cbf964eeefcb24b0555753d71a634ade2cb7147",
      "bfb13fc533d8ac2e75ba6cb4a4911f45cbc66500"
    ],
    "message_list": [
      "Data subcommand move to click:\n\n1) create a folder for data click commands\n2) place subcommands of verdi data in different files",
      "data upf is moved to click",
      "Put list-related functions to a separate file",
      "data bands (not fully) transferred to click",
      "Grouping the list option arguments",
      "Removing not needed load_dbenv",
      "data bands are fully transferred to click",
      "Merge branch 'click-data' of github.com:yakutovicha/aiida_core into click-data",
      "More progress on cif",
      "Remove label and description from data lavel",
      "Merge branch 'click-data' of github.com:yakutovicha/aiida_core into click-data",
      "More progress on cif",
      "Merge branch 'click-data' of github.com:yakutovicha/aiida_core into click-data",
      "data parameter are transferred to click",
      "Merge branch 'click-data' of github.com:yakutovicha/aiida_core into click-data",
      "More progress on cif",
      "Merge branch 'click-data' of github.com:yakutovicha/aiida_core into click-data",
      "Put export in a separate file",
      "Merge branch 'click-data' of github.com:yakutovicha/aiida_core into click-data",
      "More progress on cif",
      "More progress on cif",
      "data remote is fully transferred to click",
      "Merge branch 'click-data' of github.com:yakutovicha/aiida_core into click-data",
      "More progress on cif deposit",
      "Merge branch 'click-data' of github.com:yakutovicha/aiida_core into click-data",
      "More progress on cif deposit",
      "Ported verdi data structure to click",
      "Merge branch 'click-data' of github.com:yakutovicha/aiida_core into click-data",
      "Transfer verdi data trajectory to click",
      "Transfer verdi data array to click"
    ]
  },
  {
    "pr_number": 2712,
    "commits_list": [
      "5fa84cf7f813601dcfa3db593f400b7b5e579a05"
    ],
    "message_list": [
      "Update requirement for dependency `psycopg2==2.8`\n\nStarting from `psycopg2==2.8` the standard package will no longer\ninclude the binary wheels. To install the binaries, one has to install\nthe package `psycopg2-binary` instead. Since `aiida-core` was using by\ndefault the binaries before this change, we change the dependency to\nexplicitly install from the wheels. Even though for production it is\nadvised to build from source, the only problems that can occurr are due\nto mismatch between internal libraries, but these are rare. Contrarily,\ninstalling from source will have more system dependencies and induce\nlonger build times. Since we have not experienced problems with the\nwheels until now, we stick with them for the time being."
    ]
  },
  {
    "pr_number": 2706,
    "commits_list": [
      "80c5a345348ecb001d214dbdaf59e854f0086c43",
      "c671d8cc1a1806546466a59f27c8b84a1e06127e"
    ],
    "message_list": [
      "Replace `TextField` stored for storing JSON with proper `JSONField`\n\nThe following model fields were using a text field to store JSON:\n\n * `DbComputer.metadata`\n * `DbComputer.transport_params`\n * `DbAuthInfo.auth_params`\n * `DbAuthInfo.metadata`\n * `DbLog.metadata`\n\nNow that Django supports a JSON field for the Postgres JSONB field it is\nbetter to replace the old implementation such that these columns become\npropertly queryable and we are no longer responsible for proper\nserialization and deserialization",
      "Clean the `AuthInfo` frontend and backend implementations"
    ]
  },
  {
    "pr_number": 2159,
    "commits_list": [
      "deb95bd215f048c9780dcfaa51a74f505fe0a961",
      "2990b2dd3cee84cd365eb3034eb4b46da1624a41",
      "fa31682df5c6aba762060b1e140165065ccf60d4",
      "c1f0a5b830be0dd82f553380a298bddb9b84289a"
    ],
    "message_list": [
      "Travis CI: Add Python 3.7 to the testing \n\nLet\u2019s make sure that we have all the _async_ issues resolved.",
      "Merge branch 'develop' into patch-1",
      "Merge branch 'develop' into patch-1",
      "Update dependency PyYAML==3.13"
    ]
  },
  {
    "pr_number": 2110,
    "commits_list": [
      "657dbc70cb35ab82d488ec69542bbf43c06c348d",
      "2a056edb1645f345f7dd3ac6536910deb5c80ddd",
      "ab05da8025970fb361f34ea474b1dd070c447021",
      "d1c884a5953cc1f71db1c786965fd00a496b095b",
      "a57d6a7f2fe6f9b59d47b52aa7e48b0805925b06",
      "ed66684e0b8e7686d64cc1d46daa626ab4edba93",
      "06912e01c989936ab2a7732c3ff1eba17c950d17",
      "4947daa2b083a451a17d491b2d64f47396bd131a",
      "d725696bacafe8ebc92e279083834fe6a4a5e7a8",
      "2b488c0a06c893d4a56d9a28c6e05344f50b18e6",
      "c417c1a86c64f4f5e303f60a4c8d955dab72d1d8"
    ],
    "message_list": [
      "Fix several issues with the hashing method\n\n* dict items are sorted _after_ hashing, fixing the behavior for\n  keys which can not be sorted\n* unicode/str and str/bytes are explicitly handled, with a different\n  type salt\n* in general, the combination between two hashes is done via the\n  method of boost::hash_combine, not by concatenating the hexdigest",
      "Add docstrings, remove dead code.",
      "setup_requirements: add unittest2 for python 2 for subTest context",
      "common/test_hashing: add tests for password hashing functions",
      "common/test_hashing: use unittest2 on Py2 for subTest",
      "common/hashing: allow argon2 password scheme",
      "common/hashing: rewrite hashing based on a blake2b tree-based hash method",
      "common/hashing: make everything Python 2 compatible",
      "hashing: implement hashing for tz-aware datetime objs",
      "fix collisions with end-digest for containers",
      "Merge branch 'develop' into fix_python3_hashing"
    ]
  },
  {
    "pr_number": 4175,
    "commits_list": [
      "966956da26822ab31e6503abace6b868c57f552f",
      "9b105cab6bf2b8ce9f1dc9d1d9030b5694d7c5d9"
    ],
    "message_list": [
      "`SshTransport.gettree`: allow non-existing nested target directories\n\nThe `gettree` method would raise an `OSError` for the `SshTransport` if\nthe target local path contains intermediate subdirectories that are not\ncreated beforehand. The same would work without problem for the\n`LocalTransport` plugin. It makes sense to just create non-existing\ndirectories along the way, so the `os.mkdir` command that was used\noriginally to create the target directory is replaced with the method\n`os.makedirs` setting `exists_ok=True`.",
      "Merge branch 'develop' into fix/4174/ssh-transport-gettree-nested"
    ]
  },
  {
    "pr_number": 2705,
    "commits_list": [
      "6e23cdc65d04e698228c38d95b75283d8ba5f677",
      "3b5cff726c597d326ff7a803530f572b4005d343"
    ],
    "message_list": [
      "Change the heuristics of the `AIIDA_PATH` environment variable\n\nThe location of the configuration folder will be determined and\noptionally created following these heuristics:\n\n * If the `AIIDA_PATH` variable is set, all the paths will be checked\n   to see if they contain a configuration folder. The first one to be\n   encountered will be set as `AIIDA_CONFIG_FOLDER`. If none of them\n   contain one, a configuration folder will be created in the last\n   path considered.\n\n * If the `AIIDA_PATH` variable is not set the `DEFAULT_AIIDA_PATH`\n   value will be used as base path and if it does not yet contain a\n   configuration folder, one will be created.\n\nThis new approach will guarantee that a configuration folder will\nalways be set and created if necessary. The one downside is that if\nsomeone sets the environment variable to the wrong value, a config\nfolder will be created without a warning.",
      "Test"
    ]
  },
  {
    "pr_number": 4871,
    "commits_list": [
      "678dd0bebe9a519130e3f6c2416335fa97139c37",
      "38da950e9e88f0b8f9886b5d8458fbf5b2c36065"
    ],
    "message_list": [
      "engine: implement sub-dicts for context in workchains\n\nIncidentally, this also fixes an issue where no exceptions were raised\nfor either unknown awaitable actions or awaitable mismatches for lists\nof awaitables since an `assert \"some str\"` is always True (hence never\ntriggers).",
      "docs: document the workchain context nesting"
    ]
  },
  {
    "pr_number": 5594,
    "commits_list": [
      "9eec94dacbfb3bd8a41f454c72d95ff1499b794b"
    ],
    "message_list": [
      "Use new installed code in CI test (disable all other tests to check)"
    ]
  },
  {
    "pr_number": 878,
    "commits_list": [
      "1cf1b453375e8036ce1935f1161fd63fe0d9cb90",
      "0f8718de8a869bb4b16a669222e38ba9712b3e86",
      "86c994ddf39f27bb0916aab5e2cca563b6d1a0df",
      "224128c58c293511deb4980bf3e3866a543b1ce7",
      "f573ea266f82fc61170e06939b06c60d3d9d0f25",
      "a3af5fa769a49ca200ba43213661aeabdcf9a9a5",
      "dc3d9b6f1d7120727c777a384c78adbe8843ee99",
      "f49ff832ad497ac8778ca85609f6fd63f624a252",
      "0fbc324f9c5556f36af6355617afd1909ca55db8",
      "2ea94bfe707b2baf0c3f146589734978fcdb8248",
      "9136eb5aa25b16cd0da55e6e0ba540b1efe588cf",
      "6fc554d76e3c83f1370dd9907510acd027eb36c0",
      "aadba66ac3bb35cc3c1becba3f9936cadfbbe49b",
      "88001722cb1fdfa21a0a55536300e87ed5db6011",
      "f50715012633b8842bba4c6d94fa1d4cc4378dec",
      "43ad11a6009516e79a535b861c55734d85704887",
      "01a689670b4138be496d3ba1e0bf0a58600931f5",
      "c30e7fc144a4cd61c50a7c2d094be84e4cf76730",
      "1473cf7bbc023a3342d570f1e7e7cf41e475baf0",
      "63787fa3ff81f6373b6ad17b7cb493e539ef20a3",
      "029d780e57c520bb059d6831b850612c781bec85",
      "60e3eacb587cdd807357fba3a4f31b437ccd3741",
      "3cde5ab9b0b7589b3037b67c52be71737225fcdf",
      "58dc8152fbadc297f87fb54a3360a3d25c21f415",
      "a5448684bec3f460fb1804c3cfd74ab5ed0cf48b",
      "95efb61fad7a573fbe1767f089f1891f40b8c143",
      "4022c562400751083cc942a7f747803a117abfb0",
      "96ff216c3b1f2a0c7cb48ec4d2d1dbc7d6556216",
      "26c3459d1249bca2c704cde520c551f7c20d16d3",
      "597ea82a2cd6285734bd923268703056d606f879"
    ],
    "message_list": [
      "Merge remote-tracking branch 'main_repo/develop' into develop\n\nConflicts:\n\taiida/restapi/translator/base.py",
      "Merge remote-tracking branch 'main_repo/develop' into develop",
      "Merge remote-tracking branch 'main_repo/develop' into develop",
      "Merge remote-tracking branch 'main_repo/develop' into develop",
      "Merge remote-tracking branch 'main_repo/develop' into develop",
      "Merge remote-tracking branch 'main_repo/develop' into develop",
      "In Restapi, fixed nodecount id to visualise node tree",
      "fixed issue in band visualization",
      "updated statistics function in restapi",
      "Merge branch 'develop' of https://github.com/aiidateam/aiida_core into develop",
      "Merge branch 'develop' into materialscloud",
      "Merge pull request #740 from waychal/materialscloud\n\nwill move to separate fork in the future",
      "Merge branch 'release_v0.10.0' into materialscloud",
      "added custom schema file",
      "Merge branch 'release_v0.10.0' into materialscloud",
      "Merged release_v0.10.0",
      "Merge pull request #839 from waychal/materialscloud\n\nMerge waychal/materialscloud into aiidateam/materialscloud",
      "Extended structure visualization endpoint to return data in different formats",
      "Added info endpoint to return server info like current aiida version",
      "Added /server/enpoints in REST API",
      "Returned exception to handle in parent if not 404 exception",
      "Merge branch 'release_v0.10.0' into materialscloud",
      "Merge branch 'release_v0.10.0' into issue_793",
      "Merge branch 'release_v0.10.0' into issues_805_restapi_structure_endpoint",
      "Merge pull request #845 from waychal/materialscloud\n\nMerge waychal/Materialscloud into aiidateam/materialscloud",
      "Updated structure endpoint to download and visualize data in diff formats",
      "Added testcase for structure visualization endpoint",
      "Merge branch 'materialscloud_aiidateam' into issues_805_restapi_structure_endpoint_mcloud",
      "Merge branch 'materialscloud_aiidateam' into issue_777_restapi_user_endpoint_mcloud",
      "Merge branch 'materialscloud_aiidateam' into issue_793_restapi_server_endpoints_mcloud"
    ]
  },
  {
    "pr_number": 4810,
    "commits_list": [
      "21382834731291b543ca9bfbc55a57fe8a3c7fda",
      "7c314dfafe9da9083967721fdd4e9e2a90641be6",
      "e460ab47d7a420125d7b63a5a11c7e2b228b6495",
      "ba4f1a64b2a1ae4888e6bc6b333ffc24c32eb995",
      "e42637b799f971ec2348a3a150a6dd07ef7f19d2",
      "b0a2a5b3367af92d3a1ab91077654ef01650a9a3"
    ],
    "message_list": [
      "Change input() to prompt in query_string",
      "Change input() to prompt in query_yes_no",
      "Merge branch 'develop' into issue_3933_change_prompting_implementation",
      "Change input() to prompt in ask_answer",
      "Merge branch 'issue_3933_change_prompting_implementation' of https://github.com/NinadBhat/aiida-core into issue_3933_change_prompting_implementation",
      "Add type annotations"
    ]
  },
  {
    "pr_number": 5428,
    "commits_list": [
      "adc03e1bb88f293c52be3a4761fa0582490b052d",
      "4c53e17ad53284015bda09817f23dbbf3fb5d653"
    ],
    "message_list": [
      "verdi run autocomplete fix for scriptname argument\n\nChange the argument type of verdi run [scriptname] to `click.File` so\nthat the autocomplete will call `shell_complete` of the type.",
      "[pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci"
    ]
  },
  {
    "pr_number": 2540,
    "commits_list": [
      "78302dc547e134b972c9b488d481a105ca2bbf57"
    ],
    "message_list": [
      "Improve the efficiency of `DbLog` migration for SqlAlchemy\n\nThe old implementation was using a `NOT IN` with the list of all node\nids to filter all log entries that had an `objpk` that did not\ncorrespond to a node entry. This is extremely inefficient as it loads\nthe entire node pk column in memory and has to traverse it for each log\nentry. In this commit, we change this to a construct that will simply\nsearch for the objpk in the node table, which is indexed and simply\nreturns 1 if it exists, which is than checked in the filter of the logs."
    ]
  },
  {
    "pr_number": 3765,
    "commits_list": [
      "8a6e2b9f665402ee6a0631f400e7f8ebb6c4a878"
    ],
    "message_list": [
      "suggest running `reentry scan` when EP not found\n\nAlready had two users at the hackathon that stumbled over this.\nA better error message should make this issue go away."
    ]
  },
  {
    "pr_number": 4513,
    "commits_list": [
      "5ea5be76c75fbfa3d5d73248f2e7a6f1f0045c1c"
    ],
    "message_list": [
      "Fix ZeroDivisionError (#4512)"
    ]
  },
  {
    "pr_number": 2671,
    "commits_list": [
      "d32aa14140670bdd8904f561aa05a0c118e17745"
    ],
    "message_list": [
      "Implement `__getitem__` on `Dict` node sub class\n\nThis shortcut will allow users to directly use `node[key]` instead of\nhaving to go through either `get_dict` or `dict` property."
    ]
  },
  {
    "pr_number": 4865,
    "commits_list": [
      "fc83b8b4f6db3f5c8f90d3d687f1bcdd5fae89ce",
      "6f2a8bcfeba5fc87869e0a029a4b48f56f89ce78"
    ],
    "message_list": [
      "Fix `aiida.cmdline.utils.decorators.with_dbenv` always loading the db\n\nThe premise of the method `with_dbenv` is that it will load the profile\nand corresponding database environment if not already loaded. However,\ndue to a bug in `load_backend_if_not_loaded` it was actually *always*\nloading the environment for every call, even if the database env was\nalready loaded, leading to very expensive calls for nothing.",
      "Merge branch 'develop' into fix/4864/fix-with-dbenv"
    ]
  },
  {
    "pr_number": 2687,
    "commits_list": [
      "e5abcd9b114efec2b979d1a43fc553893861d0f6"
    ],
    "message_list": [
      "Make the outputs of a live process nestable\n\nThe outputs namespace of a process specification allows for arbitrary\nnesting, just like the inputs. The inputs of a process, when passed,\nwill have to have a nesting structure that matches that of the inputs\nprocess specification. However, the internal structure of outputs of a\nprocess were not keeping the same nested structure as their process\nspecification counterpart. Of course when the processes finishes and the\noutputs have to be linked up through links to the process node, the\nsupport for nesting disappears and the mapping will have to be\nflattened. However, while the process is \"alive\" the internal outputs\nshould keep the nesting structure of the process spec."
    ]
  },
  {
    "pr_number": 4214,
    "commits_list": [
      "f0c4623ea98640268ed6b080f7c9895c5ad3dada"
    ],
    "message_list": [
      "Implement `skip_orm` option for SqlAlchemy `Group.remove_nodes`\n\nThe current implementation of `Group.remove_nodes` is very slow. For a\ngroup of a few tens of thousands of nodes, removing a thousand can take\nmore than a day. The same problem exists for `add_nodes` which is why a\nshortcut was added to the backend implementation for SqlAlchemy. Here,\nwe do the same for `remove_nodes`. The `SqlaGroup.remove_nodes` now\naccepts a keyword argument `skip_orm` that, when True, will delete the\nnodes by directly constructing a delete query on the join table."
    ]
  },
  {
    "pr_number": 4370,
    "commits_list": [
      "882b75e3b675d8f3b699da96cbac91bab1dddf89"
    ],
    "message_list": [
      "`CalcJob`: improve logging in `parse_scheduler_output`\n\nThe level of the log that is fired if no detailed job info is available\nis changed from `WARNING` to `INFO`. Since not all schedulers implement\nthe feature of retrieving this detailed job info, such as the often used\n`DirectScheduler`, using a warning is not very apt. If the information\nis missing, nothing is necessarily wrong, so `INFO` is better suited.\n\nOn the contrary, if the `Scheduler.parse_output` excepts, that is grave\nand so its level is changed from a warning to an error.\n\nFinally, a new condition is added where the scheduler does implement the\nmethod to retrieve the detailed job info, but the command fails. In this\ncase, the return value will be non-zero. This value is now checked\nexplicitly and if the case, a info log is fired and the detailed job\ninfo is set to `None`, which will cause the parsing to be skipped. This\ncase can for example arise when using the `SlurmScheduler` plugin, which\ndoes implement the detailed job info feature, however, not all SLURM\ninstallations have the job accounting feature enabled, which is required\nby the plugin."
    ]
  },
  {
    "pr_number": 3962,
    "commits_list": [
      "77692cee2020f00c740db8225cf385f2acbed114"
    ],
    "message_list": [
      "Require Click ~=7.1.\n\nTo test the revised 'update-dependency' workflow. Do not merge."
    ]
  },
  {
    "pr_number": 4405,
    "commits_list": [
      "58f12a59a85211a05c6d99fdedc5c3dccc76aa0c"
    ],
    "message_list": [
      "`verdi setup`: forward broker defaults to interactive mode\n\nThe options for the message broker configuration do define defaults,\nhowever, the interactive clones for `verdi setup`, which are defined in\n`aiida.cmdline.params.options.commands.setup` override the default with\nthe `contextual_default` which sets an empty default, unless it is taken\nfrom an existing profile. The result is that for new profiles, the\nbroker options do not specify a default, even though for most usecases\nthe defaults will be required."
    ]
  },
  {
    "pr_number": 2244,
    "commits_list": [
      "1ea6e18441cd993aee395c7e125ab5707f587054"
    ],
    "message_list": [
      "Allow existing profile to be overriden in `verdi setup`\n\nWith the `--force` flag a user can suppress the exception that is raised\nwhen one tries to create an existing profile in non-interactive mode. With\nthe flag the existing profile will be overriden with the specified values\nexcept for the profile UUID which will be kept the same as it is auto\ngenerate and can also not be specified by the user on the command line."
    ]
  },
  {
    "pr_number": 4265,
    "commits_list": [
      "ea1882ed957350fba94fe0e1326082d2b4cf22c3",
      "55e1210a0f985572aeb38b247c2805c3d549649f",
      "54cab83dde747cc09be9d738ad7562003a6ad6f9",
      "db956b2201af272cb18768bb94a1e5ef5630cc2e"
    ],
    "message_list": [
      "Pin Python version for test-install:pip workflow to 3.7.7.\n\nAs a stop-gap solution for issue #4256.",
      "Update Python versions for shared steps in test-install workflow to 3.8.",
      "Merge branch 'develop' into ci/issue-4256-pin-python-to-377",
      "Consistently use setup-python action v2 in all GA workflows."
    ]
  },
  {
    "pr_number": 5860,
    "commits_list": [
      "48d4e0b59098eff8c55873d4394ed813c6cf80aa",
      "1aca65b5c4c8afffbbf4d3a78f703ed966294238"
    ],
    "message_list": [
      "CLI: verdi export to a yaml file with keys from code class\n\nfixes #3521\n\nAdd `verdi code export` command to export code from command line as a\nymal file.\nThis is mentioned in usability improvement as well as having a command to export the code and computer setup.\nKeys of YAML file are read from the cli option of the corresponding code class.",
      "Merge branch 'main' into fea/xx/export-code-setup"
    ]
  },
  {
    "pr_number": 5478,
    "commits_list": [
      "38b14dc623c7d467610e61a3825e0d53bded7794"
    ],
    "message_list": [
      "Add option to use double quotes for `Code` and `Computer` CLI arguments\n\nSo far, the command line arguments of the code exeuction line in the\nsubmit script written by the `Scheduler` plugin, would be wrapped by\nsingle quotes to prevent special characters, such as spaces, from being\ninterpreted incorrectly. However, for certain use cases, the quoting has\nan undesirable effect. For example, if an argument contains a bash\nvariable, indicated by a `$` sign, the single quotes will prevent it\nfrom being dereferenced correctly.\n\nA concrete use-case is the requested feature of being able to define a\nDocker container as a `Code`, which needs certain arguments to be quoted\nin double quotes. This PR adds a new property `use_double_quotes` to the\n`Code` and `Computer` classes that determines whether arguments should\nbe quoted using single or double quotes.\n\nThe settings are communicated through the `cmdline_params` attribute of\na `JobTemplateCodeInfo` instance to the scheduler plugin. The plugin is\nthen responsible for respecting these booleans. To be able to\ndistinguish between the command line arguments of the computer and the\ncode, they are passed separately in `prepend_cmdline_params` and\n`cmdline_params`, respectively."
    ]
  },
  {
    "pr_number": 2517,
    "commits_list": [
      "26e426dd303033b4bf0f64130d1e1a25a364d8fe",
      "537ca15b32ec4070ff05a2178cb15ad706e388ed"
    ],
    "message_list": [
      "Renaming `DbNode.type` to `DbNode.node_type`",
      "Fix migrations"
    ]
  },
  {
    "pr_number": 4316,
    "commits_list": [
      "c85e929c2227c9882868c969c76573a7fa092d2e"
    ],
    "message_list": [
      "`verdi computer test`: fix bug in spurious output test\n\nThe test that checks for spurious output when executing a normal command\non a computer over the transport had a bug in it that went unnoticed\nbecause the code path was not tested. If the `stderr` of the command\ncontained any output the command would raise because the test that is\ncalled `_computer_test_no_unexpected_output` would incorrectly return a\ntuple of length one instead of two in that case.\n\nIn addition to adding tests to hit this code path, the message that is\nprinted in the case of non-empty stdout or stderr is deduplicated and\nadapted to be bit clearer and refer directly to the documentation\ninstead of through a Github issue."
    ]
  },
  {
    "pr_number": 2552,
    "commits_list": [
      "1ba85b2b0b034d6f85043b191620465ef478e543",
      "30b12531b06b6bf6177a764def3668e69185b494"
    ],
    "message_list": [
      "Renaming `DbNode.type` to `DbNode.node_type`",
      "Remove any use of our ORM in the migrations and tests\n\nMany of the migrations and their respective unit tests used the ORM of\n`aiida-core` or the database model classes directly. However, as the\ndatabase models evolve and change, during migrations one cannot rely on\nthe current implementation of the database models or the ORM as it may\nnot be commensurate of their state at the time of the migration. This\nmeans that the migrations absolutely have to be written relying only on\nraw SQL or on the model classes that are provided through the migration\nmechanism of the various backends, as they will make sure that those\nfake classes reflect the state of the models at the time of the migration."
    ]
  },
  {
    "pr_number": 5029,
    "commits_list": [
      "9e220e4952cbec0f5813ec6716e2f7618d8d24d8"
    ],
    "message_list": [
      "CI: Test direct installation via conda.\n\nInstead of just testing whether we can install into the environment\ndefined by environment.yaml."
    ]
  },
  {
    "pr_number": 1897,
    "commits_list": [
      "c376b46c79a6075c5440698600e697029001cfa3"
    ],
    "message_list": [
      "Correcting command line option 'code' for 'verdi data ... deposit'\n\nValue of ``--code`` has to be stored to key ``code_label`` in order to be passed correctly to ``deposit()``."
    ]
  },
  {
    "pr_number": 5218,
    "commits_list": [
      "27790522b3997a30fafb650751945442c8404953"
    ],
    "message_list": [
      "Add support for `Enum` types to `aiida.orm.utils.serialize`\n\nEnum's are often used data types that currently cannot be used in\nprocess inputs, even if the ports are `non_db`, because even the\n`non_db` inputs are serialized as part of the entire `Process` instance\nthat is serialized when a checkpoint is created.\n\nThe custom YAML serializer and deserializer that are defined in the\n`aiida.orm.utils.serialize` module that are used for serializing process\ninstances and their inputs, are updated with a dumper and a loader for\n`Enum` instances. The full class identifier is generated from the\ndefault object loader provided by `plumpy` and concatenated by the `|`\nsign and the enum's values, it is represented as a YAML scalar.\n\nThe loader uses the same object loader to load the enum class from the\nidentifier string and reconstruct the enum instance from the serialized\nvalue.\n\nThe original tests were written in `tests/common/test_serialize.py` and\nare moved to `tests/orm/utils/test_serialize.py` to conform with the\nstandard to mirror the package hierarchy. The tests are converted to\n`pytest` style and a new test is added by performing a round trip of an\nenum instance (de)serialization."
    ]
  },
  {
    "pr_number": 2967,
    "commits_list": [
      "0168476f51eb17d29343133b8bbf9a64e13161a7"
    ],
    "message_list": [
      "Improve `safe_interval` validation `verdi computer configure`\n\nThe computer configuration command for the transport specified that\nan integer was required. However, a float value makes more sense. In\naddition to this change, an additional validation callback was added to\nmake sure that the interval is positive.\n\nThe addition of the validation through the default option callback\nmechanism required a small adaptation to the `InteractiveOption`.\nThe behavior of error handling was different if the option was\nexplicitly defined on the command line but without an explicit\n`--non-interactive` flag. If a string value was passed, the type\nvalidation would fail and prompt loop was never entered. However, if a\nnegative number was passed, the type validation would pass, but then the\nnormal callback would fail and in this case the prompt would start.\nHowever, this was with the context improperly configured and so\nexceptions would occur in the contextual defaults."
    ]
  },
  {
    "pr_number": 4834,
    "commits_list": [
      "ba96cfac5653de961a74c45c307d0b605c50a5dc"
    ],
    "message_list": [
      "\u2b06\ufe0f UPDATE: psgu v0.2"
    ]
  },
  {
    "pr_number": 3001,
    "commits_list": [
      "becb8b5f8b8d782c6f87fc9205164fb41faa0172"
    ],
    "message_list": [
      "Allow to specify number of workers in `verdi daemon start`\n\nNow you can start more than just one worker when starting the daemon."
    ]
  },
  {
    "pr_number": 2540,
    "commits_list": [
      "78302dc547e134b972c9b488d481a105ca2bbf57"
    ],
    "message_list": [
      "Improve the efficiency of `DbLog` migration for SqlAlchemy\n\nThe old implementation was using a `NOT IN` with the list of all node\nids to filter all log entries that had an `objpk` that did not\ncorrespond to a node entry. This is extremely inefficient as it loads\nthe entire node pk column in memory and has to traverse it for each log\nentry. In this commit, we change this to a construct that will simply\nsearch for the objpk in the node table, which is indexed and simply\nreturns 1 if it exists, which is than checked in the filter of the logs."
    ]
  },
  {
    "pr_number": 4202,
    "commits_list": [
      "20b50de092e546b645d7388dbebb09485b4db47f"
    ],
    "message_list": [
      "Remove .gitignore from abandoned 'sphinxext/tests' folder.\n\nThe tests for the Sphinx extension have moved to the top-level\n'tests/' directory. In the 'aiida/sphinxext/tests' directory,\na lonely '.gitignore' was left over - this is now deleted."
    ]
  },
  {
    "pr_number": 813,
    "commits_list": [
      "0a44a501c83c82720ffe26512b4dfbae8c913ca8"
    ],
    "message_list": [
      "Corrected base module path in node translator"
    ]
  },
  {
    "pr_number": 5606,
    "commits_list": [
      "7f0e518d868e9c13a71364082325d683574874af"
    ],
    "message_list": [
      "Dependencies: set upper limit `werkzeug<2.2`\n\nThe `werkzeug==2.2` release is breaking the REST API causing most of the\nroutes to return a 404 error."
    ]
  },
  {
    "pr_number": 3256,
    "commits_list": [
      "3fb5941a0344c1f51913cb33ce86b2b53fd75746"
    ],
    "message_list": [
      "Docs: update \"AiiDA internals for developers\" to `aiida-core==1.0.0`"
    ]
  },
  {
    "pr_number": 5631,
    "commits_list": [
      "c9ee1cd058a901b38e990c8b00318368e196e00f",
      "413b8c626a10393a32e8ab09b1c97291a4ba6dab",
      "0171a78ce6d05b9bf0739019d87d53e9d68cc92e",
      "95eb1e5166da79ee814515439c136af4692d0088"
    ],
    "message_list": [
      "`Profile`: make definition of daemon filepaths dynamic\n\nThe filepaths relevant to the daemon that are provided through the\n`Profile.filepaths` property, where defined statically at the module\nlevel. This means that they would be fixed as soon as the `profile`\nmodule would be first imported based on the `DAEMON_DIR` and the\n`DAEMON_LOG_DIR` variables of the `settings` module.\n\nThis would be problematic if those variables were to be changed, because\nthey wouldn't be reflected by `Profile.filepaths`. An example of such a\ncase is the running of the test suite without a pre-configured test\nprofile. In this case, a temporary test profile is created from scratch\nincluding the AiiDA configuration directory. Since the path of the\nconfiguration directory changes, the filepaths of the daemon also\nchanged and this would have to be reflected by `Profile.filepaths`.",
      "`DaemonClient`: add the `get_env` method\n\nThe `DaemonClient.get_env` method is added which provides a copy of the\nenvironment that is used by the process in which the client is running.\nThis is used by the `start_daemon` and `_start_daemon` methods to ensure\nthat the launched subprocesses use the exact same environment as the\nprocess of the client.\n\nThe implementation is largely taken from the `get_env_with_venv_bin`\nfunction of the `aiida.cmdline.utils.common` utility. This function is\ndeprecated since its is purely relevant to the daemon functionality and\nso should just be contained in the `DaemonClient`.\n\nThere is a single important change and that is that `get_env` adds the\ncurrent content of `sys.path` to the `PYTHONPATH` key. This is important\nas to guarantee that the daemon process will have the same module paths\nas the main process. This ensures that all modules that are importable\nby the main process, will also be importable by the daemon process.",
      "`DaemonClient`: close `CircusClient` after call\n\nThe `CircusClient` opens a `zmq` socket which wasn't being closed. By\ntransforming `DaemonClient.client` into a context manager that\nautomatically calls `client.stop()` at the end, the socket is closed\nproperly. This gets rid of the multitude of `ResourceWarnings` at the\nend of the test suite.",
      "Fixtures: Add `started_daemon_client` and `stopped_daemon_client`\n\nThe fixtures `started_daemon_client` and `stopped_daemon_client` are\nadded which are function scoped fixtures that ensure that the daemon is\nrunning or stopped, respectively. The fixtures return an instance of the\n`DaemonClient` which can be used to interact further with the daemon\nwhere necessary.\n\nThe fixtures both use the `daemon_client` fixture which is session\nscoped and will ensure that the daemon is stopped at the end of the test\nsession. The `started_daemon_client` replaces the `with_daemon` fixture\nwhich was not going through the `DaemonClient` designed for this\npurpose."
    ]
  },
  {
    "pr_number": 4376,
    "commits_list": [
      "6a36582cebe6afef644594304d95700bc900fc87",
      "11a36e6b3a2ef865400a4d0343cf28dc2b4423da",
      "f2dc9648f880bfdf6463669da914413a54dc2f01"
    ],
    "message_list": [
      "ORM: homogenize attributes/extras methods of backend node\n\nMake sure the code for the attributes and extras methods are identical,\nas a first step towards refactoring the code to use a mixin class for\nthese methods. These changes should have no influence on how the methods\nfunction.\n\nAdd exception chaining for exceptions raised directly during the handling\nof another exception. There is only a minor difference in the output, but\nit should make it clear that this exception was raised purposefully.",
      "ORM: move attributes/extras methods of backend node to mixins\n\nMove the attributes and extras methods to two mixin classes called\n`BackendEntityAttributesMixin` and `BackendEntityExtrasMixin`, stored in\nthe new aiida.orm.implementation.entities.py module. The mixin classes\nrely on the `is_stored` and `_flush_if_stored` methods, so these are\nadded as abstract methods. They are \"mixed in\" at the BackendNode` level\nwhere the abstract methods of the attributes and extras are removed.\n\nMove the `_flush_if_stored` method to the `BackendEntity` class, which\nis added leftmost to the `BackendNode` parent classes. This method can\nbe used by all backend entities.\n\nMove `BackendEntity` and `BackendCollection` classes to\n`aiida.orm.implementation.entities.py` module.\n\nMove `validate_attribute_extra_key` and `clean_value` methods to new\nmodule `aiida.orm.implementation.utils.py`.\n\nMove the calls to `validates_attribute_extra_key` method from the front\nend `Node` class to the backend mixin classes `AttributesBackendEntity`\nand `ExtrasBackendEntity`. This way the key/value validation/cleaning is\nboth done at the backend level, which is more consistent. Moreover, this\nmeans other frontend classes won't have to add this call to\n`validates_attribute_extra_key` to their methods, when they want to use\nthe attributes/extras methods.\n\nAdd exception chaining for all the modules that are adjusted.",
      "ORM: move attributes/extras methods of frontend node to mixins\n\nMove all methods related to attributes and extras from the frontend\n`Node` class to separate mixin classes called `EntityAttributesMixin`\nand `EntityExtrasMixin`. This makes it easier to add these methods to\nother frontend entity classes and makes the code more maintainable."
    ]
  },
  {
    "pr_number": 3587,
    "commits_list": [
      "198fbbb4658b26c92c116b447914b235db4ad93b",
      "698fea0fdf1b95e96e89411f8917f897c7623725"
    ],
    "message_list": [
      "print path to config file in verdi status\n\nFrom time to time, the need arises to have a look inside the currently\nactive AiiDA configuration file.\nWhile the path to the file repository is printed in the output of `verdi\nstatus` and `verdi profile show`, one needs to edit this path to get to\nthe config file (and, in principle, the repository can even be located\ncomepletely elsewhere).\n\nPrinting the location of the config file is helpful for daily use, also\nas a reminder which configuration file is currently active.",
      "Merge branch 'develop' into show-aiida-path"
    ]
  },
  {
    "pr_number": 4860,
    "commits_list": [
      "a313eb93390fbeef7e18c4051e2354c4f6a9b70c",
      "9c8c4515ef2fdf454567a6fd1f57a18f6bc37463"
    ],
    "message_list": [
      "CI: use `postgres-12` since update to Ubuntu Focal 20.04\n\nThe CI configuration uses `ubuntu-latest` which recently became Ubuntu\nFocal Fossa (20.04) which no longer has `postgres-10` available, but\ninstead provides `postgres-12`.",
      "Switch to install `postgresql` instead"
    ]
  },
  {
    "pr_number": 5165,
    "commits_list": [
      "cf63c3ea8fbed9adaf8a38f245a9606c377ab97f",
      "63e590dfffe505675b710ffc564075a9ec8c0a4a"
    ],
    "message_list": [
      "\ud83d\udc4c IMPROVE: constructor of base data types\n\nAdapt the constructor of the `Dict` and `List` data types so the\nvalue no longer needs to be provided as a keyword argument, but can\nsimply be passed as the first input.\n\nRemove the `*args` from the `BaseType` constructor, instead specifying\nthe `value` input argument. Otherwise users could just pass multiple\npositional arguments and it would only use the first without raising an\nerror.\n\nAlso fixes two issues with the `List` class:\n\n* The `remove` method was not working as prescribed. Instead of\nremoving the first element in the list with the specified value, it\nsimply deleted the element with index `value`.\n* The `set_list()` method didn't make a copy of the `list` input, so any\nmodification done to the `List` instance would also affect the original\n`list`. If the same list is used to initialise several `List` instances,\nadapting one would affect the other.\n\nFinally, add tests for the `List` class.",
      "Refactor data tests"
    ]
  },
  {
    "pr_number": 5251,
    "commits_list": [
      "cfaf6e7ae0915df13b748d58d497f834cf3b48ef",
      "b968dad34f382ed1dbf528f5a080b99c7c7234bd",
      "4ff2782863225849f33194341b2c7229d05395ab",
      "4b89c6b6220e36ea37c18453fa8540108f61b1ce"
    ],
    "message_list": [
      "\u203c\ufe0f BREAKING: Compare `Dict` nodes by content\n\nCurrently there is an inconsistency in how the base data type node\ninstances compare equality. All base types compare based on the content\nof the node, whereas `Dict` instances rely on the UUID fallback\nintroduced in #4753. After a long discussion started by #1917, it was\nfinally decided that the best way forward is to make the equality\ncomparison consitent among the base types (see #5187).\n\nHere we adapt the `__eq__` method of the `Dict` class to compare\nequality by content instead of relying on the fallback comparison of\nthe UUIDs.",
      "Merge branch 'develop' into fix/1917/dict-equality",
      "Remove `__ne__` and update tests",
      "Merge branch 'develop' into fix/1917/dict-equality"
    ]
  },
  {
    "pr_number": 3765,
    "commits_list": [
      "8a6e2b9f665402ee6a0631f400e7f8ebb6c4a878"
    ],
    "message_list": [
      "suggest running `reentry scan` when EP not found\n\nAlready had two users at the hackathon that stumbled over this.\nA better error message should make this issue go away."
    ]
  },
  {
    "pr_number": 2159,
    "commits_list": [
      "deb95bd215f048c9780dcfaa51a74f505fe0a961",
      "2990b2dd3cee84cd365eb3034eb4b46da1624a41",
      "fa31682df5c6aba762060b1e140165065ccf60d4",
      "c1f0a5b830be0dd82f553380a298bddb9b84289a"
    ],
    "message_list": [
      "Travis CI: Add Python 3.7 to the testing \n\nLet\u2019s make sure that we have all the _async_ issues resolved.",
      "Merge branch 'develop' into patch-1",
      "Merge branch 'develop' into patch-1",
      "Update dependency PyYAML==3.13"
    ]
  },
  {
    "pr_number": 1197,
    "commits_list": [
      "6c1252c09f37e1f6383ac9929a287be751d75ec3",
      "7c2d7890a73adc9e6f4fbb90d844dbb46c0d829d"
    ],
    "message_list": [
      "Homogenize the output of verdi work list and verdi calculation list\n\nNow that all Calculations are routed through the Process layer and\ntherefore are guaranteed to share the same information through the\nprocess_state and finish_status attributes, we can update the verdi\ncommands 'calculation list' and 'work' list to use the same labels,\nformat and default order. This hopefully creates a more consistent\nand therefore more easy to understand output for the user.\n\nUnfortunately since the querying for JobCalculations is relatively\nhardcoded in AbstractJobCalculation class, I had to duplicate some\nof the logic to format the various data attributes in the two\ndisplay commands. Ideally this would be abstracted but that would\ntake a huge effort.",
      "Make 'state' and 'job_state' compound projection for 'calculation list'\n\nThe default set of projections for `verdi calculation list` should not be\nwider than 80 columns roughly, otherwise the lines for people with default\nshell widths will just wrap. However, with the recent changes of homogenizing\nthe spheres of WorkCalculations and JobCalculations through the process\nlayer, we want to give `verdi work list` and `verdi calculation list` a\nsimiliar default output. In addition to the default projections of the\nformer, the calculations also have the calculation and scheduler state which\nshould remain there by default, but this would make the output to wide. The\nsolution is to define compound projections:\n\n\t* state\n\t* job_state\n\nThe first applies to both `work` and `calculation` and will show the tuple\nof `process_state` and `finish_status`. The second only applies to the\n`calculation list` and is the tuple of the old calculation state and the\nscheduler state."
    ]
  },
  {
    "pr_number": 2333,
    "commits_list": [
      "7549fd5c69facc033180534b228331ad0ad42b67",
      "b5bf6dedd38c999fb70ca79c821de09af5a5f688"
    ],
    "message_list": [
      "Add test for availability of sudo in quicksetup\n\nQuicksetup needs to access the Postgres database, but does not\nknow anything about the system setup. It initially tries to\nsee if it is the postgres superuser, but if it is not, it tries\nto become the postgres superuser via sudo. This is the backstop\nposition and relies on the user having a modern Linux-like system.\nIf, however, the sudo command is not available to this user at all\n(for example, perhaps on an HPC cluser), an error will be raised and\nquicksetup will fail ungracefully. This commits add a test to see\nif sudo can be found before attempting to use it. If it is not, no\nattempt is made, and the final manual setup message is printed.\n\n- Add simple check of the availability of the sudo command to\n  the user.\n- Add warning message if sudo cannot be found.\n- Update click.echo calls to the AiiDA echo function.\n- Update selected messages to 'warning' type messages.",
      "Merge branch 'provenance_redesign' into fix_2004_quicksetup_assumes_presence_of_sudo"
    ]
  },
  {
    "pr_number": 3262,
    "commits_list": [
      "c703d1c7c7df0bc93efcffddbe736787709c84f0",
      "da308b68b87b5ed3bb20b7b7ec795264060c8439",
      "1a0648e8b7461d4e20083782ea24c3f1e37729e9",
      "16b2999daa714dda155421d9567b71aa4784413f",
      "70d4f2bb08de6a36c77b5f08b235d9d852560379"
    ],
    "message_list": [
      "aiida.tools.importexport.__init__ doc update",
      "Use proper rules for Links flags for export\n\nAdding some missing tests for the flags `input_forward`,\n`create_reversed`, `return_reversed`, `call_reversed` revealed that some\nNodes were falsely exported or were missing.\n\nIn order to fix this, make the code more transparent, and speed up the\nexport, it has been moved to a utility function and optimized.\nIndeed, Links were collected twice during export. Now everything is done\nat the same time.\n\nArchives _must_ now be self-consistent in terms of the provenance graph\nit represent. I.e., all Nodes mentioned in the list of Links and Groups\n_must_ be included in the archive.",
      "Separate into 12 basic Link flags upon export\n\nAll 12 possible flags to follow AiiDA Links forwards or backwards have\nbeen added to the export functionality.\nWhile, they are implemented, only some a togglable.\nThe default values, as well as determining which flags are togglable,\nhave been put into a config dict, and all references to default rules\nnow defer to this central dict for a more easy configuration.\n\nSplitting the `input_forward` flag into `input_calc_forward` and\n`input_work_forward`. The same is true for `call_reversed`.\nThese are now usable to users from `verdi export create`.\n\nWhen on a testing profile, one can switch any flag without raising.",
      "More explicit mention of togglable link_flags\n\nIn order to make it more clear, which Link flags are togglable and which\nare not, the LINK_FLAGS dict in .importexport.common.config has been\nstripped down to only list all link rules and whether to follow them or\nnot and the togglability has been put directly in the utility function\nthat retrieves Nodes to be exported.",
      "Address review comments by @sphuber"
    ]
  },
  {
    "pr_number": 5594,
    "commits_list": [
      "9eec94dacbfb3bd8a41f454c72d95ff1499b794b"
    ],
    "message_list": [
      "Use new installed code in CI test (disable all other tests to check)"
    ]
  },
  {
    "pr_number": 4214,
    "commits_list": [
      "f0c4623ea98640268ed6b080f7c9895c5ad3dada"
    ],
    "message_list": [
      "Implement `skip_orm` option for SqlAlchemy `Group.remove_nodes`\n\nThe current implementation of `Group.remove_nodes` is very slow. For a\ngroup of a few tens of thousands of nodes, removing a thousand can take\nmore than a day. The same problem exists for `add_nodes` which is why a\nshortcut was added to the backend implementation for SqlAlchemy. Here,\nwe do the same for `remove_nodes`. The `SqlaGroup.remove_nodes` now\naccepts a keyword argument `skip_orm` that, when True, will delete the\nnodes by directly constructing a delete query on the join table."
    ]
  },
  {
    "pr_number": 4434,
    "commits_list": [
      "a7f48674f1ad7ee1cb1e08ee31f0803c3e9ff810",
      "cea718bcd96b9207a8beae87a87612fd11791176"
    ],
    "message_list": [
      "node.open: more useful warnings\n\naiida-core started warning when methods like `Node.read` and `Node.del`\nare used outside a context manager.\nThe warning, however, did not include any indication of where the\nproblematic function call was made (nor did it include instructions on\nhow to fix it).\n\nFurthermore, this PR fixes an overlooked use of `Node.read` in\naiida-core that did not rely on the context manager.",
      "Merge branch 'develop' into issue_4335_deprecation_warning"
    ]
  },
  {
    "pr_number": 4492,
    "commits_list": [
      "18d58825b04c0c336fece6a4399f72dcc1086a67"
    ],
    "message_list": [
      "Docs: extend REST API\n\nMove the documentation on how to extend the AiiDA REST API to the\n\"Internal Architecture\" section.\n\nCo-authored-by: Giovanni Pizzi <gio.piz@gmail.com>\nCo-authored-by: ramirezfranciscof <ramirezfranciscof@users.noreply.github.com>"
    ]
  },
  {
    "pr_number": 4405,
    "commits_list": [
      "58f12a59a85211a05c6d99fdedc5c3dccc76aa0c"
    ],
    "message_list": [
      "`verdi setup`: forward broker defaults to interactive mode\n\nThe options for the message broker configuration do define defaults,\nhowever, the interactive clones for `verdi setup`, which are defined in\n`aiida.cmdline.params.options.commands.setup` override the default with\nthe `contextual_default` which sets an empty default, unless it is taken\nfrom an existing profile. The result is that for new profiles, the\nbroker options do not specify a default, even though for most usecases\nthe defaults will be required."
    ]
  },
  {
    "pr_number": 4511,
    "commits_list": [
      "d3ef9a0f91666604209f9b47e2c602ce80c8619a",
      "785fbe6606b4a790d94c9bbf182bbea1d3bdb84d",
      "f823c652ecf257317ba94b06a2edeef8cc68d036"
    ],
    "message_list": [
      "Docs: clarify public API\n\nMake the definition of the AiiDA public API more explicit and add\nexamples.",
      "Apply suggestions from code review",
      "Merge branch 'develop' into docs-public-api"
    ]
  },
  {
    "pr_number": 2552,
    "commits_list": [
      "1ba85b2b0b034d6f85043b191620465ef478e543",
      "30b12531b06b6bf6177a764def3668e69185b494"
    ],
    "message_list": [
      "Renaming `DbNode.type` to `DbNode.node_type`",
      "Remove any use of our ORM in the migrations and tests\n\nMany of the migrations and their respective unit tests used the ORM of\n`aiida-core` or the database model classes directly. However, as the\ndatabase models evolve and change, during migrations one cannot rely on\nthe current implementation of the database models or the ORM as it may\nnot be commensurate of their state at the time of the migration. This\nmeans that the migrations absolutely have to be written relying only on\nraw SQL or on the model classes that are provided through the migration\nmechanism of the various backends, as they will make sure that those\nfake classes reflect the state of the models at the time of the migration."
    ]
  },
  {
    "pr_number": 4663,
    "commits_list": [
      "c5f0a6ce949849d990bebebd12cdf55894a48eb3",
      "c0d1f235e85403069b810cf990288b793c82c53f"
    ],
    "message_list": [
      "CLI: show daemon worker load even when under threshold\n\nThe extra query performed in `verdi process list` to get the load on the\ndaemon runners is slow - and usually results in no output for the user\nin case that the load is below the threshold. With this change, the load\nis shown even when below the threshold. Furthermore, the query is only\nperformed if the daemon is running and skipped otherwise.",
      "Merge branch 'develop' into process-list-2"
    ]
  },
  {
    "pr_number": 4695,
    "commits_list": [
      "013d0985617cb98cca53ceb888a6c8707b43e37d"
    ],
    "message_list": [
      "engine: try cleanup transports"
    ]
  },
  {
    "pr_number": 2517,
    "commits_list": [
      "26e426dd303033b4bf0f64130d1e1a25a364d8fe",
      "537ca15b32ec4070ff05a2178cb15ad706e388ed"
    ],
    "message_list": [
      "Renaming `DbNode.type` to `DbNode.node_type`",
      "Fix migrations"
    ]
  },
  {
    "pr_number": 4741,
    "commits_list": [
      "e74e4b915cd941af224a5448b03b684e0b54096c",
      "983311e6c918af03b2aa62e698d3edff57cc7728",
      "81edfcfcfbe32490d129d468223ca3107ecaab86",
      "74fb15a0365070d109415fcd7a4d9aa4c87a3a7b"
    ],
    "message_list": [
      "close django connection in REST API\n\nThe REST API has no need for the django connection, since it is only\nusing the QueryBuilder so far.\n\nI've tested that even storing ORM entities still works inside the REST\nAPI, i.e. if needed the django connection is simply reestablished on the\nfly.",
      "scope connection drop to django backend",
      "disable connection pooling by default",
      "fix test"
    ]
  },
  {
    "pr_number": 4779,
    "commits_list": [
      "db9cb92406160fee61b37a6573e3a6c286bf1d05",
      "4407c52af5d1834332316dcceec805a34d5687d9",
      "d1ec61a983a35df435552ad06dbef74baff3fc07",
      "4e93b199877dd00527d157ede1a32f3b3db9b504",
      "13c6b3af4dae2018a55fc00efd310add7e5c166b",
      "c24610bdda7cb866e38901ffb5aacee0b6028865",
      "df9135a3b0f9b7314c8b38ffb094664d4548da4b",
      "8ab4e65ca2d146fb0865b98c9eb1818a275bd31c",
      "a7692fdfd35fa576b7791bb95a1c8624fdbc4473"
    ],
    "message_list": [
      "simplify test setup",
      "introduce refurbish_database",
      "switch back to classproperty",
      "try fixing rabbitmq tests\n\nfloats failed to be stored because of missing user.\nI suspect this occurs when tests are \"cleaning\" the db without\nrepopulating the user.",
      "try fixing import/export and visualization tests\n\nsimply move refurbish_db to AiidaArchiveTestCase",
      "try fixing outdated computer issue",
      "fix computer loading",
      "try fixing remaining 4 tests",
      "Merge branch 'develop' into simplify-test"
    ]
  },
  {
    "pr_number": 1872,
    "commits_list": [
      "d0166c22e80c464d4c812d978394f9cc1ef17a31"
    ],
    "message_list": [
      "Disable caching for InlineCalculation."
    ]
  },
  {
    "pr_number": 4363,
    "commits_list": [
      "65ea5a00ee8572a0ecc8ef7194f3619a7a62ecdc",
      "0689d2bcf779fee5880ffaa08bc21d6a591b8c73",
      "745ca9c59b764c5bb39808c2fe8c3b74ad682bc0",
      "4543bf7702827285eee543f99b5000164cbf4726",
      "5f22ff6f427d7f113b342945ee48494e79c24b67",
      "13fa71aa67aa46d43d07b2ef7159fa4c590dd2fd",
      "88b88c9a400648fe979f1f7e8e8f53c85f784b64",
      "81319e4ee0170c9eb80c35a1d6d5d25d3e1a719f",
      "a141f4734afad90893135089d78e7e7a0bd1cfa2",
      "5f84473b6dcb235d5268650f1bb39f0c10c3fea9",
      "9f5267612fc45ab7c52b8d9640757fe9078f94c3",
      "bcab0a9c14aed7ee9661c915e7989a31d92bb5ad",
      "41cd80b0c4446d281fd38b50a4d9a4d907021625"
    ],
    "message_list": [
      "adapt ssh.py to handle subclassing file transport part.\n- try to minimize intrusivity and changes in ssh.py (no new class/subclass)\n- initialization of the file transport is moved to another method\n- some calls are untouched, as they have to be overriden entirely\n- some calls had to be split (by adding a wrapper around the self.sftp call), so that only this wrapper is overriden.",
      "limit output of this logger call to 1000 chars.\nWhen sending files with ssonly plugin, they get pasted entirely in the log otherwise.",
      "please linter",
      "add docstrings",
      "clarify error message\n\nCo-authored-by: ramirezfranciscof <ramirezfranciscof@users.noreply.github.com>",
      "Don't catch everything, just sshexception.\nAdd full plugin entry point\nplease linter",
      "add comments for stat/lstat functionality and output (from paramiko)",
      "rename symlink_internal to _symlink",
      "try to avoid warning by circleci",
      "try again",
      "transport name was renamed (and be more explicit by giving the full plugin name)",
      "allow subclasses to configure cropping of log message without changing default behavior",
      "Merge branch 'develop' into sshbase"
    ]
  },
  {
    "pr_number": 4900,
    "commits_list": [
      "10dd22462fa04dfc1901566c60b5b399a37dbbab"
    ],
    "message_list": [
      "Profile: only initialise the repository during the migration\n\nThe new disk object store migration that will ship with `v2.0` requires\nto be initialised once and only once, and it will generate the necessary\nfolders and configuration file. This process was being done in the\nmethod `Profile.get_repository` guarded by a check in case it was\nalready initialised.\n\nThe problem with this was that the container could be initialised too\nearly during the early stages of a profile setup. As soon as the\nrepository was fetched, it would be initialised generating, among other\nthings, the UUID. This would then trigger the check that the database\ncontained the same UUID, which would of course fail, since the database\nwas empty. This could have been fixed by ignoring the check at this\npoint, but the real problem is that the repository should not be\ninitialised at this point. The only point at which the repo should be\ninitialised is during the corresponding database migration that\nintroduced the disk object store repository. Both for existing as well\nas for new profiles, they will go through this migration and so it and\nit alone should be responsible for initialising the repository.\n\nThis approach did create problems for the unittests though, as they\nwould sometimes clean the repository. It would this not by just removing\nthe contents, but it would delete the entire container. This meant it\nhad to be recreated, but since in normal operations this only happens\nduring the migration (which also during tests only happens once, unless\nmaybe during the migration tests themselves) and so an error would be\nraised that the repository is not initialised. The solution is to\nreinitialise a new repo as soon as the old one was destroyed. Currently\nthis is done by simply deleting the folder on disk and reinitialising an\nentire new instance. In the future, it would be better if the existing\ncontainer could be kept and its contents could simply be dropped, but\nthis would require a feature in the `disk-objectstore` library."
    ]
  },
  {
    "pr_number": 2705,
    "commits_list": [
      "6e23cdc65d04e698228c38d95b75283d8ba5f677",
      "3b5cff726c597d326ff7a803530f572b4005d343"
    ],
    "message_list": [
      "Change the heuristics of the `AIIDA_PATH` environment variable\n\nThe location of the configuration folder will be determined and\noptionally created following these heuristics:\n\n * If the `AIIDA_PATH` variable is set, all the paths will be checked\n   to see if they contain a configuration folder. The first one to be\n   encountered will be set as `AIIDA_CONFIG_FOLDER`. If none of them\n   contain one, a configuration folder will be created in the last\n   path considered.\n\n * If the `AIIDA_PATH` variable is not set the `DEFAULT_AIIDA_PATH`\n   value will be used as base path and if it does not yet contain a\n   configuration folder, one will be created.\n\nThis new approach will guarantee that a configuration folder will\nalways be set and created if necessary. The one downside is that if\nsomeone sets the environment variable to the wrong value, a config\nfolder will be created without a warning.",
      "Test"
    ]
  },
  {
    "pr_number": 5072,
    "commits_list": [
      "8ec8d3d4e183142fc4fd67544b41145f53621c43",
      "9071aa449dc9d48492fbea603b1b2554aab464be"
    ],
    "message_list": [
      "bump ipykernel dependency in test requirements\n\nipykernel 6.0.3 required importlib-metadata<4 for python 3,7, which\ncaused @chrisjsewell to use ipykernel 5 (which does not have this\nrestriction) for python3.7 in the test requirements.\n\nIn the meanwhile, my PR to relax this requirement in ipykernel has been\nmerged and ipykernel 6.1 has been released.",
      "Merge branch 'develop' into bump-ipykernel"
    ]
  },
  {
    "pr_number": 5089,
    "commits_list": [
      "67dab0f1a40ff657f7fd669d883100b973a469af",
      "28a14da546aa4163aa67b2fb0897c4eb74c61488",
      "7adc38ed0503d422db59e6856232b3c3767b5170",
      "6a1527815841b7234056f46c187cd97fff4af9be",
      "70718fd71e1d7d2cea9b627f738fe8deda6d2c68",
      "0cf8ee3e2424047bf01d4faae7b17fe6054d69b2"
    ],
    "message_list": [
      "Remove superfluous _validate from Data node\n\nThe Data nodeclass overwrote the `_validate` function but\n (a) did no validation\n (b) included commented code that referenced a private git repository\n (c) returned `None` instead of `True`.\n\nThe outcommented code involved validation of the `source` attribute and\nthe referenced issue pointed out that the value of this attribute was\nnot officially standardized (potentially leading users to using it in\ndifferent ways).\n\nSince there has not been validation of this field since 2015, it seems\nsafe to assume that it cannot be easily reintroduced at this point.",
      "Update aiida/orm/nodes/node.py\n\nCo-authored-by: Francisco Ramirez <ramirezfranciscof@users.noreply.github.com>",
      "Merge branch 'develop' into fix-validation",
      "Update aiida/orm/nodes/node.py\n\nCo-authored-by: Francisco Ramirez <ramirezfranciscof@users.noreply.github.com>",
      "Merge branch 'develop' into fix-validation",
      "Update aiida/orm/nodes/node.py"
    ]
  },
  {
    "pr_number": 2718,
    "commits_list": [
      "b30c9f401545b892c402fa8f9c932c10679a6d87"
    ],
    "message_list": [
      "Remove duplicate `verdi` command tests\n\nThe tests in `aiida/backends/tests/cmdline/commands/test_node.py`\nand `aiida/backends/tests/cmdline/commands/test_data.py` were\nessentially identical, with the latter adding some tests, so the former\nis deleted."
    ]
  },
  {
    "pr_number": 5254,
    "commits_list": [
      "53cfc8e3b1ef88f8ec33f19c0ab86a9f5080a52d",
      "6abb1d2d4b32462774ac155e44c5539713673125"
    ],
    "message_list": [
      "docs: add trove classifier for development status\n\naiida-core was missing the trove classifier for the development status",
      "Merge branch 'develop' into add-trove-classifier"
    ]
  },
  {
    "pr_number": 1716,
    "commits_list": [
      "8668252088967c87d1ee107d4c9d88f683d125c2",
      "f68ad58246014e660f89bbea99407fb883bd2613",
      "981a0b003a9cd9367671bcc6dfe7963ed6c08e2b",
      "9073634831c07ae5c8535166857bd337743bcb17",
      "5567283d0dccff76e76ba8585a2d2332bfb56638",
      "78b438e9b20d0e27e68a7f6692e4cb99d3d5a9f8"
    ],
    "message_list": [
      "Starting to convert computer to click\n\nFor now only converted enable and disable,\ntests added.",
      "This should solve the issue of the failing tests\n(old test removed/replaced, and the verdi computer test\nmoved to the right file)",
      "For some reason a couple of files were not automatically yapf'ed in the precommit",
      "Further fixes from yapf\n\nprobably I had committed with a different version?",
      "Adding updatedb to travis",
      "Merge branch 'verdi' into fix_partial_1582_verdi_computer_disable"
    ]
  },
  {
    "pr_number": 5107,
    "commits_list": [
      "64542877b5462facf17a49b73b96af8ad1a97c45",
      "c75df65ffb9539469a9ba6b23f6a6769e7b635dd"
    ],
    "message_list": [
      "CLI: fix bug in the `CliFormatter` log utility\n\nThe `format` method assumed that the `prefix` attribute would always\nexist, which is not always this case. This would result in an uncaught\nexception when a message would get logged. This went unnoticed since the\nutility was not individually tested. Additionally, messages with literal\npercentage signs would also except because it would always be\ninterpolated with the `record.args` but if that was an empty tuple, a\n`TypeError` would be raised.",
      "CLI: fix flaky `verdi daemon start` command tests\n\nThe `verdi daemon start` command tests that define a specific number of\nworkers, either through a CLI option or through the config, were flaky\nand failing. The probably cause was that the tests were changing the\nconfiguration during the test, which was changing the actual\nconfiguration and changes were not undone after the tests. These changes\ncan therefore interfere with other tests.\n\nTo fix it, the tests are converted to `pytest` style and the tests that\nmanipulate the config use a new fixture that creates a clone of the\nconfig before running the test and making sure to restore the original\nconfig after the test was done. In addition, it monkeypatches the\n`Config` class to make sure it doesn't write backups to disk when it is\nchanged which is the default behavior but is not desirable during\ntesting.\n\nFinally, a test is added for `verdi daemon status`. It had been broken\ndue to a recent refactor in the CLI logging code but went unnoticed\nsince it is not tested. The actual bug was fixed in the previous commit."
    ]
  },
  {
    "pr_number": 5386,
    "commits_list": [
      "5a5834ab06c2be8e16b4637e2c82034eeb287836"
    ],
    "message_list": [
      "CLI: reinstate the tab-completion of parameter values\n\nWe recently removed the `click-completion` dependency that used to\nprovide the tab-completion functionality for `verdi`, since as of\nv8.0 `click` ships itself with an implementation of this feature.\n\nThe problem is that the original interface was changed from:\n\n    ParameterType.complete(self, ctx, incomplete)\n\nto:\n\n    ParameterType.shell_complete(self, ctx, param, incomplete)\n\nThis change was not added when `click-completion` was dropped and so\nvalue completion was broken, but is reinstated here."
    ]
  },
  {
    "pr_number": 4994,
    "commits_list": [
      "2f6aa1c91d48b9c8ebd2d93d02eb7d9dd2249b90",
      "955f93a9c65f35872c4191c7772ac41ced96b5c0",
      "4b175af8783b8e6c8287250b77a6351887c31219",
      "014e1d56d079a4f34e27983091f26014e09f957f",
      "174d22174d71770237ff6aff5e82c8ddbb6e5ea1"
    ],
    "message_list": [
      "Change job name to use sanitised string",
      "Add job_name sanitation test for SGE scheduler",
      "Merge branch 'aiidateam:develop' into fix/sge-job-name",
      "Pre-commit fix",
      "Merge branch 'fix/sge-job-name' of ssh://github.com/mjclarke94/aiida-core into fix/sge-job-name"
    ]
  },
  {
    "pr_number": 2958,
    "commits_list": [
      "29bfae0daeed21555abd9a54135a97c8e0dcb73f"
    ],
    "message_list": [
      "Update dependency `kiwipy==0.5.1` with critical bug patch\n\nThe persistence of a task message, which represents a process that is to\nbe executed, is guaranteed by RabbitMQ, but only if the exchange, queue\nand the task message itself are configured correctly. Up till now the\nexchange and task queue were properly set to be durable, however, the\nindividual messages were sent with the default delivery mode, which\nmeans non-persistent. This caused tasks to be lost on broker restart.\nThe patch release 0.5.1 of `kiwipy` fixes this by now always marking\ntask messages as persistent."
    ]
  },
  {
    "pr_number": 5354,
    "commits_list": [
      "78e981eb07fd6c19a418de057bd92d1b2a24ab78",
      "74e613b91e72b72acee948d011bd921b8f7edfe6"
    ],
    "message_list": [
      "CI: move time consuming tests to separate workflow that runs nightly\n\nThe test that runs the Reverse Polish Notation (RPN) workchains and the\nmigrations tests are relatively time consuming and they should not be\naffected by most commits. That's why we can afford to have them just run\nnightly instead of on every PR.\n\nA `nightly` GHA workflow is added that runs every day at midnight. It\nruns a bash script `tests_nightly.sh` that in turns runs the script\n`test_polish_workchains.sh` as well as the unit test using `pytest`. The\nlatter is called with the option `-m 'nightly'` which will select only\ntests that are marked as nightly.\n\nMost tests under `tests/backends/aiida_sqlalchemy/migrations` are marked\nas nightly. This is done by applying the `@pytest.mark.nightly` decorator\ndirectly to the test functions. For the tests under the `django_branch`\nand `sqlalchemy_branch` this is instead done dynamically through the\n`pytest_collection_modifyitems` function in the `conftest.py` module\nsince having to manually apply this to all test would be too tedious.",
      "Merge branch 'develop' into fix/ci-add-nightly"
    ]
  },
  {
    "pr_number": 3434,
    "commits_list": [
      "e6c86de9eade31573ff51b6fe5b0a8f370b7ae4e",
      "b2d85184bb72ca8eb59f734aa31d715b56604ff5",
      "1daea69f7fdd14cf921427baeb9c124bd678671f"
    ],
    "message_list": [
      "Disallow pickle when storing numpy array in ArrayData\n\nUnpickling untrusted data is a potential security risk, since it\nallows arbitrary code execution. For this reason, numpy changed\nits 'load' method to no longer allow pickles by default. This\nmeans that in AiiDA, ArrayData containing pickles can be stored,\nbut not loaded. To make things consistent, we now also disallow\npickle when saving the array. Besides the security risk, pickle\nis also problematic in the context of AiiDA because it can also\nbecome unreadable e.g. when the classes it contains change names.",
      "Explicitly disallow pickle also in numpy.load method.",
      "Merge branch 'develop' into disallow_pickle"
    ]
  },
  {
    "pr_number": 5315,
    "commits_list": [
      "0951f47be739a5c264ae7cc74d2cadd6732df9ef",
      "4967108916d1f9145081d78777ac4f7b7aa49d06",
      "c0c6e35505eb5f7a1fac188a9b797cd8ca7174c4"
    ],
    "message_list": [
      "Print warning when incompatible RabbitMQ version is used\n\nThe lower limit for the `kiwipy` requirement is upped to `>=0.7.5` since\nthat provides the `server_properties` property on the `RmqCommunicator`.\nThis allows us to determine the version of the RabbitMQ server that is\nconfigured for the profile. In `load_profile` a check is added that will\nprint a warning if this version is greater than 3.7.\n\nFor RabbitMQ 3.8 and up, the default configuration will have a default\ntimeout on channels of around 30 minutes. This means that tasks that are\nnot acknowledged within that timelimit will be requeued. If the task was\nactually still running with a daemon worker, it can be started by a\nsecond runner simultaneously with all sorts of problems as a consequence.",
      "Address review comments",
      "Fix tests"
    ]
  },
  {
    "pr_number": 4773,
    "commits_list": [
      "e9bf2077ea6040944f6b4faa0bda3446a82d6554",
      "3e437766c5d04b0ba56192f150861423736ea3ab",
      "f52d3be120f4c203a3bd2a187631df36aea6a5f8"
    ],
    "message_list": [
      "\ud83d\udc1b FIX: `ModificationNotAllowed` on workchain kill",
      "move to after ctx update",
      "streamline"
    ]
  },
  {
    "pr_number": 3614,
    "commits_list": [
      "85120979e8f831255b893b08fb1048c78147e91b",
      "c40e6887c2fbb49583d27f315ca3d6e558f2e33d",
      "c03f7f83d554e7aa977ec67bc5d47b70ea5e06c4"
    ],
    "message_list": [
      "Allow for the 'file' parameter in SinglefileData.set_file to be a pathlib.Path.",
      "Refactor SinglefileData tests.\n\nIntroduce two pytest fixtures to check the content of a\n`SinglefileData`: a version that checks the content and filename\nonce, and a second version that checks before and after `.store()`.\n\nThe tests are refactored to use these fixtures, without (hopefully)\nfunctional changes to the tests.",
      "Merge branch 'develop' into allow_pathlib_path_in_singlefile"
    ]
  },
  {
    "pr_number": 5215,
    "commits_list": [
      "f85d0291ace35852bbba1e84af400275bf97e07a"
    ],
    "message_list": [
      "`verdi code setup`: validate the uniqueness of label for local codes\n\nIn commit d25339dd7e8f8bed40145392dd5166817f3afef5 `verdi code setup`\nwas improved to have a callback for the `label` option to check for its\nuniqueness. However, it only implemented this for \"remote\" computers,\nwhich have an associated `Computer` and so the uniqueness criterion is\non the \"full label\", which is the `label@computer.label`.\n\nHowever, there are also \"local\" codes, which don't have an associated\ncomputer and so only have the label as the identifier that should be\nunique. The callback `validate_label_uniqueness` is now updated to\ndistinguish between these two cases."
    ]
  },
  {
    "pr_number": 3637,
    "commits_list": [
      "91e85c59d5dba5e3946dfa3860b2d104349fce26",
      "e02ee97a209fd08ab7d61d34e8d8d4388fc92cf6"
    ],
    "message_list": [
      "Add hook and exit_code keyword for more nuanced cache validation.\n\nAdds a method `is_valid_cache` to the `Process` class, which is\ncalled with a `ProcessNode` instance as an argument. This method\nis called by `ProcessNode.is_valid_cache`. It allows implementing\ncustom behavior in `Process` sub-classes, such as the `CalcJob`\nclasses implemented in plugins.\n\nAs a default, the `Process.is_valid_cache` checks whether the\nexit code returned from the process has a -- newly introduced --\n`invalidates_cache` attribute set to `True`. This is a simple\nway of marking the as cache invalid for a specific exit code only.\n\nNote that this is a backwards-incompatible change, which can\nbreak code that uses unpacking of ExitCode, for example\n\n    status, message = ExitCode(...)\n\nThe new ways of controlling caching are documented in the\ndeveloper's documentation.",
      "Define calcfunctions at module level in tests.\n\nThis makes the calcfuntions importable, which is now required for\nthem to be used in caching.\n\nAlso catching only ValueError when getting the process_class,\ninstead of a broad except."
    ]
  },
  {
    "pr_number": 4688,
    "commits_list": [
      "d87fa607ec23eca40b8f27808be5b21feedcd055",
      "1d5cd7fb63997122b902de4d205265076dde7c3a",
      "537becb946197f1f30e5552880a1cf2938e9b0c2",
      "d3ff1537e1079c03bb1eb2460bc0afc68713f57b",
      "1d2ee9d0deb977571b4afd9eccb5fd0cea739b84",
      "d3e5e9d86ca81b74dac4f6e4b442680b4088ef0b"
    ],
    "message_list": [
      "Improve namedtuples\n\nReplace old-style namedtuples with `typing.NamedTuple`.\nThis allows for typing and proper default values.",
      "Merge branch 'develop' into namedtuple-classes",
      "fix pre-commit",
      "Update utils.py",
      "Merge branch 'develop' into namedtuple-classes",
      "Merge branch 'develop' into namedtuple-classes"
    ]
  },
  {
    "pr_number": 1962,
    "commits_list": [
      "8a1e95de47f84e56bad59a89a7075821b4fe4ec1",
      "7e776e1f4011e769ae2989c95a2239085d8f14d4"
    ],
    "message_list": [
      "Implement method for JobCalculation to create a restart builder\n\nThe `get_builder` method will return a \"clean\" builder for the given\n`JobProcess`, without any inputs defined. Here we implement `get_builder_restart`\nfor the `JobCalculation` that will also return a `JobProcessBuilder` but with\nthe inputs and options pre-set with those that were used for the original node.\nThe builder can then immediately be resubmitted to run another instance of that\ncalculation, while of course giving the chance to change one or more inputs.\n\nFor example:\n\n    calculation = load_node(<pk>)  # Some completed JobCalculation node\n    builder = calculation.get_builder_restart()\n    results = run(builder)",
      "Merge branch 'develop' into fix_230_job_process_restart"
    ]
  },
  {
    "pr_number": 4087,
    "commits_list": [
      "76cf2f2f117cd2aec9b01f9fef73150870cc0bb2",
      "b42204df111337eb50c25edd40e03e549bd0a9da",
      "3049de614ab29945b20a0b0e603d8449b6ac688a"
    ],
    "message_list": [
      "Docs: Add how-to on running workflows",
      "Apply reviewer suggestion\n\nCo-authored-by: Carl Simon Adorf <carl.simon.adorf@gmail.com>",
      "Docs: Apply reviewer suggestions/Fix arithmetic"
    ]
  },
  {
    "pr_number": 2767,
    "commits_list": [
      "659b094476538a205cd393c075ea4e1b1d7116bd"
    ],
    "message_list": [
      "Remove hybrid properties from SqlaAlchemy and dummy models\n\nSoon the current hard-coded dummy model, to map the Django models on\nSqlAlchemy models, which drives the `QueryBuilder` implementation, will\nbe replaced with dynamically generated Aldjemy models. However, these do\nnot support hybrid properties so they have to be removed."
    ]
  },
  {
    "pr_number": 3138,
    "commits_list": [
      "0c66c9058323e96841c860082070f2204b9f0ade"
    ],
    "message_list": [
      "Move responsibility of loading profile to `ProfileParamType`\n\nCurrently, the loading of the profile is done in the body of the `verdi`\ncommand group itself. By moving this to the `ProfileParamType`, it\nbecomes easy for external scripts to also provide a `--profile` flag to\nrun the script for a profile other than the default.\n\nIn addition, prevent loading the ORM backend through the configuration.\nThe `load_profile` method was calling `configure_logging` with the\noption `with_orm=True` in order to make sure that the `DbLogHandler` was\nproperly configured for the new profile. However, `load_profile` should\nnot load the backend. This should be done lazily by the `Manager`. Since\nnow the profile is loaded by default when calling `verdi`, the backend\nwas always being loaded, even if not necessary, causing `verdi` and\ntherewith the tab-completion to grind to a halt."
    ]
  },
  {
    "pr_number": 757,
    "commits_list": [
      "a1e75f5c60ab1237fe89010a2cb66412309ced62",
      "84e7998710b497e7b96e14ca6309b516d3b3b8a0",
      "f212127f5e5a632f56e7c7dc6e8804c75abf0860",
      "b3f1b8703caa5f58ed06a4e59ede61d3ef538269",
      "baaa0f8869df2002b80e5fb264e5ef9915d3ab6b",
      "c8700fcf7822b6e9ca4fc0a6194c25efcfe42613",
      "c938eeff8a765ad783fac44a3f4099b6cdc7b2cf",
      "aa59d11e08d6d2e564e2d2a0eff9eff243ad5e92",
      "26a672cfa0ef37ee41bf491499f0db918e4ba915",
      "2557bb8caea6cee0861f0f3cf75530a5d014f543",
      "126acefb1b4ff5b691892777a69650b87d45bd81",
      "6b023d6a7ab0ba3703c697796c6076df14b49226",
      "e006fed03e777ce2a84269b3bea7357bc4d129b4",
      "f4ccf4240b91ad622fd92f35d18723b8c8ddc360",
      "29dd6db87b3e2a667a8e0e12d17463daef225eb4",
      "200c6f00cd7caa0f98153dd982a195317f2a0cd4",
      "519dbbc49eb98cf075a86c4f83732294338b2a3c",
      "cba1e7796e168220c317fcdd3db07ba111ad1cc6",
      "0d0c3e46d4015c21efcbd6874d394234fa8eeb16",
      "4a91ad0d99b8d3d9bcaadf543008e9b2d7655897",
      "dc209d68c3a9c148df61e0abb55b8969cf385f32",
      "d056c3c45659c7841d86135cc367b8300d9148a2",
      "75b773064bf094c79ce654411a6504f93aed7539",
      "b4d079221145730950109b4ea62d4229d0113de8",
      "80b12ebada3d0e9b90d8e2924421ec6ec64449a0",
      "e9dde01eaa10476aa39c840f525da309154fe9ca",
      "00fe7adb3cc37278b7a2972739009b7bcc1987da",
      "aaad33076eb4973713839a1ec416ea1246ab6ddb",
      "2d56ef9411ecf54a386ccab48e59077df8db5d41",
      "c700013ce94b0f467fcde0b519cf60a6d0c09987"
    ],
    "message_list": [
      "add test for quicksetup (only for travis)",
      "add backend option to quicksetup test call",
      "fixed typo TestRunner -> CliRunner",
      "try to get quicksetup to work",
      "fix #747",
      "add `verdi setup` test",
      "fix typo",
      "use pgtest cluster for setup test",
      "setup test: setup postgres instance correctly",
      "setup test: cleanup properly",
      "setup test: fix typo",
      "setup test: fix typo",
      "setup test: fix more typos",
      "setup test: make quiet",
      "update control.postgres unittest",
      "add test for user config in setup",
      "rename default _pg_execute_* for clarity",
      "make setup click command subcommand of verdi",
      "fix typo in test_user_config",
      "delete unused import",
      "simulate calling setup_cmd without the -p option to verdi",
      "setup test: make repo absolute path",
      "setup test: fix repo, backend",
      "setup test: fix missing import",
      "add test_setup script to pre-commit",
      "setup test: add confirmation to input for only-configure",
      "setup test: correct input",
      "setup test: skip unchanged profile attributes",
      "setup test: add more required input",
      "setup test: add more required inputs"
    ]
  },
  {
    "pr_number": 3348,
    "commits_list": [
      "3ac26b1ab447a1bdaac115e10908a2593f04c687"
    ],
    "message_list": [
      "Ensure exceptions when launching daemon runners are logged\n\nSince the daemon workers launched by the Circus daemonizer can except\nbefore the logging is configured `aiida.engine.daemon.runner.start_daemon`\nthe stderr stream of those sub processes needs to be also forwarded by\nthe watchers for them to end up in the daemon log file. If not these\nstack traces are lost forever and the user won't notice that there is\nsomething wrong. Especially because `verdi daemon status` will actually\nshow the daemon to be running with an active worker, it is just that it\nis a single worker everything, dying and immediately being revived."
    ]
  },
  {
    "pr_number": 5549,
    "commits_list": [
      "7ab2a873d29128c52d2711f4091942c7117c0c06"
    ],
    "message_list": [
      "ORM: raise when trying to pickle instance of `Entity`\n\nThe ORM entities should not be pickled since it is not clear whether the\nconnection to the storage backend should be included as well and if it\nwere, implementing that in a consistent way across implementations is\nnot feasible.\n\nTherefore, we now raise an explicit an `InvalidOperation` when an\ninstance of `Entity` is being pickled. This is achieved by overriding\nthe `__getstate__` dunder method which will be called by the `pickle`\nmodule when dumping a class instance."
    ]
  },
  {
    "pr_number": 4348,
    "commits_list": [
      "78b73dc186c62cdcdc2c30f7c2dbc4acf866f3b3"
    ],
    "message_list": [
      "`TemplateReplacerCalcJob`: make `files` namespace dynamic\n\nThe `files` namespace is supposed to accept any `SinglefileData` or\n`RemoteData`, but it was not marked dynamic explicitly. In that case,\nonly explicitly defined ports are excepted, which is not what is\nintended here."
    ]
  },
  {
    "pr_number": 1566,
    "commits_list": [
      "9cdef10a92e069d504c70ed44959398ee7282b41",
      "c3993d6866a0cd7e1d07ca59490a393f8af5994f",
      "535381ee236be0d1f3625afd7e24d6ea9f5c9150",
      "f899395f263b2a1052535a053dcee41181754b64"
    ],
    "message_list": [
      "load plugin commands before trying to call them",
      "load plugin commands only in click.Group subclass",
      "stop sphinx from trying to link to click docs",
      "move nitpick exceptions to the exceptions file"
    ]
  },
  {
    "pr_number": 4467,
    "commits_list": [
      "d0d80acbfd929cd97a5dc85d82e5921904ea6b18",
      "6a57672e5a717d710bb3e29fe549901b8b7570ab",
      "98c15798ee6d1bf0ad463abf071a950cce377636",
      "bbbaae774c5d1feeaba9833108759e03be093533",
      "88102107405b7a6a865d73b3c8dc32e064a3ff63",
      "9e60ad10c00a77c414a30012ce7464929f70069a"
    ],
    "message_list": [
      "Add info on export format to internals section",
      "Remove all section + add extra info",
      "Change references to export => archive (+includes)",
      "Apply corrections + redirections.",
      "Merge branch 'develop' into docs-4037",
      "Merge branch 'develop' into docs-4037"
    ]
  },
  {
    "pr_number": 1684,
    "commits_list": [
      "b9c527b5e7ee4d0b9a292b7c68ccb068381bf7ca",
      "057442027edaf47eb2f91e5256a5abd7dd82049a",
      "9e54c223820d0d980d3e5cf0be3c62dcc4494670",
      "38777a08c1bdc252b2dfcacd4a23eeb29b85ee36",
      "f065ca33f38b1b9914c6b43815fb34af8e0d1b00"
    ],
    "message_list": [
      "make prompts yellow (add help for invalid args to prompts)",
      "add prompt usage message at the beginning.",
      "update interactive option tests for new start message",
      "fix remaining interactive option tests",
      "add interactive.py to pre-commit"
    ]
  },
  {
    "pr_number": 5653,
    "commits_list": [
      "ca7290dfbf5886580ccc9d586d11ce4f5d332383",
      "441312c4e40c80da990bc6996f887950bb354938",
      "9970edf37d104adeaeffd9bf7e9732965e667346",
      "b89366c0ec43b82adebe1b6e14b05c3fc62f6469",
      "d2323d462a9c84ba83f24117fb78e5b102a53e85",
      "0ac70b781a1bd403278502ab9cc0dfa0bd83a2c4",
      "8b55a2aa5036db218d46e8e37cd6d85dc30d5ada",
      "7a4532e80e8bf491b666b0251e4349b7528e7647",
      "654d4fe298ebc0f0b8ab6c79ccab7db18342affc",
      "e4da3808b836175f5d1ad6a27368501483962bf6"
    ],
    "message_list": [
      "`Repository.copy_tree`: omit subdirectories from `path` when copying\n\nThe intention for `copy_tree` was to copy only the contents of the\nrelative subpath, however, it was including the subdirectories. This was\nunintentional and users also probably won't expect this behavior. For\nexample, when copying the content of the `relative/sub` subdirectory,\none wouldn't expect the content to be copied inside a `relative/sub`\nfolder as well. This is also not the behavior of `shutil.copytree`.\nWhen executing `shutil.copytree('relative/sub', target`), the folder\n`relative/sub` won't be present in the `target` directory.\n\nThe solution is to make the target path of the file to be copied\nrelative to the source path within the repository.\n\nAlthough this change is technically breaking backwards compatibility, it\nis correcting a bug of a feature that was introduced in `v2.0`. It is\nvery unlikely that any plugins started depending on this behavior so we\nstill have the possibility to fix it now.\n\nCherry-pick: 7a39ad21256d3ad8ca156781277b92c182da0801",
      "`CalcJob`: fix bug in `local_copy_list` provenance exclusion\n\nWhen the `local_copy_list` was provided a directory as source, the\ncontents would be copied to the sandbox folder in `upload_calculation`.\nThe point of using the `local_copy_list` is to guarantee that the files\ncopied this way, are not also copied to the repository of the process\nnode, since this would duplicate content. This was accomplished by\nadding the files copied through this method to `provenance_exclude_list`.\n\nThe implementation contained a bug though when the `local_copy_list`\ninstruction also defined a target. In this case the target was not\nincluded in the paths to be excluded and so were missed when copying the\ncontents of the sandbox to the repository of the process node. This is\ncorrected and regression tests are added.\n\nCherry-pick: 0bfe0008e27076fffba8dbcc4d3883509e1e4401",
      "\ud83d\udc1b FIX: SQLite: apply escape `\\` in QueryBuilder `like` and `ilike` (#5553)\n\nFor `like` and `ilike`,\nthe special characters `_` and `%` may need to be escaped if they have literal meanings (`LIKE .. ESCAPE` in SQL).\nHowever, the SQLite backends do not implicitly treat `\\` as escape sequence (unlike Postgres): https://sqlite.org/lang_expr.html\n\nThis commit makes `\\` explicitly an escaping character for SQLite backends. Note, the use of `\\` for escaping is already hard-coded in aiida-core.\n\nCherry-pick: 29a77643634439b682516302fe50f9addd093372",
      "\ud83d\udc1b FIX: Archive creation after packing repository (#5570)\n\n`disk_objectstore.PackedObjectReader` does not implement the ability to `SEEK_END`,\nrequired to determine the size of a stream.\nThe size of the stream is required to determine if the ZIP64 extension should be used,\nso in this case, we now defer to `force_zip64=True`.\n\nCherry-pick: 46d244e32ac5eca2e22a3d088314591ce064be57",
      "Engine: fix bug that allowed non-storable inputs to be passed to process (#5532)\n\nThe basic assumption for a `Process` in `aiida-core` is that all of its\ninputs should be storable in the database as nodes. Under the current\nlink model, this means that they should be instances of the `Data` class\nor subclasses thereof. There is a noticeable exception for ports that\nare explicitly marked as `non_db=True`, in which case the value is not\nlinked as a node, but is stored as an attribute directly on the process\nnode itself, or not stored whatsoever.\n\nThis basic rule was never explicitly enforced, which made it possible to\ndefine processes that would happily take non-storable inputs. The input\nwould not get stored in the database, but would be available within the\nprocesses lifetimes from the `inputs` property allowing it to be used.\nThis will most likely result into unintentional loss of provenance.\n\nThe reason is that the default `valid_type` of the top-level inputs\nnamespace of the `Process` class was never being set to `Data`. This\nmeant that any type would be excepted for a `Process` and all its\nsubclasses unless the valid type of a port was explicitly overridden.\nThis meant that for normal dynamic namespaces, even non-storable types\nwould be accepted just fine. Setting `valid_type=Data` for the input\nnamespace of the `Process` class fixes the problem therefore.\n\nCherry-pick: 5c1eb3fadf257faa978ed6dee961f7d01ee21c48",
      "Engine: fix bug when caching from process with nested outputs (#5538)\n\nWhen the execution of a process is being skipped because it has a valid\ncache source and caching is enabled, the process would except if the\nprocess has nested output namespaces. Since the nesting cannot be\nliterally preserved in the database, the nesting is representing by\nusing double underscores between each level of nesting in the link\nlabel.\n\nThe problem occurs when `Process._create_and_setup_db_record` would be\ncalled which would create the outputs of the cloned node, however, it\nwould use these \"collapsed\" link labels containing the double\nunderscores. Passing these to `self.out` would cause the validation to\nfail because the process port validation expects the actual nesting.\n\nThe solution is to restore the database nesting representation to the\none expected by `Process.out`, which amounts to replacing the namespace\nseparators.\n\nCherry-pick: b936da373499c63f4af4bcb9024500ffc9bf7f9e",
      "\ud83d\udcda DOCS: Add intersphinx aliases for __all__ imports (#5657)\n\nThis commit adds alias objects to the sphinx python domain.\nThis allows for any project to reference, e.g.\n`aiida.orm.Data` instead of `aiida.orm.nodes.data.data.Data`, etc.\n\nCherry-pick: 359116382b4eb0ffe708f0b36301c18f9f31f28f",
      "\ud83d\udc1b FIX distinct queries (#5654)\n\nThis commit principally fixes issues with distinct queries.\nSince refactoring for sqlalchemy 1.4,\nthe query has been initialised with a starting \"dummy\" projection from which to join.\n\n```python\nquery = session.query(starting_table.id)\n```\n\nThe returned projection was then removed.\nThis is problematic, though, when requesting distinct result rows,\nbecause the dummy projection is also used to calculate uniqueness.\n\nThis has now been changed to use the `select_from` method:\nhttps://docs.sqlalchemy.org/en/14/orm/query.html#sqlalchemy.orm.Query.select_from,\nsuch that now we can initialise without any projection.\n\n```python\nquery = session.query().select_from(starting_table)\n```\n\nThe backend QueryBuilder code is also refactored,\nprincipally to make the `SqlaQueryBuilder._build` logic more understandable.\n\nCherry-pick: 9fa2d88a7fdb2c144d4949bd80d32b8b10f40f87",
      "Release `v2.0.4`",
      "Merge remote-tracking branch 'origin/main' into release/2.0.4"
    ]
  },
  {
    "pr_number": 3389,
    "commits_list": [
      "0abc635b672ce0b4068927ae567a8ff4fde8aa90"
    ],
    "message_list": [
      "Prune input mapping of `ProcessBuilder` before creating `Process`\n\nThe validation of a `ProcessSpec` will trigger for a port as soon as a\nvalue has been passed. For a port namespace this is also the case, even\nif it contains no actual values. This was a problem when launching a\nprocess from a `ProcessBuilder`. If it contained an optional namespace,\nit would always pass at least an empty dictionary for said port, even if\nthe user did not specify any specific input within it. This would then\nalways trigger the validation which would fail. This limitation made the\nconcept of optional port namespaces fundamentally incompatible with the\n`ProcessBuilder`. Here we add a method to \"prune\" the input mapping of a\nbuilder before it is being passed to the process constructor. This will\nremove any portnamespace that is completely empty, meaning it contains\nno other values other than other empty dictionaries (namespaces)."
    ]
  },
  {
    "pr_number": 1909,
    "commits_list": [
      "71df10191c1c98ea6eb12bebdf7092dbccc12248",
      "beb816b735f47bb6953eb8d4d78dc63ab3cb5500",
      "e39914a9ffdac524c665e6ea3b8d6d577fcde7bf"
    ],
    "message_list": [
      "fix #1900 by removing `preexec_fn=os.setsid`",
      "Merge branch 'develop' into fix-1900",
      "Merge branch 'develop' into fix-1900"
    ]
  },
  {
    "pr_number": 5736,
    "commits_list": [
      "dfe1764d93710ea94b7dbd183e747e873a1157a0",
      "0c5890c6e05cdfb1452762382867002c6ba80b52"
    ],
    "message_list": [
      "`QueryBuilder`: Add test that excepts when mutating nodes in `iterall`\n\nCurrently, when iterating over the rows of a query using `iterall` or\n`iterdict`, will raise `sqlalchemy.ProgrammingError` if any of the nodes\nreturned by the iterator are mutated, for example by setting an extra.\n\nThe reason is that adding an extra is mutating the node, and the model\nis wrapped in the `ModelWrapper` class which will automatically call\nsave if a field of a stored node is changed. This is calling `commit` on\nthe session which is invalidating the cursor. The documentation of\nsqlalchemy explicitly states that one should not commit on a cursor that\nis being used to iterate over a query with `yield_per`.",
      "`QueryBuilder`: use a nested session in `iterall` and `iterdict`\n\nThis will prevent the `ModelWrapper` from calling commit on the session\nwhen any of the yielded results is mutated as this will cause the cursor\nof the connection to be reset and it to become invalid. In the next\niteration of the yield an exception will be raised.\n\nNote that a session transaction should only be opened if we are not\nalready inside one, in which case a `nullcontext` is used."
    ]
  },
  {
    "pr_number": 3464,
    "commits_list": [
      "b9ce74bb9bee3a27418292ef1d1d6008e20366b7",
      "5d4b2d8421c05be42097b4f6335abae1f1c94186",
      "69991d137c12cf2a3b9cebd152491acdd9f1af45",
      "30e9703446b099f89d26e8d604e509fddbc4a078",
      "698553a4c9ef8540f8795cc07cba9f1f25da9bdf",
      "71321a19ee5083ec6044148cec784dacaa4bd733",
      "aaf12c10abcfda13cea2bc7562ecb5e7263c4054",
      "3931460a9c0dddc3561774d6962d2a2e88ccc37b",
      "f496488d0483125d663861f1b2310bc0f72660e4",
      "238c32e177004f941c8b52db7ae5330bd641a65e"
    ],
    "message_list": [
      "Reduce the number of top-level sections in \"Working with AiiDA\".",
      "Combine the \"Concepts ...\" and \"Workflow ...\" sections.\n\nCombining the \"Terms and Concepts\" and the \"Workflow Development\"\nsections into a \"Reference Guide\" section.",
      "Move the development sections one level down.",
      "Move REST API section into API chapter.",
      "Docs: minor restructuring\n\n * Move \"Getting started\" within \"Working with AiiDA\"\n * Merged \"Python API\" and \"Scripting\" into \"Python interface\"\n * Renamed \"Calculation and work functions\" to \"Process functions\"",
      "Use sub-sections instead of a list in \"Groups\" section.\n\nThis also avoids the issue of having a single sub-section in the\nsection.",
      "Remove non-sensical reference to previous section.",
      "Update the \"Working/Data\" section headers.",
      "Add header to intro to \"Working/Process Functions\" section.",
      "Merge branch 'develop' into fix/issue-3447"
    ]
  },
  {
    "pr_number": 3847,
    "commits_list": [
      "05fc0a7bd92f970394d0acc970afd75cb774f317",
      "be597213a719a5e96b3523002e7f6f91d30c52fd",
      "c5a3ff6c3af0e12e2ca66f8d96a364e5bb8d2523",
      "26da37e87a5ad12bb5e993ef98058083f499af43",
      "8645aa3cefbdd0037ce6d186526acbbb4cce80d9",
      "fa2f7055b05d1006e96aa2a3edc6087459fd53a3",
      "9b78a01ac10822b83b3a5413a9ffa34dd67ac86c"
    ],
    "message_list": [
      "Implement the dependency_management 'check-requirements' command.\n\nTo check whether the environments frozen in the 'requirements/*.txt'\nfiles are matching the dependency specification of 'setup.json'.",
      "Implement 'check-requirements' GitHub actions job.",
      "Checkout specified head_ref in update-requirements workflow.",
      "Execute 'update-requirements' workflow only upon repository_dispatch.\n\nWith type 'update-requirements-command'.",
      "Remove unnecessary line break.",
      "Add missing 'DEFAULT' argument to 'check-requirements' command.",
      "Apply suggestions from code review\n\nCo-Authored-By: Leopold Talirz <leopold.talirz@gmail.com>"
    ]
  },
  {
    "pr_number": 5680,
    "commits_list": [
      "a39f211e4f12d7e281c9c135ff5359ccecff0390"
    ],
    "message_list": [
      "`Code`: Use deprecated legacy code as base class for successors\n\nThe legacy `Code` class was deprecated in favor of a more generic and\nimproved `AbstractCode` class. The \"remote\" and \"local\" variants that\nthe single `Code` class provided, were recreated as separate\nimplementations of `AbstractCode`: `InstalledCode` and `PortableCode`.\n\nHowever, by having them subclass `AbstractCode`, any plugin code that\nwould perform a type check on a code input based on the `Code` class,\nwould now fail. A workaround is to have the two new concrete\nimplementations actually subclass `Code`. Since `Code` was made to\nsubclass `AbstractCode`, they still inherit all the functionality from\nthat base class as they need it. The one change is that whenever they\ncall `super` they have to explicitly call `super(Code, self)` to skip\nthe implementation of `Code` and instead go straigh to `AbstractCode`.\nOnce `Code` is officially removed, these workarounds can also be removed\nand the normal `super` call can be reinstated."
    ]
  },
  {
    "pr_number": 3962,
    "commits_list": [
      "77692cee2020f00c740db8225cf385f2acbed114"
    ],
    "message_list": [
      "Require Click ~=7.1.\n\nTo test the revised 'update-dependency' workflow. Do not merge."
    ]
  },
  {
    "pr_number": 5830,
    "commits_list": [
      "158d33145317326c8eefbfe369c801eb8e88d79a",
      "ead544bb5674d75f54e6da0fd053adc82e47754b"
    ],
    "message_list": [
      "ORM: Fix typing of `aiida.orm.nodes.data.code` module\n\nThe original type `PurePath` being returned by `filepath_executable` and\n`get_execname` has been changed to `PurePosixPath` since currently the\nrest of AiiDA's infrastructure requires/assumes that `Computer`s are\nrunning POSIX operating systems.",
      "Merge branch 'main' into fix/5819/orm-code-typing"
    ]
  },
  {
    "pr_number": 5267,
    "commits_list": [
      "74c75225ba527562394c6774b9f91f532eae4396",
      "24c2145f1f0193e0f42c0808c6219ca144dac46d",
      "fa2e3c782f6a6e07aaceff6b985479c7e16dcc65"
    ],
    "message_list": [
      "Fix migration for adding `core` prefix\n\nSome of the SQL commands for the migration that adds the `core` prefix\nhave typo's in them. This meant that the entry points weren't migrated\ncorrectly.\n\nHere we fix the typo's for `KpointsData` and `RemoteStashFolderData`.",
      "Merge branch 'develop' into fix/5266/core-migration",
      "Update legacy v0.12 test archive"
    ]
  },
  {
    "pr_number": 3566,
    "commits_list": [
      "be778650f8e293607c229e41efcb92f3abe29253",
      "1340ce924df601856fbd07ca17aaf26da5a920cd",
      "05e369765c36ce55044af14d531229cd364823db",
      "e6515b7c8b7d4c02f69e741a3d63e10da679bd72",
      "662daff5826d694a9f8dd08630f3351e7cf8b328",
      "a6d5d501651849968745514d841b988d6df9f1b5",
      "d99083ecae3b3e047119fa9cad69f3042f46106c",
      "b8c458b7cf91bc50b3bf9dbc18a9820f3d903880"
    ],
    "message_list": [
      "Drop Python 2: start removal of python 2 compatibility\n\nRemove python 2 from supported version in `setup.json` and remove all\ndependencies that were only necessary for `python<3.5`. Additionally\npython 2 is removed from the build matrix on Travis.\n\nAdd explicit `python_requires` keyword to `setup.json` which will ensure\nthat `pip>=9` and other clients that support the metadata 1.2 spec\nonly install a compatible version for the current Python runtime when\ninstalling the package.",
      "Drop Python 2: remove all `from __future__` statements\n\nThe removal of all future statements was performed with the following:\n\n    find . -type f -not -path './.git*' -exec sed -i '/from __future__.*$/d' {} +\n\nThe `modernizer` hook is also removed from the pre-commit config.",
      "Drop Python 2: remove `six` and all the references in the code\n\nThe majority of adaptations was done with the following commands:\n\nfind . -type f -not -path './.git*' -exec sed -i '/\\(import six\\|from six\\).*$/d' {} +\nfind . -type f -not -path './.git*' -exec sed -i '/\\(six\\.add_metaclass\\).*$/d' {} +\nfind . -type f -not -path './.git*' -exec sed -i 's/six.integer_types/int/g' {} +\nfind . -type f -not -path './.git*' -exec sed -i 's/six.string_types/str/g' {} +\nfind . -type f -not -path './.git*' -exec sed -i 's/six.text_type/str/g' {} +\nfind . -type f -not -path './.git*' -exec sed -i 's/six.binary_type/bytes/g' {} +\nfind . -type f -not -path './.git*' -exec sed -i 's/six.moves.StringIO/io.StringIO/g' {} +\nfind . -type f -not -path './.git*' -exec sed -i 's/six.StringIO/io.StringIO/g' {} +\nfind . -type f -not -path './.git*' -exec sed -i 's/six.moves.BytesIO/io.BytesIO/g' {} +\nfind . -type f -not -path './.git*' -exec sed -i 's/six.BytesIO/io.BytesIO/g' {} +\nfind . -type f -not -path './.git*' -exec sed -i 's/six.moves.range/range/g' {} +\nfind . -type f -not -path './.git*' -exec sed -i 's|six.iterkeys(\\([^)]*\\))|\\1.keys()|' {} +\nfind . -type f -not -path './.git*' -exec sed -i 's|six.iteritems(\\([^)]*\\))|\\1.items()|' {} +\nfind . -type f -not -path './.git*' -exec sed -i 's|six.itervalues(\\([^)]*\\))|\\1.values()|' {} +",
      "Drop Python 2: remove useless inheritance of `object`\n\nThis is no longer needed for python 3.",
      "Drop Python 2: adapt the Sphinx process extension test reference",
      "Drop Python 2: simplify various compatibility work arounds",
      "Drop Python 2: remove explicit declaration of strings being unicode\n\nIn python 3 strings are stored as unicode by default so now that we\ndropped pyhon 2 support there is no need to keep prefixing them with the\nstring encoding declaration `u`.",
      "Drop Python 2: remove arguments from `super` calls\n\nThis is no longer necesary in python 3. To remove all the occurrences\nof this usage, the following command was used:\n\n    find . -type f -not -path './.git*' -exec \\\n    sed -i 's/super([^)]*,[^)]*)/super()/g' {} +\n\nThe regex tries to find all instances of `super()` where there is\ncontent between the parentheses. The search for `[^)]*` means to look\nfor all characters except a closing parens, which essentially makes the\nsearch non-greedy. Having just that would not be enough and we have to\nexplicitly add another such clause separated by a comma. This is the\nexact format required by python 2 where the `super` call expects two\narguments, first being the class itself and the second the reference,\ntypically `self` or `cls. By making the regex more specific we avoid\nmatching cases like:\n\n    def test_without_super(self):\n\nwhich would be replaced to\n\n    def test_without_super():\n\nif we don't include the explicit comma in the regex between the parens."
    ]
  },
  {
    "pr_number": 3118,
    "commits_list": [
      "3677b18452720ebe7f107ec9ea9fc68982b43a1c",
      "4cca568190ffc6fd475e3bca7868d06e7b619262"
    ],
    "message_list": [
      "remove stale daemon pid in status check",
      "Merge branch 'develop' into daemon-stale"
    ]
  },
  {
    "pr_number": 3798,
    "commits_list": [
      "9f5170523ef8488b5bb45685cb88bb1ec4a19c79",
      "80d893596f39c333668bc95eaf0eefc5fb67f515",
      "8c0785aeab5a4682c41113447c4370b850643060",
      "cedb3090a09ace18a120234d87e597afd2bab64f",
      "651f44dd1bdc6f7372b9760243ddd646c978dfb3",
      "637f8260b5806cd14b00c0cd6ef16bf4db4de923",
      "610db3129732ae2f60b242d6c94eab4ddb66d1a5",
      "d6d7b76ea215e9fa7c5a06db04b1f6051b2a5674",
      "c2599f54dac3c83ffbdf9c72fec025d2f163aa56",
      "68621fcd8bb7175853dac0444e4af01fda83670b"
    ],
    "message_list": [
      "plot energy levels when only 1 kpoints",
      "seperate plot code by template",
      "move _prepare_mpl_* together",
      "address problem",
      "add band gnuplot plot",
      "not break bands datalist test",
      "pre-commit",
      "click file write test gnuplot",
      "Merge branch 'develop' into plot_energy_level",
      "Fix issue #2462: bands export error"
    ]
  },
  {
    "pr_number": 4265,
    "commits_list": [
      "ea1882ed957350fba94fe0e1326082d2b4cf22c3",
      "55e1210a0f985572aeb38b247c2805c3d549649f",
      "54cab83dde747cc09be9d738ad7562003a6ad6f9",
      "db956b2201af272cb18768bb94a1e5ef5630cc2e"
    ],
    "message_list": [
      "Pin Python version for test-install:pip workflow to 3.7.7.\n\nAs a stop-gap solution for issue #4256.",
      "Update Python versions for shared steps in test-install workflow to 3.8.",
      "Merge branch 'develop' into ci/issue-4256-pin-python-to-377",
      "Consistently use setup-python action v2 in all GA workflows."
    ]
  },
  {
    "pr_number": 654,
    "commits_list": [
      "992cd63cb1a352c109466e11d522c4e9943a9b7c"
    ],
    "message_list": [
      "Reverting several commits made for issue #641 regarding input links\n\nThe changes made all passed the tests but after more extensive testing\nit was found that the changes break the legacy workflows. The changes\nallowed the existence of multiple links of the same type with the same\nlabel, as long as the source node was different. The legacy workflow\nand calculation plugins bank on the link label for a given link type\nto be unique but this was no longer the case. The link system will\nneed a big overhaul soon anyway, so it will be better to leave the\ncurrent state unchanged and do the change in a separate release.\n\nThis reverts the following commits:\n\n * 627a7648e453b34c7865e373237113a5fe532826\n * 11741dd4dc6c95e2a82347be3187ab19b001f9d5\n * 777f80d2e09647447580f13c8af58b201dcd073d\n * aeac53b5b390d0093cad82053e30129c328df483"
    ]
  },
  {
    "pr_number": 3571,
    "commits_list": [
      "2eefb88957dec212fc92134fda4ec61ab16168c3",
      "fab1c56ee5b1a59fa68fc3f4b191e8daf37c9c5a"
    ],
    "message_list": [
      "Enable CI on Github\n\nAdd a GitHub workflow `.github/workflows/ci.yml` that will trigger on\npush and pull requests. It will run the five actions it defines:\n\n * `conda`: will install conda and create the environment\n * `docs`: builds the documentation with nitpick warning\n * `pre-commit`: runs pre-commit on all files\n * `tests`: runs `verdi devel tests` and the stand alone test files\n * `verdi`: runs the tests that check the load time is acceptable\n\nAll tests are performed for python 3.7 except for the `tests` action\nthat is done for 3.5 as well. Both python versions are run for both\ndatabase backends in a matrix strategy. Even though we support 3.6 as\nwell we do not explicitly test it since it will require another two\nbuilds and testing 3.5 and 3.7 should give decent guarantees.\n\nFinally the argument for multiple individual actions instead of joining\nthem is based on the fact that there does not seem to be a limit on\nconcurrent number of actions on GitHub as of this writing. This means\nthat by spreading them out, allows running them in parallel which should\nreduce the overal runtime of the continuous integration workflow.",
      "Remove Travis configuration and simplify Jenkins setup\n\nThe majority of tests are now run through a GitHub CI workflow. This\nmeans it is not necessary to also run those on Jenkins, especially that\nJenkins was originally configured to just run additional tests that were\nmore computationally intensive. Therefore, we only keep the RPN test to\nrun on Jenkins for now."
    ]
  },
  {
    "pr_number": 3857,
    "commits_list": [
      "ec35a0b3957abbfb8e769d63cdfbf6a753a8f78f"
    ],
    "message_list": [
      "Add the `-l/--limit` option to `verdi group show`"
    ]
  },
  {
    "pr_number": 5029,
    "commits_list": [
      "9e220e4952cbec0f5813ec6716e2f7618d8d24d8"
    ],
    "message_list": [
      "CI: Test direct installation via conda.\n\nInstead of just testing whether we can install into the environment\ndefined by environment.yaml."
    ]
  },
  {
    "pr_number": 3242,
    "commits_list": [
      "66c8f139028c9f3ccd1c4c4270ccbe51c6d90a56",
      "a5824d6798f2df07a87efad865d7be9be63a4079",
      "765d6a71f7aa1b8f182c7fc1cf85b45ea4285ad4",
      "bda651996a8051313e3876f4119fba37654ce8ce"
    ],
    "message_list": [
      "Rewrite `extract_tree` for archive import\n\nThe function `extract_tree` now returns a `Folder` object, pointing to\nthe archive folder, which should not automatically erase the folder\nafter import.\n\nCorrect import functions for extract_tree.\nThis was a problem when creating the new Node repository, since the\nimport functions would move the source directory (i.e. delete the source\ndirectory after a successful copy).",
      "Rework `extract_tree` and unpacking archives\n\nThe extraction of export archives is now handled by the `Archive` class,\nwhich will point to a `SandboxFolder` if the archive is not a folder, in\nwhich case it will point to the folder, making it a `Folder` object\ninstead - as to not erase the archive when done with it.\n\nThe use of the extraction functions now solely fall upon the `Archive`\nclass.",
      "Archive class is used for all migrations\n\nThe Archive class has been introduced to migrations and tests alike,\nwith the purpose of reducing \"noise\" and making the focus of Archive\nclear for imports and dealing with export archives.",
      "Addressing some suggestions by @sphuber\n\nThis commit handles the more easily implemented suggestions and should\nlater be merged with other commits, where it makes sense."
    ]
  },
  {
    "pr_number": 2576,
    "commits_list": [
      "6b532b6402362bc21f357d585ce1a94980d98b25",
      "20a44a09a78781f90b71d82795112cabc9cdca1e"
    ],
    "message_list": [
      "Import/Export tests: Add decorator for temporary directories",
      "Update tests with `@with_temp_dir` decorator.\n\nFix `test_multiple_import_for_single_node` in `TestLogs`.\nMake sure \"existing\" `Log` UUID is checked correctly."
    ]
  },
  {
    "pr_number": 3921,
    "commits_list": [
      "76ed3f6227d7c1d35edfe32cc102c67ca83d21b7"
    ],
    "message_list": [
      "Unpin the `click` dependency\n\nThis was pinned because `7.1` broke our pre-commit hooks.\nThe breaking of the tests was caused by two separate commits:\n\n  * 718485be48263056e7036ea9a60ce11b47e2fc26\n  * 37d897069f58f7f2a016c14b0620c6d387430b4b\n\nThe first one changes the format of the output of a command if a\nrequired option is missing. The double quotes around the option have\nbeen changed to single quotes.\n\nThe second commit is more pernicious: the `Editor.edit_file` method was\nupdated to escape the editor and filename parameters passed to the sub\nprocess call. Our tests were abusing the absence of escaping to pass\narguments to the editor command, in our case `vim`, as to make the\nnormally interactive command a non-interactive one for testing purposes.\nNow that the editor command is escaped, the arguments are understood by\nbash to be part of the command which of course then cannot be found and\nthe test fails. We \"fix\" this problem by patching the changed method in\n`click` to undo the escaping."
    ]
  },
  {
    "pr_number": 3940,
    "commits_list": [
      "c6ae2623162462ce7ae177be411ebc82053a7aa5"
    ],
    "message_list": [
      "Remove the equality operator of `ExitCode`\n\nThe operator implementation assumed that `other` would always be an\ninstance of `ExitCode` as well and so therefore was guaranteed to\ncontain the same attributes. However, comparing it to a bare tuple would\nalso invoke the `ExitCode.__eq__` method and would raise and\n`AttributeError` as a result. The desired behavior for the equality\noperator is that it returns True for any other tuple of the same length\nand content. Since this is exactly what is implemented by the tuple\nbase type, we do not have to override it at all."
    ]
  },
  {
    "pr_number": 3935,
    "commits_list": [
      "173f76f96e635341c18d14982d25ad58bf82879b"
    ],
    "message_list": [
      "Do not ignore `type_string` in constructor of `Group`\n\nDoing so would actually break backwards compatibility. Code that creates\ngroups with explicit custom type strings, would no longer be able to\nquery for them as the type string was silently converted to `core`. Even\nthough accepting the passed `type_string` will cause warnings when\nloading them from the database, that is preferable then breaking\nexisting code."
    ]
  },
  {
    "pr_number": 5094,
    "commits_list": [
      "7629c16912d450c4e99d4d92b4a1e799dcb851c0",
      "4e322ce3cf525417e033d26a59d86cb5f45c05b7"
    ],
    "message_list": [
      "\ud83d\udd27 MAINTAIN: Add `verdi devel run-sql`\n\nFor quickly running raw SQL commands on the database",
      "Add a basic test for the new command"
    ]
  },
  {
    "pr_number": 1374,
    "commits_list": [
      "9e1d40ea4e1ffbc407854b3683361b8568569e3f"
    ],
    "message_list": [
      "Enable running of sphinx-apidoc on readthedocs"
    ]
  },
  {
    "pr_number": 5405,
    "commits_list": [
      "e71e07583a14a89ea55bd0cab207fc83d41b6bcc",
      "6d11b55f4987c7e93d9c9bc1ac839ba21fa7b7aa",
      "0754aff8570a5babd218fe9194bd00f280955333",
      "d6dab5e9b4bf8c9faff34180740299d92aa32dbc",
      "af4ab3d833ded5851070f6999c61b8f444eab803",
      "e69ba2dda75934e02d920da04d9aec0fe1d4b9d3"
    ],
    "message_list": [
      "\ud83d\udcda DOCS: Replace sphinx-panels with sphinx-design\n\nsphinx-panels is deprecated,\nsee: https://sphinx-design.readthedocs.io/en/furo-theme/get_started.html#migrating-from-sphinx-panels",
      "center text in cards",
      "increase gutter",
      "Improve landing page",
      "cards: Increase shadow, remove title bold",
      "Update get_started.rst"
    ]
  },
  {
    "pr_number": 5860,
    "commits_list": [
      "48d4e0b59098eff8c55873d4394ed813c6cf80aa",
      "1aca65b5c4c8afffbbf4d3a78f703ed966294238"
    ],
    "message_list": [
      "CLI: verdi export to a yaml file with keys from code class\n\nfixes #3521\n\nAdd `verdi code export` command to export code from command line as a\nymal file.\nThis is mentioned in usability improvement as well as having a command to export the code and computer setup.\nKeys of YAML file are read from the cli option of the corresponding code class.",
      "Merge branch 'main' into fea/xx/export-code-setup"
    ]
  },
  {
    "pr_number": 2070,
    "commits_list": [
      "6e2e9413f8fc3982909205a564e7598666800760",
      "c12f7113c68a3a65c3699ea4491fadb8a39075e2"
    ],
    "message_list": [
      "Ensure that `export_tree` respects `create_reversed` for Data nodes\n\nThe `create_reversed` flag was only being applied when traversing the\ngraph from a `Calculation` to `Data` node, but not reversed. This made\nit impossible to export _only_ data nodes without its creators even\nif `create_reversed` was set to `False`.",
      "Merge branch 'develop' into fix_2068_export_data_create_reversed_false"
    ]
  },
  {
    "pr_number": 4316,
    "commits_list": [
      "c85e929c2227c9882868c969c76573a7fa092d2e"
    ],
    "message_list": [
      "`verdi computer test`: fix bug in spurious output test\n\nThe test that checks for spurious output when executing a normal command\non a computer over the transport had a bug in it that went unnoticed\nbecause the code path was not tested. If the `stderr` of the command\ncontained any output the command would raise because the test that is\ncalled `_computer_test_no_unexpected_output` would incorrectly return a\ntuple of length one instead of two in that case.\n\nIn addition to adding tests to hit this code path, the message that is\nprinted in the case of non-empty stdout or stderr is deduplicated and\nadapted to be bit clearer and refer directly to the documentation\ninstead of through a Github issue."
    ]
  },
  {
    "pr_number": 4370,
    "commits_list": [
      "882b75e3b675d8f3b699da96cbac91bab1dddf89"
    ],
    "message_list": [
      "`CalcJob`: improve logging in `parse_scheduler_output`\n\nThe level of the log that is fired if no detailed job info is available\nis changed from `WARNING` to `INFO`. Since not all schedulers implement\nthe feature of retrieving this detailed job info, such as the often used\n`DirectScheduler`, using a warning is not very apt. If the information\nis missing, nothing is necessarily wrong, so `INFO` is better suited.\n\nOn the contrary, if the `Scheduler.parse_output` excepts, that is grave\nand so its level is changed from a warning to an error.\n\nFinally, a new condition is added where the scheduler does implement the\nmethod to retrieve the detailed job info, but the command fails. In this\ncase, the return value will be non-zero. This value is now checked\nexplicitly and if the case, a info log is fired and the detailed job\ninfo is set to `None`, which will cause the parsing to be skipped. This\ncase can for example arise when using the `SlurmScheduler` plugin, which\ndoes implement the detailed job info feature, however, not all SLURM\ninstallations have the job accounting feature enabled, which is required\nby the plugin."
    ]
  },
  {
    "pr_number": 2768,
    "commits_list": [
      "62b398bc3d16812cd587a1fa15f0e43eca303583",
      "4a11d5cb9d40d0f1f11fe11f3c58daf7901b7822",
      "f44c16d58c6791f8ba6b14553e0cb0c20d2907c8",
      "6e40e7d723ddfe397e825afca568b014316a9d12",
      "2ef18e660ac47e70328d96883c0e645fb29099ce",
      "1089287532f3dd67ee452561f6b9ca012e04cb40",
      "b236a657ea5973cede6855b4ad74b12f28123a1e",
      "104f4e4ff23d5d7c55db2b0977c1e22ddc582578",
      "32a9ab3db608edb87f59b07a5579e28fc3f3abbf",
      "89fb689e9669fc8bb9a6565a3881bc1b59f72a9d",
      "92e302c16b54d9ea0bd24a19dbfb147a375044af",
      "6ecae00265b7596e0afcc633934866b51bae90da",
      "2b4c552dbc2b128482a8356e943f4586c22c6b4b",
      "49b3c4f369485135163e25de648cbf9ccdbf0e0e",
      "f893c28bca69ac334bf0e277c42fce9740109f37",
      "aca762645f10eb9bba67ed292a62398e19ddb4ba",
      "608bac336b015a516d2c5cba8aa1938f04e55e1c",
      "c6addf86c5ad39e7d5b8216d75d667c9be1cb397",
      "65c60abc3b8f0fdb0f3a6fba186bbf0bd01d6e2e"
    ],
    "message_list": [
      "CJS fix",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "add node attribute, to store the path of the folder created during a dry run",
      "Delete clean.sh",
      "Merge branch 'develop' into fix-dry-run-folder",
      "store submission folder within CalcJobNode repository",
      "Merge branch 'develop' into fix-dry-run-folder",
      "pylint fix",
      "Merge remote-tracking branch 'upstream/develop' into fix-dry-run-folder",
      "Merge remote-tracking branch 'upstream/develop' into fix-dry-run-folder",
      "Merge remote-tracking branch 'upstream/develop' into fix-dry-run-folder",
      "clean Jenkins workspace",
      "Merge remote-tracking branch 'upstream/develop' into fix-dry-run-folder"
    ]
  },
  {
    "pr_number": 3002,
    "commits_list": [
      "62b398bc3d16812cd587a1fa15f0e43eca303583",
      "4a11d5cb9d40d0f1f11fe11f3c58daf7901b7822",
      "f44c16d58c6791f8ba6b14553e0cb0c20d2907c8",
      "6e40e7d723ddfe397e825afca568b014316a9d12",
      "2ef18e660ac47e70328d96883c0e645fb29099ce",
      "1089287532f3dd67ee452561f6b9ca012e04cb40",
      "b236a657ea5973cede6855b4ad74b12f28123a1e",
      "104f4e4ff23d5d7c55db2b0977c1e22ddc582578",
      "791ee3d1d2857d8718e4f7e77a7ba113b5ee6407",
      "1b004812686d079b5bd3fd8b83a3b3b804d7a97d",
      "6cdbc601e0cb7dd3dfbb9ed0a77c8b6ff4182992",
      "6f9df6cb9b04d4fa622213d26a2326f7385e7e7d",
      "524bd8ccb12e314db2604d5eb309e7f1c0322436",
      "ea49760f714aa6c3c5bd4c47ff5f7138f17c0d0a",
      "fb9f42f27c75fdffde89546c224bc498f5bef12b",
      "47376ca5b3981d5923ea7237fc42eb5a83c2fef4",
      "5f7c933d9a67c381b97c183a227da6afea1570bf",
      "10d00c04331a43f98d41c30f491d565bb0c3ea79",
      "3b3f597369eb5e0a02649b079fd5d363fec22b18",
      "fb30237c32fb8209fa841e0cc01fdca6d9be545e",
      "9b047a61002512d90b3cca117ded141087dfdb53",
      "d1dd15311ab7cb77b6fd063063027cb237776b77",
      "f59e991317ef4752138dc98a1d1f1938a28b00d2"
    ],
    "message_list": [
      "CJS fix",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "Merge remote-tracking branch 'upstream/develop' into develop",
      "add order options to `verdi process list`",
      "pylint fix",
      " Improve `SchedulerError` message in `PbsBaseClass._parse_joblist_output`  (#2995)\n\nInclude the return value and content of `stdout` and `stderr`.",
      "add order options to `verdi process list`",
      "Merge branch 'verdi-proc-list-order' of https://github.com/chrisjsewell/aiida_core into verdi-proc-list-order",
      "add order options to `verdi process list`",
      "Merge branch 'verdi-proc-list-order' of https://github.com/chrisjsewell/aiida_core into verdi-proc-list-order"
    ]
  },
  {
    "pr_number": 3080,
    "commits_list": [
      "afb58466ae07e2f4c11349ed6f50959f3daf0e1e"
    ],
    "message_list": [
      "Fix the implementation of `UpfData.get_upf_groups`\n\nThe query builder statement was using the deprecated `type` attribute\nwhich has been replaced by `type_string`. It also returned duplicates in\nthe case of filtering on elements.\n\nCo-Authored-By: Sebastiaan Huber <mail@sphuber.net>"
    ]
  },
  {
    "pr_number": 5811,
    "commits_list": [
      "5d9fa3eea1a98d15da0ba34d4fc9af8061299130",
      "2cf1de830164c7d2f017ed83be666082ae242525",
      "47414a5ede900044b582dbf02d77b6df901c6714",
      "2884f445db840f5a76d4895b48ccbff945988410",
      "ec23b921491a4baa7aaf33790a27c7524394d70a",
      "bbc4cd7fbf368b2c86f7766451af797c49dbdc7a"
    ],
    "message_list": [
      "\ud83d\udd27 Make type-checking opt-out\n\nAlso update mypy and fix some failures",
      "Fix sphinx api docs",
      "Merge branch 'main' into type-checking",
      "Merge branch 'main' into type-checking",
      "Update process.py",
      "Update querybuilder.py"
    ]
  },
  {
    "pr_number": 4267,
    "commits_list": [
      "589f2da30553365b186c5b3c3178b2c77b58d02c"
    ],
    "message_list": [
      "apply fix for jenkins"
    ]
  },
  {
    "pr_number": 2671,
    "commits_list": [
      "d32aa14140670bdd8904f561aa05a0c118e17745"
    ],
    "message_list": [
      "Implement `__getitem__` on `Dict` node sub class\n\nThis shortcut will allow users to directly use `node[key]` instead of\nhaving to go through either `get_dict` or `dict` property."
    ]
  }
]